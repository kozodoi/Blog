<article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Layer-Wise Learning Rate in PyTorch</h1><p class="page-description">Implementing discriminative learning rate across model layers</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2022-03-29T00:00:00-05:00" itemprop="datePublished">
        Mar 29, 2022
      </time>• 
          <span itemprop="author" itemscope itemtype="http://schema.org/Person">
            <span class="p-author h-card" itemprop="name">Nikita Kozodoi</span></span>
       • <span class="read-time" title="Estimated read time">
    
    
      8 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i>
      
        <a class="category-tags-link" href="/categories/#python">python</a>
        &nbsp;
      
        <a class="category-tags-link" href="/categories/#deep learning">deep learning</a>
        &nbsp;
      
        <a class="category-tags-link" href="/categories/#pytorch">pytorch</a>
        &nbsp;
      
        <a class="category-tags-link" href="/categories/#tutorial">tutorial</a>
        
      
      </p>
    

    
      
        <div class="pb-5 d-flex flex-wrap flex-justify-end">
          <div class="px-2">
  <label class="switch">
    <input type="checkbox" href="/python/deep%20learning/pytorch/tutorial/2022/03/29/discriminative-lr.html" id="theme-toggle" checked onclick="modeSwitcher();">
      <span class="slider round"></span>
  </label>
</div>

          <div class="px-2">

    <a href="https://github.com/kozodoi/website/tree/master/_notebooks/2022-03-29-discriminative-lr.ipynb" role="button" target="_blank">
<img class="notebook-badge-image" src="/assets/badges/github.svg" alt="View on GitHub">
    </a>
</div>

          <div class="px-2">
    <a href="https://colab.research.google.com/github/kozodoi/website/blob/master/_notebooks/2022-03-29-discriminative-lr.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/assets/badges/colab.svg" alt="Open in Colab"/>
    </a>
</div>
        </div>
      
      <img src="/images/covers/lr.png" style="width:100%"/>
    <script type="text/javascript" src="https://platform-api.sharethis.com/js/sharethis.js#property=607a88d1f94d5c00182f2b41&product=inline-share-buttons" async="async"></script>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul class="section-nav">
<li class="toc-entry toc-h1"><a href="#1.-Overview">1. Overview </a></li>
<li class="toc-entry toc-h1"><a href="#2.-Implementation">2. Implementation </a>
<ul>
<li class="toc-entry toc-h2"><a href="#2.1.-Identifying-network-layers">2.1. Identifying network layers </a></li>
<li class="toc-entry toc-h2"><a href="#2.2.-Specifying-learning-rates">2.2. Specifying learning rates </a></li>
<li class="toc-entry toc-h2"><a href="#2.3.-Setting-up-the-optimizer">2.3. Setting up the optimizer </a></li>
</ul>
</li>
<li class="toc-entry toc-h1"><a href="#3.-Closing-words">3. Closing words </a></li>
</ul><!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2022-03-29-discriminative-lr.ipynb
-->

<div class="container" id="notebook-container">
        
    
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><em>Last update: 29.03.2022</em></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="1.-Overview">
<a class="anchor" href="#1.-Overview" aria-hidden="true"><span class="octicon octicon-link"></span></a>1. Overview<a class="anchor-link" href="#1.-Overview"> </a>
</h1>
<p>In deep learning tasks, we often use transfer learning to take advantage of the available pre-trained models. Fine-tuning such models is a careful process. On the one hand, we want to adjust the model to the new data set. On the other hand, we also want to retain and leverage as much knowledge learned during pre-training as possible.</p>
<p>Discriminative learning rate is one of the tricks that can help us guide fine-tuning. By using lower learning rates on deeper layers of the network, we make sure we are not tempering too much with the model blocks that have already learned general patterns and concentrate fine-tuning on further layers.</p>
<p>This blog post provides a tutorial on implementing discriminative layer-wise learning rates in <code>PyTorch</code>. We will see how to specify individual learning rates for each of the model parameter blocks and set up the training process.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="2.-Implementation">
<a class="anchor" href="#2.-Implementation" aria-hidden="true"><span class="octicon octicon-link"></span></a>2. Implementation<a class="anchor-link" href="#2.-Implementation"> </a>
</h1>
<p>The implementation of layer-wise learning rates is rather straightforward. It consists of three simple steps:</p>
<ol>
<li>Identifying a list of trainable layers in the neural net.</li>
<li>Setting up a list of model parameter blocks together with the corresponding learning rates.</li>
<li>Supplying the list with this information to the model optimizer.</li>
</ol>
<p>Let's go through each of these steps one by one and see how it works!</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="2.1.-Identifying-network-layers">
<a class="anchor" href="#2.1.-Identifying-network-layers" aria-hidden="true"><span class="octicon octicon-link"></span></a>2.1. Identifying network layers<a class="anchor-link" href="#2.1.-Identifying-network-layers"> </a>
</h2>
<p>The first step in our journey is to instantiate a model and retrieve the list of its layers. This step is essential to figure out how exactly to adjust the learning rate as we go through different parts of the network.</p>
<p>As an example, we will load one of the CNNs from the <code>timm</code> library and print out its parameter groups by iterating through <code>model.named_parameters()</code> and saving their names in a list called <code>layer_names</code>. Note that the framework discussed in this post is model-agnostic. It will work with any architecture, including CNNs, RNNs and transformers.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># instantiate model</span>
<span class="kn">import</span> <span class="nn">timm</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">timm</span><span class="o">.</span><span class="n">create_model</span><span class="p">(</span><span class="s1">'resnet18'</span><span class="p">,</span> <span class="n">num_classes</span> <span class="o">=</span> <span class="mi">2</span><span class="p">)</span>

<span class="c1"># save layer names</span>
<span class="n">layer_names</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">param</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">()):</span>
    <span class="n">layer_names</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'</span><span class="si">{</span><span class="n">idx</span><span class="si">}</span><span class="s1">: </span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>0: conv1.weight
1: bn1.weight
2: bn1.bias
3: layer1.0.conv1.weight
4: layer1.0.bn1.weight
5: layer1.0.bn1.bias
...
58: layer4.1.bn2.weight
59: layer4.1.bn2.bias
60: fc.weight
61: fc.bias
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>As the output suggests, our model has 62 parameter groups. When doing a forward pass, an image is fed to the first convolutional layer named <code>conv1</code>, whose parameters are stored as <code>conv1.weight</code>. Next, the output travels through the batch normalization layer <code>bn1</code>, which has weights and biases stored as <code>bn1.weight</code> and <code>bn1.bias</code>. From that point, the output goes through the network blocks grouped into four big chunks labeled as <code>layer1</code>, ..., <code>layer4</code>. Finally, extracted features are fed into the fully connected part of the network denoted as <code>fc</code>.</p>
<p>In the cell below, we reverse the list of parameter group names to have the deepest layer in the end of the list. This will be useful on the next step.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># reverse layers</span>
<span class="n">layer_names</span><span class="o">.</span><span class="n">reverse</span><span class="p">()</span>
<span class="n">layer_names</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">5</span><span class="p">]</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>['fc.bias',
 'fc.weight',
 'layer4.1.bn2.bias',
 'layer4.1.bn2.weight',
 'layer4.1.conv2.weight']</pre>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="2.2.-Specifying-learning-rates">
<a class="anchor" href="#2.2.-Specifying-learning-rates" aria-hidden="true"><span class="octicon octicon-link"></span></a>2.2. Specifying learning rates<a class="anchor-link" href="#2.2.-Specifying-learning-rates"> </a>
</h2>
<p>Knowing the architecture of our network, we can reason about the appropriate learning rates.</p>
<p>There is some flexibility in how to approach this step. 
The key idea is to gradually reduce the learning rate when going deeper into the network. 
The first layers should already have a pretty good understanding of general domain-agnostic patterns after pre-training. 
In a computer vision setting, the first layers may have learned to distinguish simple shapes and edges; in natural language processing, the first layers may be responsible for general word relationships. 
We don't want to update parameters on the first layers too much, so it makes sense to reduce the corresponding learning rates. 
In contrast, we would like to set a higher learning rate for the final layers, especially for the fully-connected classifier part of the network. 
Those layers usually focus on domain-specific information and need to be trained on new data.</p>
<p>The easiest approach to incorporate this logic is to incrementally reduce the learning rate when going deeper into the network. 
Let's simply multiply it by a certain coefficient between 0 and 1 after each parameter group.
In our example, this would gives us 62 gradually diminishing learning rate values for 62 model blocks.</p>
<p>Let's implement it in code! Below, we set up a list of dictionaries called <code>parameters</code> that stores model parameters and learning rates. 
We will simply go through all parameter blocks and iteratively reduce and assign the appropriate learning rate.
In our example, we start with <code>lr = 0.01</code> and multiply it by <code>0.9</code> at each step.
Each item in <code>parameters</code> becomes a dictionary with two elements:</p>
<ul>
<li>
<code>params</code>: tensor with the model parameters</li>
<li>
<code>lr</code>: corresponding learning rate</li>
</ul>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># learning rate</span>
<span class="n">lr</span>      <span class="o">=</span> <span class="mf">1e-2</span>
<span class="n">lr_mult</span> <span class="o">=</span> <span class="mf">0.9</span>

<span class="c1"># placeholder</span>
<span class="n">parameters</span> <span class="o">=</span> <span class="p">[]</span>

<span class="c1"># store params &amp; learning rates</span>
<span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">name</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">layer_names</span><span class="p">):</span>
    
    <span class="c1"># display info</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'</span><span class="si">{</span><span class="n">idx</span><span class="si">}</span><span class="s1">: lr = </span><span class="si">{</span><span class="n">lr</span><span class="si">:</span><span class="s1">.6f</span><span class="si">}</span><span class="s1">, </span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>
    
    <span class="c1"># append layer parameters</span>
    <span class="n">parameters</span> <span class="o">+=</span> <span class="p">[{</span><span class="s1">'params'</span><span class="p">:</span> <span class="p">[</span><span class="n">p</span> <span class="k">for</span> <span class="n">n</span><span class="p">,</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">()</span> <span class="k">if</span> <span class="n">n</span> <span class="o">==</span> <span class="n">name</span> <span class="ow">and</span> <span class="n">p</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">],</span>
                    <span class="s1">'lr'</span><span class="p">:</span>     <span class="n">lr</span><span class="p">}]</span>
    
    <span class="c1"># update learning rate</span>
    <span class="n">lr</span> <span class="o">*=</span> <span class="n">lr_mult</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>0: lr = 0.010000, fc.bias
1: lr = 0.009000, fc.weight
2: lr = 0.008100, layer4.1.bn2.bias
3: lr = 0.007290, layer4.1.bn2.weight
4: lr = 0.006561, layer4.1.conv2.weight
5: lr = 0.005905, layer4.1.bn1.bias
...
58: lr = 0.000022, layer1.0.conv1.weight
59: lr = 0.000020, bn1.bias
60: lr = 0.000018, bn1.weight
61: lr = 0.000016, conv1.weight
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>As you can see, we gradually reduce our learning rate from <code>0.01</code> for the bias on the classification layer to <code>0.00001</code> on the first convolutional layer. Looks good, right?!</p>
<p>Well, if you look closely, you will notice that we are setting different learning rates for parameter groups from the same layer. For example, having different learning rates for <code>fc.bias</code> and <code>fc.weight</code> does not really make that much sense. To address that, we can increment the learning rate only when going from one group of layers to another. The cell below provides an improved implementation.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<details class="description">
      <summary class="btn btn-sm" data-open="Hide Code" data-close="Show Code"></summary>
        <p></p>
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1">#collapse-hide</span>

<span class="c1"># learning rate</span>
<span class="n">lr</span>      <span class="o">=</span> <span class="mf">1e-2</span>
<span class="n">lr_mult</span> <span class="o">=</span> <span class="mf">0.9</span>

<span class="c1"># placeholder</span>
<span class="n">parameters</span>      <span class="o">=</span> <span class="p">[]</span>
<span class="n">prev_group_name</span> <span class="o">=</span> <span class="n">layer_names</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">'.'</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>

<span class="c1"># store params &amp; learning rates</span>
<span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">name</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">layer_names</span><span class="p">):</span>
    
    <span class="c1"># parameter group name</span>
    <span class="n">cur_group_name</span> <span class="o">=</span> <span class="n">name</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">'.'</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
    
    <span class="c1"># update learning rate</span>
    <span class="k">if</span> <span class="n">cur_group_name</span> <span class="o">!=</span> <span class="n">prev_group_name</span><span class="p">:</span>
        <span class="n">lr</span> <span class="o">*=</span> <span class="n">lr_mult</span>
    <span class="n">prev_group_name</span> <span class="o">=</span> <span class="n">cur_group_name</span>
    
    <span class="c1"># display info</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'</span><span class="si">{</span><span class="n">idx</span><span class="si">}</span><span class="s1">: lr = </span><span class="si">{</span><span class="n">lr</span><span class="si">:</span><span class="s1">.6f</span><span class="si">}</span><span class="s1">, </span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>
    
    <span class="c1"># append layer parameters</span>
    <span class="n">parameters</span> <span class="o">+=</span> <span class="p">[{</span><span class="s1">'params'</span><span class="p">:</span> <span class="p">[</span><span class="n">p</span> <span class="k">for</span> <span class="n">n</span><span class="p">,</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">()</span> <span class="k">if</span> <span class="n">n</span> <span class="o">==</span> <span class="n">name</span> <span class="ow">and</span> <span class="n">p</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">],</span>
                    <span class="s1">'lr'</span><span class="p">:</span>     <span class="n">lr</span><span class="p">}]</span>
</pre></div>

    </div>
</div>
</div>

    </details>
<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>0: lr = 0.010000, fc.bias
1: lr = 0.010000, fc.weight
2: lr = 0.009000, layer4.1.bn2.bias
3: lr = 0.009000, layer4.1.bn2.weight
4: lr = 0.009000, layer4.1.conv2.weight
5: lr = 0.009000, layer4.1.bn1.bias
...
58: lr = 0.006561, layer1.0.conv1.weight
59: lr = 0.005905, bn1.bias
60: lr = 0.005905, bn1.weight
61: lr = 0.005314, conv1.weight
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>This looks more interesting!</p>
<p>Note that we can become very creative in customizing the learning rates and the decay speed. There is no fixed rule that always works well. In my experience, simple linear decay with a multiplier between 0.9 and 1 is a good starting point. Still, the framework provides a lot of space for experimentation, so feel free to test out your ideas and see what works best on your data!</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="2.3.-Setting-up-the-optimizer">
<a class="anchor" href="#2.3.-Setting-up-the-optimizer" aria-hidden="true"><span class="octicon octicon-link"></span></a>2.3. Setting up the optimizer<a class="anchor-link" href="#2.3.-Setting-up-the-optimizer"> </a>
</h2>
<p>We are almost done. The last and the easiest step is to supply our list of model parameters together with the selected learning rates to the optimizer. In the cell below, we provide <code>parameters</code> to the Adam optimizer, which is one of the most frequently used ones in the field.</p>
<p>Note that we don't need to supply the learning rate to <code>Adam()</code> as we have already done it in our <code>parameters</code> object. As long as individual learning rates are available, <code>optimizer</code> will prioritize them over the single learning rate supplied to the <code>Adam()</code> call.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># set up optimizer</span>
<span class="kn">import</span> <span class="nn">torch.optim</span> <span class="k">as</span> <span class="nn">optim</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">parameters</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>This is it! Now we can proceed to training our model as usual. When calling <code>optimizer.step()</code> inside the training loop, the optimizer will update model parameters by  subtracting the gradient multiplied by the corresponding group-wise learning rates. This implies that there is no need to adjust the training loop, which usually looks something like this:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<details class="description">
      <summary class="btn btn-sm" data-open="Hide Code" data-close="Show Code"></summary>
        <p></p>
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1">#collapse-hide</span>

<span class="c1"># loop through batches</span>
<span class="k">for</span> <span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span> <span class="ow">in</span> <span class="n">data_loader</span><span class="p">:</span>

    <span class="c1"># extract inputs and labels</span>
    <span class="n">inputs</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="n">labels</span> <span class="o">=</span> <span class="n">labels</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

    <span class="c1"># passes and weights update</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">set_grad_enabled</span><span class="p">(</span><span class="kc">True</span><span class="p">):</span>
        
        <span class="c1"># forward pass </span>
        <span class="n">preds</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
        <span class="n">loss</span>  <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">preds</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>

        <span class="c1"># backward pass</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span> 

        <span class="c1"># weights update</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

    </details>
</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="3.-Closing-words">
<a class="anchor" href="#3.-Closing-words" aria-hidden="true"><span class="octicon octicon-link"></span></a>3. Closing words<a class="anchor-link" href="#3.-Closing-words"> </a>
</h1>
<p>In this post, we went through the steps of implementing a layer-wise discriminative learning rate in <code>PyTorch</code>. I hope this brief tutorial will help you set up your transfer learning pipeline and squeeze out the maximum of your pre-trained model. If you are interested, check out my other blog posts on tips on deep learning and <code>PyTorch</code>. Happy learning!</p>

</div>
</div>
</div>
</div>

<script type="application/vnd.jupyter.widget-state+json">
{"state": {"13c1b0b60dc243baa0859d1c6bd17129": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "FloatProgressModel", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "FloatProgressModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "ProgressView", "bar_style": "success", "description": "100%", "description_tooltip": null, "layout": "IPY_MODEL_6ec176035b11447582886c6d337eeff6", "max": 87306240, "min": 0, "orientation": "horizontal", "style": "IPY_MODEL_b89c0127f3a94527b58b7d710c43e951", "value": 87306240}}, "149e60260f0b49268d9e6ce0c8461dce": {"model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "model_name": "LayoutModel", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "19703cdff07249fab2d78d23d35192f3": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "HTMLModel", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HTMLModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HTMLView", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_149e60260f0b49268d9e6ce0c8461dce", "placeholder": "\u200b", "style": "IPY_MODEL_68071ef4d8ad4ce782ddc9b045860848", "value": " 83.3M/83.3M [00:00&lt;00:00, 116MB/s]"}}, "44310773133f44999edfef5f5ff34fbb": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "HBoxModel", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HBoxModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HBoxView", "box_style": "", "children": ["IPY_MODEL_13c1b0b60dc243baa0859d1c6bd17129", "IPY_MODEL_19703cdff07249fab2d78d23d35192f3"], "layout": "IPY_MODEL_ef696dddbf0e4bdf851a3c08acd0a3c9"}}, "68071ef4d8ad4ce782ddc9b045860848": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "DescriptionStyleModel", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "DescriptionStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "description_width": ""}}, "6ec176035b11447582886c6d337eeff6": {"model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "model_name": "LayoutModel", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "b89c0127f3a94527b58b7d710c43e951": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "ProgressStyleModel", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "ProgressStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "bar_color": null, "description_width": "initial"}}, "ef696dddbf0e4bdf851a3c08acd0a3c9": {"model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "model_name": "LayoutModel", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}}, "version_major": 2, "version_minor": 0}
</script>



  </div><!--<div id="disqus_thread"></div>
 <script>
   var disqus_config = function () {
     this.page.url = 'https://kozodoi.me/python/deep%20learning/pytorch/tutorial/2022/03/29/discriminative-lr.html';
     this.page.identifier = 'https://kozodoi.me/python/deep%20learning/pytorch/tutorial/2022/03/29/discriminative-lr.html';
   };
   (function() {
     var d = document, s = d.createElement('script');
     s.src = 'https://kozodoi.disqus.com/embed.js';
     s.setAttribute('data-timestamp', +new Date());
     (d.head || d.body).appendChild(s);
   })();
 </script>
 <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>
--><hr style="height:1px;border-width:0;color:rgb(50,50,50);background-color:rgb(50,50,50)">

  Liked the post? Share it on social media!
  <div class="sharethis-inline-share-buttons" style = "margin-top: 3px;"></div>

  <br>

  You can also buy me a cup of tea to support my work. Thanks!
  <div class="tea">
    <a href="https://www.buymeacoffee.com/kozodoi"><img src="https://img.buymeacoffee.com/button-api/?text=Buy me tea &emoji=&slug=kozodoi&button_colour=FFDD00&font_colour=000000&font_family=Lato&outline_colour=000000&coffee_colour=ffffff" align="left"></a>
  </div>

  <a class="u-url" href="/python/deep%20learning/pytorch/tutorial/2022/03/29/discriminative-lr.html" hidden></a>

  <script src="/assets/js/mode_switcher.js"></script>

</article>
