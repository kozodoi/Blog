{
  
    
        "post0": {
            "title": "Medical content creation in the age of generative AI",
            "content": "This post was originally published on the AWS ML Blog . 1. Introduction Generative AI and transformer-based large language models (LLMs) have been in the top headlines recently. These models demonstrate impressive performance in question answering, text summarization, code, and text generation. Today, LLMs are being used in real settings by companies, including the heavily-regulated healthcare and life sciences industry (HCLS). The use cases can range from medical information extraction and clinical notes summarization to marketing content generation and medical-legal review automation (MLR process). In this post, we explore how LLMs can be used to design marketing content for disease awareness. . Marketing content is a key component in the communication strategy of HCLS companies. It’s also a highly non-trivial balance exercise, because the technical content should be as accurate and precise as possible, yet engaging and empowering for the target audience. The main goal of the marketing content is to raise awareness about certain health conditions and disseminate knowledge of possible therapies among patients and healthcare providers. By accessing up-to-date and accurate information, healthcare providers can adapt their patients’ treatment in a more informed and knowledgeable way. However, medical content being highly sensitive, the generation process can be relatively slow (from days to weeks), and may go through numerous peer-review cycles, with thorough regulatory compliance and evaluation protocols. . Could LLMs, with their advanced text generation capabilities, help streamline this process by assisting brand managers and medical experts in their generation and review process? . To answer this question, the AWS Generative AI Innovation Center recently developed an AI assistant for medical content generation. The system is built upon Amazon Bedrock and leverages LLM capabilities to generate curated medical content for disease awareness. With this AI assistant, we can effectively reduce the overall generation time from weeks to hours, while giving the subject matter experts (SMEs) more control over the generation process. This is accomplished through an automated revision functionality, which allows the user to interact and send instructions and comments directly to the LLM via an interactive feedback loop. This is especially important since the revision of content is usually the main bottleneck in the process. . Since every piece of medical information can profoundly impact the well-being of patients, medical content generation comes with additional requirements and hinges upon the content’s accuracy and precision. For this reason, our system has been augmented with additional guardrails for fact-checking and rules evaluation. The goal of these modules is to assess the factuality of the generated text and its alignment with pre-specified rules and regulations. With these additional features, you have more transparency and control over the underlying generative logic of the LLM. . This post walks you through the implementation details and design choices, focusing primarily on the content generation and revision modules. Fact-checking and rules evaluation require special coverage and will be discussed in an upcoming post. . 2. Full Post ... (continue reading here) . .",
            "url": "https://kozodoi.me/blog/20240703/medical-content-generation",
            "relUrl": "/blog/20240703/medical-content-generation",
            "date": " • Jul 3, 2024"
        }
        
    
  
    
        ,"post1": {
            "title": "RAG with User Interaction",
            "content": "This post was originally published on the AWS ML Blog . 1. Introduction One of the most common applications of generative AI and large language models (LLMs) is answering questions based on a specific external knowledge corpus. Retrieval-Augmented Generation (RAG) is a popular technique for building question answering systems that use an external knowledge base. To learn more, refer to Build a powerful question answering bot with Amazon SageMaker, Amazon OpenSearch Service, Streamlit, and LangChain. . Traditional RAG systems often struggle to provide satisfactory answers when users ask vague or ambiguous questions without providing sufficient context. This leads to unhelpful responses like “I don’t know” or incorrect, made-up answers provided by an LLM. In this post, we demonstrate a solution to improve the quality of answers in such use cases over traditional RAG systems by introducing an interactive clarification component using LangChain. . The key idea is to enable the RAG system to engage in a conversational dialogue with the user when the initial question is unclear. By asking clarifying questions, prompting the user for more details, and incorporating the new contextual information, the RAG system can gather the necessary context to provide an accurate, helpful answer—even from an ambiguous initial user query. . 2. Full Post ... (continue reading here) . .",
            "url": "https://kozodoi.me/blog/20231113/rag-with-human-interaction",
            "relUrl": "/blog/20231113/rag-with-human-interaction",
            "date": " • Nov 13, 2023"
        }
        
    
  
    
        ,"post2": {
            "title": "Implementing PCA from Scratch",
            "content": "Last update: 26.03.2023. All opinions are my own. . 1. Overview . This blog post provides a tutorial on implementing the Principal Component Analysis algorithm using Python and NumPy. We will set up a simple class object, implement relevant methods to perform the decomposition, and illustrate how it works on a toy dataset. . Why are we implementing PCA from scratch if the algorithm is already available in scikit-learn? First, coding something from scratch is the best way to understand it. You may know many ML algorithms, but being able to write it down indicates that you have really mastered it. Second, implementing algorithms from scratch is a common task in ML interviews in tech companies, which makes it a useful skill that a job candidate should practice. Last but not least, it&#39;s a fun exercise, right? :) . This post is part of &quot;ML from Scratch&quot; series, where we implement established ML algorithms in Python. Check out other posts to see further implementations. . 2. How PCA works . Before jumping to implementation, let&#39;s quickly refresh our minds. How does PCA work? . PCA is a popular unsupervised algorithm used for dimensionality reduction. In a nutshell, PCA helps you to reduce the number of feature in your dataset by combining the features without loosing too much information. More specifically, PCA finds a linear data transformation that projects the data into a new coordinate system with a fewer dimensions. To capture the most variation in the original data, this projection is done by finding the so-called principal components - eigenvectors of the data&#39;s covariance matrix - and multiplying the actual data matrix with a subset of the components. This procedure is what we are going to implement. . P.S. If you need a more detailed summary of how PCA works, check out this Wiki page. . 3. Implementing PCA . Let&#39;s start the implementation! The only library we need to import is numpy: . import numpy as np . In line with object-oriented programming practices, we will implement PCA as a class with a set of methods. We will need the following five: . __init__(): initialize the class object. | fit(): center the data and identify principal components. | transform(): transform new data into the identified components. | Let&#39;s sketch a class object template. Since we implement functions as class methods, we include self argument for each method: . class PCA: def __init__(self): pass def fit(self): &quot;&quot;&quot; Find principal components &quot;&quot;&quot; pass def transform(self): &quot;&quot;&quot; Transform new data &quot;&quot;&quot; pass . Now let&#39;s go through each method one by one. . The __init__() method is run once when the initialize the PCA class object. . One thing need to do on the initialization step is to store meta-parameters of our algorithm. For PCA, there is only one meta-parameter we will specify: the number of components. We will save it as self.num_components. . Apart from the meta-parameters, we will create three placeholders that we will use to store important class attributes: . self.components: array with the principal component weights | self.mean: mean variable values observed in the training data | self.variance_share: proportion of variance explained by principal components | . def __init__(self, num_components): self.num_components = num_components self.components = None self.mean = None self.variance_share = None . Next, let&#39;s implement the fit() method - the heart of our PCA class. This method will be applied to a provided dataset to identify and memorize principal components. . We will do the following steps: . Center the data by subtracting the mean values for each variable. Normalizing variables is important to make sure that their impact in the data variation is similar and does not depend on the range of that variable. We will also memorize the mean values as self.mean as we will need it later for the data transformation. | Calculate eigenvectors of the covariance matrix. First, we will use np.cov() to get the covariance matrix of the data. Next, we will leverage np.linalg.eig() to do the eigenvalue decomposition and obtain both eigenvalues and eigenvectors. | Sort eigenvalues and eigenvectors in the decreasing order. Since we will use a smaller number of components compared to the number of variables in the original data, we would like to focus on components that reflect more data variation. In our case, eigenvectors that correspond to larger eigenvalues capture more variation. | Store an array with the top num_components components as self.components. | Finally, we will calculate and memorize the data variation explained by the selected components as self.variance_share. This can be computed as a cumulative sum of the corresponding eigenvalues divided by the total sum of eigenvalues. . def fit(self, X): &quot;&quot;&quot; Find principal components &quot;&quot;&quot; # data centering self.mean = np.mean(X, axis = 0) X -= self.mean # calculate eigenvalues &amp; vectors cov_matrix = np.cov(X.T) values, vectors = np.linalg.eig(cov_matrix) # sort eigenvalues &amp; vectors sort_idx = np.argsort(values)[::-1] values = values[sort_idx] vectors = vectors[:, sort_idx] # store principal components &amp; variance self.components = vectors[:self.num_components] self.variance_share = np.sum(values[:self.num_components]) / np.sum(values) . The most difficult part is over! Last but not least, we will implement a method to perform the data transformation. . This will be run after calling the fit() method on the training data, so we only need to implement two steps: . Center the new data using the same mean values that we used on the fitting stage. | Multiply the data matrix with the matrix of the selected components. Note that we will need to transpose the components matrix to ensure the right dimensionality. | def transform(self, X): &quot;&quot;&quot; Transform data &quot;&quot;&quot; # data centering X -= self.mean # decomposition return np.dot(X, self.components.T) . Putting everything together, this is how our implementation looks like: . class PCA: def __init__(self, num_components): self.num_components = num_components self.components = None self.mean = None self.variance_share = None def fit(self, X): &quot;&quot;&quot; Find principal components &quot;&quot;&quot; # data centering self.mean = np.mean(X, axis = 0) X -= self.mean # calculate eigenvalues &amp; vectors cov_matrix = np.cov(X.T) values, vectors = np.linalg.eig(cov_matrix) # sort eigenvalues &amp; vectors sort_idx = np.argsort(values)[::-1] values = values[sort_idx] vectors = vectors[:, sort_idx] # store principal components &amp; variance self.components = vectors[:self.num_components] self.variance_share = np.sum(values[:self.num_components]) / np.sum(values) def transform(self, X): &quot;&quot;&quot; Transform data &quot;&quot;&quot; # data centering X -= self.mean # decomposition return np.dot(X, self.components.T) . 4. Testing the implementation . Now that we have our implementation, let&#39;s check whether it actually works. We will generate two toy data samples with 10 features using the np.random module to draw feature values from a random Normal distribution: . X_old = np.random.normal(loc = 0, scale = 1, size = (1000, 10)) X_new = np.random.normal(loc = 0, scale = 1, size = (500, 10)) print(X_old.shape, X_new.shape) . (1000, 10) (500, 10) . Now, let&#39;s instantiate our PCA class, fit it on the old data and transform both datasets! . To see if the algorithm works properly, we will generate four new examples as X_new, gradually increasing the feature values from 1 to 5. We expect the label predicted by KNN to increase from 0 to 1, since we are getting closer to examples in X1. Let&#39;s check! . # initialize PCA object pca = PCA(num_components = 8) # fit PCA on old data pca.fit(X_old) # check explained variance print(f&quot;Explained variance: {pca.variance_share:.4f}&quot;) . Explained variance: 0.8325 . Eight components explain more than 83% of the data variation. Not bad! Let&#39;s transform the data: . # transform datasets X_old = pca.transform(X_old) X_new = pca.transform(X_new) print(X_old.shape, X_new.shape) . (1000, 8) (500, 8) . Yay! Everything works as expected. The new datasets have eight features instead of the original ten features. . 5. Closing words . This is it! I hope this tutorial helps you to refresh your memory on how PCA works and gives you a good idea on how to implement it yourself. You are now well-equipped to do this exercise on your own! . If you liked this tutorial, feel free to share it on social media and buy me a coffee :) Don&#39;t forget to check out other posts in the &quot;ML from Scratch&quot; series. Happy learning! .",
            "url": "https://kozodoi.me/blog/20230326/pca-from-scratch",
            "relUrl": "/blog/20230326/pca-from-scratch",
            "date": " • Mar 26, 2023"
        }
        
    
  
    
        ,"post3": {
            "title": "Implementing KNN from Scratch",
            "content": "Last update: 26.03.2023. All opinions are my own. . 1. Overview . This blog post provides a tutorial on implementing the K Nearest Neighbors algorithm using Python and NumPy. We will set up a simple class object, implement relevant methods to perform the prediction, and illustrate how it works on a toy dataset. . Why are we implementing KNN from scratch if the algorithm is already available in scikit-learn? First, coding something from scratch is the best way to understand it. You may know many ML algorithms, but being able to write it down indicates that you have really mastered it. Second, implementing algorithms from scratch is a common task in ML interviews in tech companies, which makes it a useful skill that a job candidate should practice. Last but not least, it&#39;s a fun exercise, right? :) . This post is part of &quot;ML from Scratch&quot; series, where we implement established ML algorithms in Python. Check out other posts to see further implementations. . 2. How KNN works . Before we jump to the implementation, let&#39;s quickly refresh our minds. How does KNN work? . KNN is one of the so-called lazy algorithms, which means that there is no actual training step. Instead, KNN memorizes the training data by storing the feature values of training examples. Given a new example to be predicted, KNN calculates distances between the new example and each of the examples in the training set. The prediction returned by the KNN algorithm is simply the average value of the target variable across the K nearest neighbors of the new example. . P.S. If you need a more detailed summary of how KNN works, check out this Wiki page. . 3. Implementing KNN . Let&#39;s start the implementation! The only library we need to import is numpy: . for size in layer_sizes: x = tf.keras.layers.Dense( size, kernel_initializer=&quot;he_uniform&quot;, activation=activation_fn, )(x) if size &lt; layer_sizes - 1: x = tf.keras.layers.BatchNormalization()(x) x = tf.keras.layers.Dropout(dropout_rate)(x) x = tf.keras.layers.Dense( n_outputs, kernel_initializer=&quot;he_uniform&quot;, activation=&quot;sigmoid&quot;, name=&quot;events_predictions&quot; )(x) . import numpy as np . In line with object-oriented programming practices, we will implement KNN as a class with a set of methods. We will need the following five: . __init__(): initialize the class object. | fit(): memorize the training data and store it as a class variable. | predict(): predict label for a new example. | get_distance(): helper function to calculate distance between two examples. | get_neighbors(): helper function to find and rank neighbors by distance. | The last two functions are optional: we can implement the logic inside the predict() method, but it will be easier to split the steps. . Let&#39;s sketch a class object template. Since we implement functions as class methods, we include self argument for each method: . class KNN: def __init__(self): pass def fit(self): &quot;&quot;&quot; Memorize training data &quot;&quot;&quot; pass def predict(self): &quot;&quot;&quot; Predict labels &quot;&quot;&quot; pass def get_distance(self): &quot;&quot;&quot; Calculate distance between two examples &quot;&quot;&quot; pass def get_neighbors(self): &quot;&quot;&quot; Find nearest neighbors &quot;&quot;&quot; pass . Now let&#39;s go through each method one by one. . The __init__() method is run once when the initialize the KNN class object. The only thing we need to do on the initialization step is to store meta-parameters of our algorithm. For KNN, there is only one key meta-parameter we specify: the number of neighbors. We will save it as self.num_neighbors: . def __init__(self, num_neighbors: int = 5): self.num_neighbors = num_neighbors . Next, let&#39;s implement the fit() method. As we mentioned above, on the training stage, KNN needs to memorize the training data. To simplify further calculations, we will provide the input data as two numpy arrays: features saved as self.X and labels saved as self.y: . def fit(self, X: np.array, y: np.array): &quot;&quot;&quot; Memorize training data &quot;&quot;&quot; self.X = X self.y = y . Now, let&#39;s write down a helper function to calculate distance between two examples, which are two numpy arrays with feature values. For simplicity, we will assume that all features are numeric. One of the most commonly used distance metrics is the Euclidean distance, which is calculated as a root of the sum of the squared differences between feature values. If the last sentence sounds complicated, here is how simple it looks in Python: . def get_distance(self, a: np.array, b: np.array): &quot;&quot;&quot; Calculate Euclidean distance between two examples &quot;&quot;&quot; return np.sum((a - b) ** 2) ** 0.5 . Now we are getting to the most difficult part of the KNN implementation! Below, we will write a helper function that finds nearest neighbors for a given example. For that, we will do several steps: . Calculate distance between the provided example and each example in the memorized dataset self.X. | Sort examples in self.X by their distances to the provided example. | Return indices of the nearest neighbors based on the self.num_neighbors meta-parameter. | For step 1, we will leverage the get_distance() function defined above. The trick to implement step 2 is two save a tuple (example ID, distance) when going through the training data. This will allow to sort the examples by distance and return the relevant IDs at the same time: . def get_neighbors(self, example: np.array): &quot;&quot;&quot; Find and rank nearest neighbors of example &quot;&quot;&quot; # placeholder distances = [] # calculate distances as tuples (id, distance) for i in range(len(self.X)): distances.append((i, self.get_distance(self.X[i], example))) # sort by distance distances.sort(key = lambda x: x[1]) # return IDs and distances top neighbors return distances[:self.num_neighbors] . The final step is to do the prediction! For this purpose, we implement the predict() method that expects a new dataset as a numpy array and provides an array with predictions. For each example in the new dataset, the method will go through its nearest neighbors identified using the get_neighbors() helper, and average labels across the neighbors. That&#39;s it! . def predict(self, X: np.array): &quot;&quot;&quot; Predict labels &quot;&quot;&quot; # placeholder predictions = [] # go through examples for idx in range(len(X)): example = X[idx] k_neighbors = self.get_neighbors(example) k_y_values = [self.y[item[0]] for item in k_neighbors] prediction = sum(k_y_values) / self.num_neighbors predictions.append(prediction) # return predictions return np.array(predictions) . Putting everything together, this is how our implementation looks like: . ### END-TO-END KNN CLASS class KNN: def __init__(self, num_neighbors: int = 5): self.num_neighbors = num_neighbors def fit(self, X: np.array, y: np.array): &quot;&quot;&quot; Memorize training data &quot;&quot;&quot; self.X = X self.y = y def get_distance(self, a: np.array, b: np.array): &quot;&quot;&quot; Calculate Euclidean distance between two examples &quot;&quot;&quot; return np.sum((a - b) ** 2) ** 0.5 def get_neighbors(self, example: np.array): &quot;&quot;&quot; Find and rank nearest neighbors of example &quot;&quot;&quot; # placeholder distances = [] # calculate distances as tuples (id, distance) for i in range(len(self.X)): distances.append((i, self.get_distance(self.X[i], example))) # sort by distance distances.sort(key = lambda x: x[1]) # return IDs and distances top neighbors return distances[:self.num_neighbors] def predict(self, X: np.array): &quot;&quot;&quot; Predict labels &quot;&quot;&quot; # placeholder predictions = [] # go through examples for idx in range(len(X)): example = X[idx] k_neighbors = self.get_neighbors(example) k_y_values = [self.y[item[0]] for item in k_neighbors] prediction = sum(k_y_values) / self.num_neighbors predictions.append(prediction) # return predictions return np.array(predictions) . 4. Testing the implementation . Now that we have our implementation, let&#39;s check whether it actually works. We will generate toy data using numpy. The gen_data() function below uses the np.random module to draw feature values from a random Normal distribution and assign a 0/1 label. . ### HELPER FUNCTION def gen_data( mu: float = 0, sigma: float = 1, y: int = 0, size: tuple = (1000, 10), ): &quot;&quot;&quot; Generate random data &quot;&quot;&quot; X = np.random.normal(loc = mu, scale = sigma, size = size) y = np.repeat(y, repeats = size[0]) return X, y . To simulate a simple ML problem, we will generate a dataset consisting of two samples: . 30 examples with mean features value of 1 and a label of 0. | 20 examples with mean features value of 5 and a label of 1. | ### TOY DATA GENERATION X0, y0 = gen_data(mu = 1, sigma = 3, y = 0, size = (30, 10)) X1, y1 = gen_data(mu = 5, sigma = 3, y = 1, size = (20, 10)) X = np.concatenate((X0, X1), axis = 0) y = np.concatenate((y0, y1), axis = 0) . Now, let&#39;s instantiate our KNN class, fit it on the training data and provide predictions for some new examples! . To see if the algorithm works properly, we will generate four new examples as X_new, gradually increasing the feature values from 1 to 5. We expect the label predicted by KNN to increase from 0 to 1, since we are getting closer to examples in X1. Let&#39;s check! . ### PREDICTION # fit KNN clf = KNN(num_neighbors = 5) clf.fit(X, y) # generate new examples X_new = np.stack(( np.repeat(1, 10), np.repeat(2, 10), np.repeat(4, 10), np.repeat(5, 10), )) # predict new examples clf.predict(X_new) . array([0. , 0.2, 0.8, 1. ]) . Yay! Everything works as expected. Our KNN algorithm provides four predictions for the new examples, and the prediction goes up with the increase in feature values. Our job is done! . 5. Closing words . This is it! I hope this tutorial helps you to refresh your memory on how KNN works and gives you a good idea on how to implement it yourself. You are now well-equipped to do this exercise on your own! . If you liked this tutorial, feel free to share it on social media and buy me a coffee :) Don&#39;t forget to check out other posts in the &quot;ML from Scratch&quot; series. Happy learning! .",
            "url": "https://kozodoi.me/blog/20230319/knn-from-scratch",
            "relUrl": "/blog/20230319/knn-from-scratch",
            "date": " • Mar 19, 2023"
        }
        
    
  
    
        ,"post4": {
            "title": "Layer-Wise Learning Rate in PyTorch",
            "content": "Last update: 29.03.2022. All opinions are my own. . 1. Overview . In deep learning tasks, we often use transfer learning to take advantage of the available pre-trained models. Fine-tuning such models is a careful process. On the one hand, we want to adjust the model to the new data set. On the other hand, we also want to retain and leverage as much knowledge learned during pre-training as possible. . Discriminative learning rate is one of the tricks that can help us guide fine-tuning. By using lower learning rates on deeper layers of the network, we make sure we are not tempering too much with the model blocks that have already learned general patterns and concentrate fine-tuning on further layers. . This blog post provides a tutorial on implementing discriminative layer-wise learning rates in PyTorch. We will see how to specify individual learning rates for each of the model parameter blocks and set up the training process. . 2. Implementation . The implementation of layer-wise learning rates is rather straightforward. It consists of three simple steps: . Identifying a list of trainable layers in the neural net. | Setting up a list of model parameter blocks together with the corresponding learning rates. | Supplying the list with this information to the model optimizer. | Let&#39;s go through each of these steps one by one and see how it works! . 2.1. Identifying network layers . The first step in our journey is to instantiate a model and retrieve the list of its layers. This step is essential to figure out how exactly to adjust the learning rate as we go through different parts of the network. . As an example, we will load one of the CNNs from the timm library and print out its parameter groups by iterating through model.named_parameters() and saving their names in a list called layer_names. Note that the framework discussed in this post is model-agnostic. It will work with any architecture, including CNNs, RNNs and transformers. . # instantiate model import timm model = timm.create_model(&#39;resnet18&#39;, num_classes = 2) # save layer names layer_names = [] for idx, (name, param) in enumerate(model.named_parameters()): layer_names.append(name) print(f&#39;{idx}: {name}&#39;) . 0: conv1.weight 1: bn1.weight 2: bn1.bias 3: layer1.0.conv1.weight 4: layer1.0.bn1.weight 5: layer1.0.bn1.bias ... 58: layer4.1.bn2.weight 59: layer4.1.bn2.bias 60: fc.weight 61: fc.bias . As the output suggests, our model has 62 parameter groups. When doing a forward pass, an image is fed to the first convolutional layer named conv1, whose parameters are stored as conv1.weight. Next, the output travels through the batch normalization layer bn1, which has weights and biases stored as bn1.weight and bn1.bias. From that point, the output goes through the network blocks grouped into four big chunks labeled as layer1, ..., layer4. Finally, extracted features are fed into the fully connected part of the network denoted as fc. . In the cell below, we reverse the list of parameter group names to have the deepest layer in the end of the list. This will be useful on the next step. . # reverse layers layer_names.reverse() layer_names[0:5] . [&#39;fc.bias&#39;, &#39;fc.weight&#39;, &#39;layer4.1.bn2.bias&#39;, &#39;layer4.1.bn2.weight&#39;, &#39;layer4.1.conv2.weight&#39;] . 2.2. Specifying learning rates . Knowing the architecture of our network, we can reason about the appropriate learning rates. . There is some flexibility in how to approach this step. The key idea is to gradually reduce the learning rate when going deeper into the network. The first layers should already have a pretty good understanding of general domain-agnostic patterns after pre-training. In a computer vision setting, the first layers may have learned to distinguish simple shapes and edges; in natural language processing, the first layers may be responsible for general word relationships. We don&#39;t want to update parameters on the first layers too much, so it makes sense to reduce the corresponding learning rates. In contrast, we would like to set a higher learning rate for the final layers, especially for the fully-connected classifier part of the network. Those layers usually focus on domain-specific information and need to be trained on new data. . The easiest approach to incorporate this logic is to incrementally reduce the learning rate when going deeper into the network. Let&#39;s simply multiply it by a certain coefficient between 0 and 1 after each parameter group. In our example, this would gives us 62 gradually diminishing learning rate values for 62 model blocks. . Let&#39;s implement it in code! Below, we set up a list of dictionaries called parameters that stores model parameters and learning rates. We will simply go through all parameter blocks and iteratively reduce and assign the appropriate learning rate. In our example, we start with lr = 0.01 and multiply it by 0.9 at each step. Each item in parameters becomes a dictionary with two elements: . params: tensor with the model parameters | lr: corresponding learning rate | . # learning rate lr = 1e-2 lr_mult = 0.9 # placeholder parameters = [] # store params &amp; learning rates for idx, name in enumerate(layer_names): # display info print(f&#39;{idx}: lr = {lr:.6f}, {name}&#39;) # append layer parameters parameters += [{&#39;params&#39;: [p for n, p in model.named_parameters() if n == name and p.requires_grad], &#39;lr&#39;: lr}] # update learning rate lr *= lr_mult . 0: lr = 0.010000, fc.bias 1: lr = 0.009000, fc.weight 2: lr = 0.008100, layer4.1.bn2.bias 3: lr = 0.007290, layer4.1.bn2.weight 4: lr = 0.006561, layer4.1.conv2.weight 5: lr = 0.005905, layer4.1.bn1.bias ... 58: lr = 0.000022, layer1.0.conv1.weight 59: lr = 0.000020, bn1.bias 60: lr = 0.000018, bn1.weight 61: lr = 0.000016, conv1.weight . As you can see, we gradually reduce our learning rate from 0.01 for the bias on the classification layer to 0.00001 on the first convolutional layer. Looks good, right?! . Well, if you look closely, you will notice that we are setting different learning rates for parameter groups from the same layer. For example, having different learning rates for fc.bias and fc.weight does not really make that much sense. To address that, we can increment the learning rate only when going from one group of layers to another. The cell below provides an improved implementation. . #collapse-hide # learning rate lr = 1e-2 lr_mult = 0.9 # placeholder parameters = [] prev_group_name = layer_names[0].split(&#39;.&#39;)[0] # store params &amp; learning rates for idx, name in enumerate(layer_names): # parameter group name cur_group_name = name.split(&#39;.&#39;)[0] # update learning rate if cur_group_name != prev_group_name: lr *= lr_mult prev_group_name = cur_group_name # display info print(f&#39;{idx}: lr = {lr:.6f}, {name}&#39;) # append layer parameters parameters += [{&#39;params&#39;: [p for n, p in model.named_parameters() if n == name and p.requires_grad], &#39;lr&#39;: lr}] . . 0: lr = 0.010000, fc.bias 1: lr = 0.010000, fc.weight 2: lr = 0.009000, layer4.1.bn2.bias 3: lr = 0.009000, layer4.1.bn2.weight 4: lr = 0.009000, layer4.1.conv2.weight 5: lr = 0.009000, layer4.1.bn1.bias ... 58: lr = 0.006561, layer1.0.conv1.weight 59: lr = 0.005905, bn1.bias 60: lr = 0.005905, bn1.weight 61: lr = 0.005314, conv1.weight . This looks more interesting! . Note that we can become very creative in customizing the learning rates and the decay speed. There is no fixed rule that always works well. In my experience, simple linear decay with a multiplier between 0.9 and 1 is a good starting point. Still, the framework provides a lot of space for experimentation, so feel free to test out your ideas and see what works best on your data! . 2.3. Setting up the optimizer . We are almost done. The last and the easiest step is to supply our list of model parameters together with the selected learning rates to the optimizer. In the cell below, we provide parameters to the Adam optimizer, which is one of the most frequently used ones in the field. . Note that we don&#39;t need to supply the learning rate to Adam() as we have already done it in our parameters object. As long as individual learning rates are available, optimizer will prioritize them over the single learning rate supplied to the Adam() call. . # set up optimizer import torch.optim as optim optimizer = optim.Adam(parameters) . This is it! Now we can proceed to training our model as usual. When calling optimizer.step() inside the training loop, the optimizer will update model parameters by subtracting the gradient multiplied by the corresponding group-wise learning rates. This implies that there is no need to adjust the training loop, which usually looks something like this: . #collapse-hide # loop through batches for (inputs, labels) in data_loader: # extract inputs and labels inputs = inputs.to(device) labels = labels.to(device) # passes and weights update with torch.set_grad_enabled(True): # forward pass preds = model(inputs) loss = criterion(preds, labels) # backward pass loss.backward() # weights update optimizer.step() optimizer.zero_grad() . . 3. Closing words . In this post, we went through the steps of implementing a layer-wise discriminative learning rate in PyTorch. I hope this brief tutorial will help you set up your transfer learning pipeline and squeeze out the maximum of your pre-trained model. If you are interested, check out my other blog posts on tips on deep learning and PyTorch. Happy learning! .",
            "url": "https://kozodoi.me/blog/20220329/discriminative-lr",
            "relUrl": "/blog/20220329/discriminative-lr",
            "date": " • Mar 29, 2022"
        }
        
    
  
    
        ,"post5": {
            "title": "Estimating Text Readability with Transformers",
            "content": "Last update: 21.11.2021. All opinions are my own. . 1. Overview . Estimating text complexity and readability is a crucial task for teachers. Offering students text passages at the right level of challenge is important for facilitating a fast development of reading skills. The existing tools to estimate readability rely on weak proxies and heuristics. Deep learning may help to improve the accuracy of the used text complexity scores. . This blog post overviews an interactive web app that estimates reading complexity of a custom text with deep learning. The app relies on transformer models that are part of my top-9% solution to the CommonLit Readability Prize Kaggle competition. The app is built in Python and deployed in Streamlit. The blog post provides a demo of the app and includes a summary of the modeling pipeline and the app implementation. . 2. App demo . You can open the app by clicking on this link. Alternatively, just scroll down to see the app embedded in this blog post. If the embedded version does not load, please open the app in a new tab. Feel free to play around with the app by typing or pasting custom texts and estimating their complexity with different models! Scroll further down to read some details on the app and the underlying models. . 3. Implementation . 3.1. Modeling pipeline . The app is developed in the scope of the CommonLit Readability Prize Kaggle competition on text complexity prediction. My solution is an ensemble of eight transformer models, including variants of BERT, RoBERTa and other architectures. All transformers are implemented in PyTorch and feature a custom regression head that uses a concatenated output of multiple hidden layers. . The project uses pre-trained transformer weights published on the HuggingFace model hub. Each model is then fine-tuned on a data set with 2834 text snippets, where readability of each snippet was evaluated by human experts. To avoid overfitting, fine-tuning relies on text augmentations such as sentence order shuffle, backtranslation and injecting target noise in the readability scores. . Each transformer model is fine-tuned using five-fold cross-validation repeated three times with different random splits. This GitHub repo provides the source code and documentation for the modeling pipeline. The table below summarizes the main architecture and training parameters. . . The ensemble of eight transformer models places in the top-9% of the Kaggle competition leaderboard. The web app only includes two lightweight models from a single fold to ensure fast inference on CPU: DistilBERT and DistilRoBERTa. . 3.2. App implementation . The app is built in Python using the Streamlit library. Streamlit allows implementing a web app in a single Python code file and deploying the app to the cloud server so that anyone with the Internet access can check it out. . The app code is provided in web_app.py located in the root folder of the project GitHub repo. The app is hosted on a virtual machine provided by Streamlit, which includes the list of dependencies specified in requirements.txt. It also imports some helper functions used within the modeling pipeline for text preprocessing and model initialization. . The app works by downloading weights of the selected transformer model to the virtual machine after a user selects which model to use for text readability prediction. The weights of each model are made available as release files on GitHub. After downloading the weights, the app transforms the text entered by a user into the token sequence with the tokenizer that uses text processing settings specified in the model configuration file. Next, the app runs a single forward pass through the initialized transformer network and displays the output prediction. . The snippet below provides the app source code. The code imports relevant Python modules and configures the app page. Next, it provides functionality for entering the custom text and selecting the NLP model. Finally, the code includes the inference function and some further documentation. . #collapse-show ##### PREPARATIONS # libraries import gc import os import pickle import sys import urllib.request import requests import numpy as np import pandas as pd import streamlit as st from PIL import Image # custom libraries sys.path.append(&#39;code&#39;) from model import get_model from tokenizer import get_tokenizer # download with progress bar mybar = None def show_progress(block_num, block_size, total_size): global mybar if mybar is None: mybar = st.progress(0.0) downloaded = block_num * block_size / total_size if downloaded &lt;= 1.0: mybar.progress(downloaded) else: mybar.progress(1.0) # page config st.set_page_config(page_title = &quot;Readability prediction&quot;, page_icon = &quot;:books:&quot;, layout = &quot;centered&quot;, initial_sidebar_state = &quot;collapsed&quot;, menu_items = None) ##### HEADER # title st.title(&#39;Text readability prediction&#39;) # image cover image = Image.open(requests.get(&#39;https://i.postimg.cc/hv6yfMYz/cover-books.jpg&#39;, stream = True).raw) st.image(image) # description st.write(&#39;This app uses deep learning to estimate the reading complexity of a custom text. Enter your text below, and we will run it through one of the two transfomer models and display the result.&#39;) ##### PARAMETERS # title st.header(&#39;How readable is your text?&#39;) # model selection model_name = st.selectbox( &#39;Which model would you like to use?&#39;, [&#39;DistilBERT&#39;, &#39;DistilRoBERTa&#39;]) # input text input_text = st.text_area(&#39;Which text would you like to rate?&#39;, &#39;Please enter the text in this field.&#39;) ##### MODELING # compute readability if st.button(&#39;Compute readability&#39;): # specify paths if model_name == &#39;DistilBERT&#39;: folder_path = &#39;output/v59/&#39; weight_path = &#39;https://github.com/kozodoi/Kaggle_Readability/releases/download/0e96d53/weights_v59.pth&#39; elif model_name == &#39;DistilRoBERTa&#39;: folder_path = &#39;output/v47/&#39; weight_path = &#39;https://github.com/kozodoi/Kaggle_Readability/releases/download/0e96d53/weights_v47.pth&#39; # download model weights if not os.path.isfile(folder_path + &#39;pytorch_model.bin&#39;): with st.spinner(&#39;Downloading model weights. This is done once and can take a minute...&#39;): urllib.request.urlretrieve(weight_path, folder_path + &#39;pytorch_model.bin&#39;, show_progress) # compute predictions with st.spinner(&#39;Computing prediction...&#39;): # clear memory gc.collect() # load config config = pickle.load(open(folder_path + &#39;configuration.pkl&#39;, &#39;rb&#39;)) config[&#39;backbone&#39;] = folder_path # initialize model model = get_model(config, name = model_name.lower(), pretrained = folder_path + &#39;pytorch_model.bin&#39;) model.eval() # load tokenizer tokenizer = get_tokenizer(config) # tokenize text text = tokenizer(text = input_text, truncation = True, add_special_tokens = True, max_length = config[&#39;max_len&#39;], padding = False, return_token_type_ids = True, return_attention_mask = True, return_tensors = &#39;pt&#39;) inputs, masks, token_type_ids = text[&#39;input_ids&#39;], text[&#39;attention_mask&#39;], text[&#39;token_type_ids&#39;] # clear memory del tokenizer, text, config gc.collect() # compute prediction if input_text != &#39;&#39;: prediction = model(inputs, masks, token_type_ids) prediction = prediction[&#39;logits&#39;].detach().numpy()[0][0] prediction = 100 * (prediction + 4) / 6 # scale to [0,100] # clear memory del model, inputs, masks, token_type_ids gc.collect() # print output st.metric(&#39;Readability score:&#39;, &#39;{:.2f}%&#39;.format(prediction, 2)) st.write(&#39;**Note:** readability scores are scaled to [0, 100%]. A higher score means that the text is easier to read.&#39;) st.success(&#39;Success! Thanks for scoring your text :)&#39;) ##### DOCUMENTATION # header st.header(&#39;More information&#39;) # example texts with st.expander(&#39;Show example texts&#39;): st.table(pd.DataFrame({ &#39;Text&#39;: [&#39;A dog sits on the floor. A cat sleeps on the sofa.&#39;, &#39;This app does text readability prediction. How cool is that?&#39;, &#39;Training of deep bidirectional transformers for language understanding.&#39;], &#39;Score&#39;: [1.5571, -0.0100, -2.4025], })) # models with st.expander(&#39;Read about the models&#39;): st.write(&quot;Both transformer models are part of my top-9% solution to the CommonLit Readability Kaggle competition. The pre-trained language models are fine-tuned on 2834 text snippets. [Click here](https://github.com/kozodoi/Kaggle_Readability) to see the source code and read more about the training pipeline.&quot;) # metric with st.expander(&#39;Read about the metric&#39;): st.write(&quot;The readability metric is calculated on the basis of a Bradley-Terry analysis of more than 111,000 pairwise comparisons between excerpts. Teachers spanning grades 3-12 (a majority teaching between grades 6-10) served as the raters for these comparisons. More details on the used reading complexity metric are available [here](https://www.kaggle.com/c/commonlitreadabilityprize/discussion/240886).&quot;) . . 4. Closing words . This blog post provided a demo of an interactive web app that uses deep learning to estimate text reading complexity. I hope you found the app interesting and enjoyed playing with it! . If you have any data science projects in your portfolio, I highly encourage you to try developing a similar app yourself. There are many things you could demonstrate, ranging from interactive EDA dashboards to inference calls to custom ML models. Streamlit makes this process very simple and allows hosting the app in the cloud. Happy learning! .",
            "url": "https://kozodoi.me/blog/20211121/text-readability",
            "relUrl": "/blog/20211121/text-readability",
            "date": " • Nov 21, 2021"
        }
        
    
  
    
        ,"post6": {
            "title": "Test-Time Augmentation for Tabular Data",
            "content": "Last update: 08.09.2021. All opinions are my own. . 1. Overview . Test time augmentation (TTA) is a popular technique in computer vision. TTA aims at boosting the model accuracy by using data augmentation on the inference stage. The idea behind TTA is simple: for each test image, we create multiple versions that are a little different from the original (e.g., cropped or flipped). Next, we predict labels for the test images and created copies and average model predictions over multiple versions of each image. This usually helps to improve the accuracy irrespective of the underlying model. . In many business settings, data comes in a tabular format. Can we use TTA with tabular data to enhance the accuracy of ML models in a way similar to computer vision models? How to define suitable transformations of test cases that do not affect the label? This blog post explores the opportunities for using TTA in tabular data environments. We will implement TTA for scikit-learn classifiers and test its performance on multiple credit scoring data sets. The preliminary results indicate that TTA might be a tiny bit helpful in some settings. . Note: the results presented in this blog post are currently being extended within a scope of a working paper. The post will be updated once the paper is available on ArXiV. . 2. Adapting TTA to tabular data . TTA has been originally developed for deep learning applications in computer vision. In contrast to image data, tabular data poses a more challenging environment for using TTA. We will discuss two main challenges that we need to solve to apply TTA to structured data: . how to define transformations? | how to treat categorical features? | . 2.1. How to define transformations? . When working with image data, light transformations such as rotation, brightness adjustment, saturation and many others modify the underlying pixel values but do not affect the ground truth. That is, a rotated cat is still a cat. We can easily verify this by visually checking the transformed images and limiting the magnitude of transformations to make sure the cat is still recognizable. . . This is different for tabular data, where the underlying features represent different characteristics of the observed subjects. Let&#39;s consider a credit scoring example. In finance, banks use ML models to support loan allocation decisions. Consider a binary classification problem, where we predict whether the applicant will pay back the loan. The underlying features may describe the applicant&#39;s attributes (age, gender), loan parameters (amount, duration), macroeconomic indicators (inflation, growth). How to do transformations on these features? While there is no such thing as rotating a loan applicant (at least not within the scope of machine learning), we could do a somewhat similar exercise: create copies of each loan applicant and slightly modify feature values for each copy. A good starting point would be to add some random noise to each of the features. . This procedure raises a question: how can we be sure that transformations do not alter the label? Would increasing the applicant&#39;s age by 10 years affect her repayment ability? Arguably, yes. What about increasing the age by 1 year? Or 1 day? These are challenging questions that we can not answer without more information. This implies that the magnitude of the added noise has to be carefully tuned. We need to take into account the variance of each specific feature as well as the overall data set variability. Adding too little noise will create synthetic cases that are too similar to the original applications, which is not very useful. On the other hand, adding too much noise risks changing the label of the corresponding application, which would harm the model accuracy. The trade-off between these two extremes is what can potentially bring us closer to discovering an accuracy boost. . 2.2. How to treat categorical features? . It is rather straightforward to add noise to continuous features such as age or income. However, tabular data frequently contains special gifts: categorical features. From gender to zip code, these features present another challenge for the application of TTA. Adding noise to the zip code appears non-trivial and requires some further thinking. Ignoring categorical features and only altering the continuous ones sounds like an easy solution, but this might not work well on data sets that contain a lot of information in the form of categorical data. . In this blog post, we will try a rather naive approach to deal with categorical features. Every categorical feature can be encoded as a set of dummy variables. Next, considering each dummy feature separately, we can occasionally flip the value, switching the person&#39;s gender, country of origin or education level with one click. This would introduce some variance in the categorical features and provide TTA with more diverse synthetic applications. This approach is imperfect and can be improved on, but we have to start somewhere, right? . Now that we have some ideas about how TTA should work and what are the main challenges, let&#39;s actually try to implement it! . 3. Implementing TTA . This section implements a helper function predict_proba_with_tta() to extend the standard predict_proba() method in scikit-learn such that predictions take advantage of the TTA procedure. We focus on a binary classification task, but one could easily extend this framework to regression tasks as well. . The function predict_proba_with_tta() requires specifying the underlying scikit-learn model and the test set with observations to be predicted. The function operates in four simple steps: . Creating num_tta copies of the test set. | Implementing random transformations of the synthetic copies. | Predicting labels for the real and synthetic observations. | Aggregating the predictions. | Considering the challenges discussed in the previous section, we implement the following transformations for the continuous features: . compute STD of each continuous feature denoted as std | generate a random vector n using the standard normal distribution | add alpha * n * std to each feature , where alpha is a meta-parameter. | . And for the categorical features: . convert categorical features into a set of dummies | flip each dummy variable with a probability beta, where beta is a meta-parameter. | . By varying alpha and beta, we control the transformation magnitude, adjusting the noise scale in the synthetic copies. Higher values imply stronger transformations. The suitable values can be identified through some meta-parameter tuning. . #collapse-show def predict_proba_with_tta(data, model, dummies = None, num_tta = 4, alpha = 0.01, beta = 0.01, seed = 0): &#39;&#39;&#39; Predicts class probabilities using TTA. Arguments: - data (numpy array): data set with the feature values - model (sklearn model): machine learning model - dummies (list): list of column names of dummy features - num_tta (integer): number of test-time augmentations - alpha (float): noise parameter for continuous features - beta (float): noise parameter for dummy features - seed (integer): random seed Returns: - array of predicted probabilities &#39;&#39;&#39; # set random seed np.random.seed(seed = seed) # original prediction preds = model.predict_proba(data) / (num_tta + 1) # select numeric features num_vars = [var for var in data.columns if data[var].dtype != &#39;object&#39;] # find dummies if dummies != None: num_vars = list(set(num_vars) - set(dummies)) # synthetic predictions for i in range(num_tta): # copy data data_new = data.copy() # introduce noise to numeric vars for var in num_vars: data_new[var] = data_new[var] + alpha * np.random.normal(0, 1, size = len(data_new)) * data_new[var].std() # introduce noise to dummies if dummies != None: for var in dummies: probs = np.random.binomial(1, (1 - beta), size = len(data_new)) data_new.loc[probs == 0, var] = 1 - data_new.loc[probs == 0, var] # predict probs preds_new = model.predict_proba(data_new) preds += preds_new / (num_tta + 1) # return probs return preds . . 4. Empirical benchmark . Let&#39;s test our TTA function! This section performs empirical experiment on multiple data sets to check whether TTA can improve the model performance. First, we import relevant modules and load the list of prepared data sets. All data sets come from a credit scoring environment, which represents a binary classification setup. Some of the data sets are publically available, whereas the others are subject to NDA. The public data sets include australian), german), pakdd, gmsc, homecredit and lendingclub. The sample sizes and the number of features vary greatly across the datasets. This allows us to test the TTA framework in different conditions. . #collapse-hide import numpy as np import pandas as pd import matplotlib.pyplot as plt import seaborn as sns from sklearn.ensemble import RandomForestClassifier from sklearn.model_selection import StratifiedKFold from sklearn.metrics import roc_auc_score import os import time . . #collapse-show datasets = os.listdir(&#39;../data&#39;) datasets . . [&#39;thomas.csv&#39;, &#39;german.csv&#39;, &#39;hmeq.csv&#39;, &#39;bene2.csv&#39;, &#39;lendingclub.csv&#39;, &#39;bene1.csv&#39;, &#39;cashbus.csv&#39;, &#39;uk.csv&#39;, &#39;australian.csv&#39;, &#39;pakdd.csv&#39;, &#39;gmsc.csv&#39;, &#39;paipaidai.csv&#39;] . Apart from the data sets, TTA needs an underlying ML model. In our experiment, on each data set, we will use a Random Forest classifier with 500 trees, which is a good trade-off between good performance and computational resources. We will not go deep into tuning the classifier and keep the parameters fixed for all data sets. We will then use stratified 5-fold cross-validation to train and test models with and without TTA. . #collapse-show # classifier clf = RandomForestClassifier(n_estimators = 500, random_state = 1, n_jobs = 4) # settings folds = StratifiedKFold(n_splits = 5, shuffle = True, random_state = 23) . . The cell below implements the following experiment: . We loop through the datasets and perform cross-validation, training Random Forest on each fold combination. | Next, we predict labels of the validation cases and calculate the AUC of the model predictions. This is our benchmark. | We predict labels of the validation cases with the same model but now implement TTA to adjust the predictions. | By comparing the average AUC difference before and after TTA, we can judge whether TTA actually helps to boost the predictive performance. | #collapse-show # placeholders auc_change = [] # timer start = time.time() # modeling loop for data in datasets: ##### DATA PREPARATION # import data X = pd.read_csv(&#39;../data/&#39; + data) # convert target to integer X.loc[X.BAD == &#39;BAD&#39;, &#39;BAD&#39;] = 1 X.loc[X.BAD == &#39;GOOD&#39;, &#39;BAD&#39;] = 0 # extract X and y y = X[&#39;BAD&#39;] del X[&#39;BAD&#39;] # create dummies X = pd.get_dummies(X, prefix_sep = &#39;_dummy_&#39;) # data information print(&#39;-&#39;) print(&#39;Dataset:&#39;, data, X.shape) print(&#39;-&#39;) ##### CROSS-VALIDATION # create objects oof_preds_raw = np.zeros((len(X), y.nunique())) oof_preds_tta = np.zeros((len(X), y.nunique())) # modeling loop for fold_, (trn_, val_) in enumerate(folds.split(y, y)): # data partitioning trn_x, trn_y = X.iloc[trn_], y.iloc[trn_] val_x, val_y = X.iloc[val_], y.iloc[val_] # train the model clf.fit(trn_x, trn_y) # identify dummies dummies = list(X.filter(like = &#39;_dummy_&#39;).columns) # predictions oof_preds_raw[val_, :] = clf.predict_proba(val_x) oof_preds_tta[val_, :] = predict_proba_with_tta(data = val_x, model = clf, dummies = dummies, num_tta = 5, alpha = np.sqrt(len(trn_x)) / 3000, beta = np.sqrt(len(trn_x)) / 30000, seed = 1) # print performance print(&#39;- AUC before TTA = %.6f &#39; % roc_auc_score(y, oof_preds_raw[:,1])) print(&#39;- AUC with TTA = %.6f &#39; % roc_auc_score(y, oof_preds_tta[:,1])) print(&#39;-&#39;) print(&#39;&#39;) # save the AUC delta delta = roc_auc_score(y, oof_preds_tta[:,1]) - roc_auc_score(y, oof_preds_raw[:,1]) auc_change.append(delta) # display results print(&#39;-&#39;) print(&#39;Finished in %.1f minutes&#39; % ((time.time() - start) / 60)) print(&#39;-&#39;) print(&#39;TTA improves AUC in %.0f/%.0f cases&#39; % (np.sum(np.array(auc_change) &gt; 0), len(datasets))) print(&#39;Mean AUC change = %.6f&#39; % np.mean(auc_change)) print(&#39;-&#39;) . . - Dataset: thomas.csv (1225, 28) - - AUC before TTA = 0.612322 - AUC with TTA = 0.613617 - - Dataset: german.csv (1000, 61) - - AUC before TTA = 0.796233 - AUC with TTA = 0.796300 - - Dataset: hmeq.csv (5960, 20) - - AUC before TTA = 0.975995 - AUC with TTA = 0.976805 - - Dataset: bene2.csv (7190, 28) - - AUC before TTA = 0.801193 - AUC with TTA = 0.799387 - - Dataset: lendingclub.csv (43344, 114) - - AUC before TTA = 0.625029 - AUC with TTA = 0.628207 - - Dataset: bene1.csv (3123, 84) - - AUC before TTA = 0.788607 - AUC with TTA = 0.789447 - - Dataset: cashbus.csv (15000, 642) - - AUC before TTA = 0.629648 - AUC with TTA = 0.624874 - - Dataset: uk.csv (30000, 51) - - AUC before TTA = 0.712042 - AUC with TTA = 0.723359 - - Dataset: australian.csv (690, 42) - - AUC before TTA = 0.931787 - AUC with TTA = 0.931958 - - Dataset: pakdd.csv (50000, 373) - - AUC before TTA = 0.620081 - AUC with TTA = 0.623080 - - Dataset: gmsc.csv (150000, 68) - - AUC before TTA = 0.846187 - AUC with TTA = 0.855176 - - Dataset: paipaidai.csv (60000, 1934) - - AUC before TTA = 0.716398 - AUC with TTA = 0.721679 - - Finished in 206.1 minutes - TTA improves AUC in 10/12 cases Mean AUC change = 0.002364 - . Looks like TTA is working! Overall, TTA improves the AUC in 10 out of 12 data sets. The observed performance gains are rather small: on average, TTA improves AUC by 0.00236. The results are visualized in the barplot below: . #collapse-hide objects = list(range(len(datasets))) y_pos = np.arange(len(objects)) perf = np.sort(auc_change2) plt.figure(figsize = (6, 8)) plt.barh(y_pos, perf, align = &#39;center&#39;, color = &#39;blue&#39;, alpha = 0.66) plt.ylabel(&#39;Dataset&#39;) plt.yticks(y_pos, objects) plt.xlabel(&#39;AUC Gain&#39;) plt.title(&#39;&#39;) ax.plot([0, 0], [1, 12], &#39;k--&#39;) plt.tight_layout() . . . We should bear in mind that performance gains, although appearing rather small, come almost &quot;for free&quot;. We don&#39;t need to train a new model and only require a relatively small amount of extra resources to create synthetic copies of the loan applications. Sounds good! . It is possible that further fine-tuning of the TTA meta-parameters can uncover larger performance gains. Furthermore, a considerable variance of the average gains from TTA across the data sets indicates that TTA can be more helpful in specific settings. The important factors influencing the TTA performance may relate to both the data and the classifier used to produce predictions. More research is needed to identify and analyze such factors. . 5. Closing words . The purpose of this tutorial was to explore TTA applications for tabular data. We have discussed the corresponding challenges, developed a TTA wrapper function for scikit-learn and demonstrated that it could indeed be helpful on multiple credit scoring data sets. I hope you found this post interesting. . The project described in this blog post is a work in progress. I will update the post once the working paper on the usage of TTA for tabular data is available. Stay tuned and happy learning! .",
            "url": "https://kozodoi.me/blog/20210908/tta-tabular",
            "relUrl": "/blog/20210908/tta-tabular",
            "date": " • Sep 8, 2021"
        }
        
    
  
    
        ,"post7": {
            "title": "Extracting Intermediate Layer Outputs in PyTorch",
            "content": "Last update: 23.10.2021. All opinions are my own. . 1. Overview . In deep learning tasks, we usually work with predictions outputted by the final layer of a neural network. In some cases, we might also be interested in the outputs of intermediate layers. Whether we want to extract data embeddings or inspect what is learned by earlier layers, it may not be straightforward how to extract the intermediate features from the network. . This blog post provides a quick tutorial on the extraction of intermediate activations from any layer of a deep learning model in PyTorch using the forward hook functionality. The important advantage of this method is its simplicity and ability to extract features without having to run the inference twice, only requiring a single forward pass through the model to save multiple outputs. . 2. Why do we need intermediate features? . Extracting intermediate activations (also called features) can be useful in many applications. In computer vision problems, outputs of intermediate CNN layers are frequently used to visualize the learning process and illustrate visual features distinguished by the model on different layers. Another popular use case is extracting intermediate outputs to create image or text embeddings, which can be used to detect duplicate items, included as input features in a classical ML model, visualize data clusters and much more. When working with Encoder-Decoder architectures, outputs of intermediate layers can also be used to compress the data into a smaller-sized vector containing the data represenatation. There are many further use cases in which intermediate activations can be useful. So, let&#39;s discuss how to get them! . 3. How to extract activations? . To extract activations from intermediate layers, we will need to register a so-called forward hook for the layers of interest in our neural network and perform inference to store the relevant outputs. . For the purpose of this tutorial, I will use image data from a Cassava Leaf Disease Classification Kaggle competition. In the next few cells, we will import relevant libraries and set up a Dataloader object. Feel free to skip them if you are familiar with standard PyTorch data loading practices and go directly to the feature extraction part. . Preparations . # collapse-hide ##### PACKAGES import numpy as np import pandas as pd import torch import torch.nn as nn from torch.utils.data import Dataset, DataLoader !pip install timm import timm import albumentations as A from albumentations.pytorch import ToTensorV2 import cv2 import os device = torch.device(&quot;cuda&quot;) . . # collapse-hide ##### DATASET class ImageData(Dataset): # init def __init__(self, data, directory, transform): self.data = data self.directory = directory self.transform = transform # length def __len__(self): return len(self.data) # get item def __getitem__(self, idx): # import image = cv2.imread( os.path.join(self.directory, self.data.iloc[idx][&quot;image_id&quot;]) ) image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # augmentations image = self.transform(image=image)[&quot;image&quot;] return image . . We will use a standrd PyTorch dataloader to load the data in batches of 32 images. . # collapse-show ##### DATA LOADER # import data df = pd.read_csv(&quot;../input/cassava-leaf-disease-classification/train.csv&quot;) display(df.head()) # augmentations transforms = A.Compose([A.Resize(height=128, width=128), A.Normalize(), ToTensorV2()]) # dataset data_set = ImageData( data=df, directory=&quot;../input/cassava-leaf-disease-classification/train_images/&quot;, transform=transforms, ) # dataloader data_loader = DataLoader(data_set, batch_size=32, shuffle=False, num_workers=2) . . image_id label . 0 1000015157.jpg | 0 | . 1 1000201771.jpg | 3 | . 2 100042118.jpg | 1 | . 3 1000723321.jpg | 1 | . 4 1000812911.jpg | 3 | . Model . To extract anything from a neural net, we first need to set up this net, right? In the cell below, we define a simple resnet18 model with a two-node output layer. We use timm library to instantiate the model, but feature extraction will also work with any neural network written in PyTorch. . We also print out the architecture of our network. As you can see, there are many intermediate layers through which our image travels during a forward pass before turning into a two-number output. We should note the names of the layers because we will need to provide them to a feature extraction function. . ##### DEFINE MODEL model = timm.create_model(model_name=&quot;resnet18&quot;, pretrained=True) model.fc = nn.Linear(512, 2) model.to(device) . ResNet( (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False) (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (act1): ReLU(inplace=True) (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False) (layer1): Sequential( (0): BasicBlock( (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (act1): ReLU(inplace=True) (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (act2): ReLU(inplace=True) ) (1): BasicBlock( (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (act1): ReLU(inplace=True) (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (act2): ReLU(inplace=True) ) ) ... (layer4): Sequential( (0): BasicBlock( (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (act1): ReLU(inplace=True) (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (act2): ReLU(inplace=True) (downsample): Sequential( (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (1): BasicBlock( (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (act1): ReLU(inplace=True) (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (act2): ReLU(inplace=True) ) ) (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=True) (fc): Linear(in_features=512, out_features=2, bias=True) ) . Feature extraction . The implementation of feature extraction requires two simple steps: . Registering a forward hook on a certain layer of the network. | Performing standard inference to extract features of that layer. | First, we need to define a helper function that will introduce a so-called hook. A hook is simply a command that is executed when a forward or backward call to a certain layer is performed. If you want to know more about hooks, you can check out this link. . In out setup, we are interested in a forward hook that simply copies the layer outputs, sends them to CPU and saves them to a dictionary object we call features. . The hook is defined in a cell below. The name argument in get_features() specifies the dictionary key under which we will store our intermediate activations. . ##### HELPER FUNCTION FOR FEATURE EXTRACTION def get_features(name): def hook(model, input, output): features[name] = output.detach() return hook . After the helper function is defined, we can register a hook using .register_forward_hook() method. The hook can be applied to any layer of the neural network. . Since we work with a CNN, extracting features from the last convolutional layer might be useful to get image embeddings. Therefore, we are registering a hook for the outputs of the (global_pool). To extract features from an earlier layer, we could also access them with, e.g., model.layer1[1].act2 and save it under a different name in the features dictionary. With this method, we can actually register multiple hooks (one for every layer of interest), but we will only keep one for the purpose of this example. . ##### REGISTER HOOK model.global_pool.register_forward_hook(get_features(&quot;feats&quot;)) . &lt;torch.utils.hooks.RemovableHandle at 0x7f2540254290&gt; . Now we are ready to extract features! The nice thing about hooks is that we can now perform inference as we usually would and get multiple outputs at the same time: . outputs of the final layer | outputs of every layer with a registered hook | . The feature extraction happens automatically during the forward pass whenever we run model(inputs). To store intermediate features and concatenate them over batches, we just need to include the following in our inference loop: . Create placeholder list FEATS = []. This list will store intermediate outputs from all batches. | Create placeholder dict features = {}. We will use this dictionary for storing intermediate outputs from each batch. | Iteratively extract batch features to features, send them to CPU and append to the list FEATS. | ##### FEATURE EXTRACTION LOOP # placeholders PREDS = [] FEATS = [] # placeholder for batch features features = {} # loop through batches for idx, inputs in enumerate(data_loader): # move to device inputs = inputs.to(device) # forward pass [with feature extraction] preds = model(inputs) # add feats and preds to lists PREDS.append(preds.detach().cpu().numpy()) FEATS.append(features[&quot;feats&quot;].cpu().numpy()) # early stop if idx == 9: break . This is it! Looking at the shapes of resulting arrays, you can see that the code worked well: we extracted both final layer outputs as PREDS and intermediate activations as FEATS. We can now save these features and work with them further. . ##### INSPECT FEATURES PREDS = np.concatenate(PREDS) FEATS = np.concatenate(FEATS) print(&quot;- preds shape:&quot;, PREDS.shape) print(&quot;- feats shape:&quot;, FEATS.shape) . - preds shape: (320, 2) - feats shape: (320, 512) . 4. Closing words . The purpose of this tutorial was to learn you how to extract intermediate outputs from the most interesting layers of your neural networks. With hooks, you can do all feature extraction in a single inference run and avoid complex modifications of your model. I hope you found this post helpful. . If you are interested, check out my other blog posts to see more tips on deep learning and PyTorch. Happy learning! .",
            "url": "https://kozodoi.me/blog/20210527/extracting-features",
            "relUrl": "/blog/20210527/extracting-features",
            "date": " • May 27, 2021"
        }
        
    
  
    
        ,"post8": {
            "title": "Tracking ML Experiments with Neptune.ai",
            "content": "This post is also published on the Neptune.ai blog. All opinions are my own.* . 1. Introduction . Many ML projects, including Kaggle competitions, have a similar workflow. You start with a simple pipeline with a benchmark model. Next, you begin incorporating improvements: adding features, augmenting the data, tuning the model... On each iteration, you evaluate your solution and keep changes that improve the target metric. . . The figure illustrates the iterative improvement process in ML projects. . This workflow involves running a lot of experiments. As time goes by, it becomes difficult to keep track of the progress and positive changes. Instead of working on new ideas, you spend time thinking: . “have I already tried this thing?”, | “what was that hyperparameter value that worked so well last week?” | . You end up running the same stuff multiple times. If you are not tracking your experiments yet, I highly recommend you to start! In my previous Kaggle projects, I used to rely on spreadsheets for tracking. It worked very well in the beginning, but soon I realized that setting up and managing spreadsheets with experiment meta-data requires loads of additional work. I got tired of manually filling in model parameters and performance values after each experiment and really wanted to switch to an automated solution. . This is when I discovered Neptune.ai. This tool allowed me to save a lot of time and focus on modeling decisions, which helped me to earn three medals in Kaggle competitions. . In this post, I will share my story of switching from spreadsheets to Neptune for experiment tracking. I will describe a few disadvantages of spreadsheets, explain how Neptune helps to address them, and give a couple of tips on using Neptune for Kaggle. . 2. What is wrong with spreadsheets for experiment tracking? . Spreadsheets are great for many purposes. To track experiments, you can simply set up a spreadsheet with different columns containing the relevant parameters and performance of your pipeline. It is also easy to share this spreadsheet with teammates. . Sounds great, right? . Unfortunately, there are a few problems with this. . . The figure illustrates ML experiment tracking with spreadsheets. . Manual work . After doing it for a while, you will notice that maintaining a spreadsheet starts eating too much time. You need to manually fill in a row with meta-data for each new experiment and add a column for each new parameter. This will get out of control once your pipeline becomes more sophisticated. . It is also very easy to make a typo, which can lead to bad decisions. . When working on one deep learning competition, I incorrectly entered a learning rate in one of my experiments. Looking at the spreadsheet, I concluded that a high learning rate decreases the accuracy and went on working on other things. It was only a few days later when I realized that there was a typo and poor performance actually comes from a low learning rate. This cost me two days of work invested in the wrong direction based on a false conclusion. . No live tracking . With spreadsheets, you need to wait until an experiment is completed in order to record the performance. . Apart from being frustrated to do it manually every time, this also does not allow you to compare intermediate results across the experiments, which is helpful to see if a new run looks promising. . Of course, you can log in model performance after every epoch, but doing it manually for each experiment requires even more time and effort. I never had enough diligence to do it regularly and ended up spending some computing resources not optimally. . Attachment limitations . Another issue with spreadsheets is that they only support textual meta-data that can be entered in a cell. . What if you want to attach other meta-data like: . model weights, | source code, | plots with model predictions, | input data version? | . You need to manually store this stuff in your project folders outside of the spreadsheet. . In practice, it gets complicated to organize and sync experiment outputs between local machines, Google Colab, Kaggle Notebooks, and other environments your teammates might use. Having such meta-data attached to a tracking spreadsheet seems useful, but it is very difficult to do it. . 3. Switching from spreadsheets to Neptune . A few months ago, our team was working on a Cassava Leaf Disease competition and used Google spreadsheets for experiment tracking. One month into the challenge, our spreadsheet was already cluttered: . Some runs were missing performance because one of us forgot to log it in and did not have the results anymore. | PDFs with loss curves were scattered over Google Drive and Kaggle Notebooks. | Some parameters might have been entered incorrectly, but it was too time-consuming to restore and double-check older script versions. | . It was difficult to make good data-driven decisions based on our spreadsheet. . Even though there were only four weeks left, we decided to switch to Neptune. I was surprised to see how little effort it actually took us to set it up. In brief, there are three main steps: . sign up for a Neptune account and create a project, | install the neptune package in your environment, | include several lines in the pipeline to enable logging of relevant meta-data. | . You can read more about the exact steps to start using Neptune here. Of course, going through the documentation and getting familiar with the platform may take you a few hours. But remember that this is only a one-time investment. After learning the tool once, I was able to automate much of the tracking and rely on Neptune in the next Kaggle competitions with very little extra effort . 4. What is good about Neptune? . The figure illustrates ML experiment tracking with Neptune. . Less manual work . One of the key advantages of Neptune over spreadsheets is that it saves you a lot of manual work. With Neptune, you use the API within the pipeline to automatically upload and store meta-data while the code is running. . import neptune.new as neptune run = neptune.init(project = &#39;#&#39;, api_token = &#39;#&#39;) # your credentials # Track relevant parameters config = { &#39;batch_size&#39;: 64, &#39;learning_rate&#39;: 0.001, &#39;optimizer&#39;: &#39;Adam&#39; } run[&#39;parameters&#39;] = config # Track the training process by logging your training metrics for epoch in range(100): run[&#39;train/accuracy&#39;].log(epoch * 0.6) # Log the final results run[&#39;f1_score&#39;] = 0.66 . You don’t have to manually put it in the results table, and you also save yourself from making a typo. Since the meta-data is sent to Neptune directly from the code, you will get all numbers right no matter how many digits they have. . It may sound like a small thing, but the time saved from logging in each experiment accumulates very quickly and leads to tangible gains by the end of the project. This gives you an opportunity to not think too much about the actual tracking process and better focus on the modeling decisions. In a way, this is like hiring an assistant to take care of some boring (but very useful) logging tasks so that you can focus more on the creative work. . Live tracking . What I like a lot about Neptune is that it allows you to do live tracking. If you work with models like neural networks or gradient boosting that require a lot of iterations before convergence, you know it is quite useful to look at the loss dynamics early to detect issues and compare models. . Tracking intermediate results in a spreadsheet is too frustrating. Neptune API can log in performance after every epoch or even every batch so that you can start comparing the learning curves while your experiment is still running. . . This proves to be very helpful. As you might expect, many ML experiments have negative results (sorry, but this great idea you were working on for a few days actually decreases the accuracy). . This is completely fine because this is how ML works. . What is not fine is that you may need to wait a long time until getting that negative signal from your pipeline. Using Neptune dashboard to compare the intermediate plots with the first few performance values may be enough to realize that you need to stop the experiment and change something. . Attaching outputs . Another advantage of Neptune is the ability to attach pretty much anything to every experiment run. This really helps to keep important outputs such as model weights and predictions in one place and easily access them from your experiments table. . This is particularly helpful if you and your colleagues work in different environments and have to manually upload the outputs to sync the files. . I also like the ability to attach the source code to each run to make sure you have the notebook version that produced the corresponding result. This can be very useful in case you want to revert some changes that did not improve the performance and would like to go back to the previous best version. . 4. Tips to improve Kaggle performance with Neptune . When working on Kaggle competitions, there are a few tips I can give you to further improve your tracking experience. . Using Neptune in Kaggle Notebooks or Google Colab . First, Neptune is very helpful for working in Kaggle Notebooks or Google Colab that have session time limits when using GPU/TPU. I can not count how many times I lost all experiment outputs due to a notebook crash when training was taking just a few minutes more than the allowed 9-hour limit! . To avoid that, I would highly recommend setting up Neptune such that model weights and loss metrics are stored after each epoch. That way, you will always have a checkpoint uploaded to Neptune servers to resume your training even if your Kaggle notebook times out. You will also have an opportunity to compare your intermediate results before the session crash with other experiments to judge their potential. . Updating runs with the Kaggle leaderboard score . Second, an important metric to track in Kaggle projects is the leaderboard score. With Neptune, you can track your cross-validation score automatically but getting the leaderboard score inside the code is not possible since it requires you to submit predictions via the Kaggle website. . The most convenient way to add the leaderboard score of your experiment to the Neptune tracking table is to use the &quot;resume run&quot; functionality. It allows you to update any finished experiment with a new metric with a couple of lines of code. This feature is also helpful to resume tracking crashed sessions, which we discussed in the previous paragraph. . import neptune.new as neptune run = neptune.init(project = &#39;Your-Kaggle-Project&#39;, run = &#39;SUN-123&#39;) # Add a new metric run[“LB_score”] = 0.5 # Download snapshot of model weights model = run[&#39;train/model_weights&#39;].download() # Continue working . Downloading experiment meta-data . Finally, I know that many Kagglers like to perform complex analyses of their submissions, like estimating the correlation between CV and LB scores or plotting the best score dynamics with respect to time. . While it is not yet feasible to do such things on the website, Neptune allows you to download meta-data from all experiments directly into your notebook using a single API call. It makes it easy to take a deeper dive into the results or export the meta-data table and share it externally with people who use a different tracking tool or don’t rely on any experiment tracking. . import neptune.new as neptune my_project = neptune.get_project(&#39;Your-Workspace/Your-Kaggle-Project&#39;) # Get dashboard with runs contributed by &#39;sophia&#39; sophia_df = my_project.fetch_runs_table(owner = &#39;sophia&#39;).to_pandas() sophia_df.head() . 5. Final thoughts . In this post, I shared my story of switching from spreadsheets to Neptune for tracking ML experiments and emphasized some advantages of Neptune. I would like to stress once again that investing time in infrastructure tools - be it experiment tracking, code versioning, or anything else - is always a good decision and will likely pay off with the increased productivity. Tracking experiment meta-data with spreadsheets is much better than not doing any tracking. It will help you to better see your progress, understand what modifications improve your solution, and help make modeling decisions. Doing it with spreadsheets will also cost you some additional time and effort. Tools like Neptune take the experiment tracking to a next level, allowing you to automate the meta-data logging and focus on the modeling decisions. . I hope you find my story useful. Good luck with your future ML projects! .",
            "url": "https://kozodoi.me/blog/20210430/tracking-experiments",
            "relUrl": "/blog/20210430/tracking-experiments",
            "date": " • Apr 30, 2021"
        }
        
    
  
    
        ,"post9": {
            "title": "Computing Mean & STD in Image Dataset",
            "content": "Last update: 16.10.2021. All opinions are my own. . 1. Overview . In computer vision, it is recommended to normalize image pixel values relative to the dataset mean and standard deviation. This helps to get consistent results when applying a model to new images and can also be useful for transfer learning. In practice, computing these statistics can be a little non-trivial since we usually can&#39;t load the whole dataset in memory and have to loop through it in batches. . This blog post provides a quick tutorial on computing dataset mean and std within RGB channels using a regular PyTorch dataloader. While computing mean is easy (we can simply average means over batches), standard deviation is a bit more tricky: averaging STDs across batches is not the same as the overall STD. Let&#39;s see how to do it properly! . 2. Preparations . To demonstrate how to compute image stats, we will use data from Cassava Leaf Disease Classification Kaggle competition with about 21,000 plant images. Feel free to scroll down to Section 3 to jump directly to calculations. . First, we will import the usual libraries and specify relevant parameters. No need to use GPU because there is no modeling involved. . # collapse-hide ####### PACKAGES import numpy as np import pandas as pd import torch import torchvision from torch.utils.data import Dataset, DataLoader import albumentations as A from albumentations.pytorch import ToTensorV2 import cv2 from tqdm import tqdm import matplotlib.pyplot as plt %matplotlib inline ####### PARAMS device = torch.device(&quot;cpu&quot;) num_workers = 4 image_size = 512 batch_size = 8 data_path = &quot;/kaggle/input/cassava-leaf-disease-classification/&quot; . . Now, let&#39;s import a dataframe with image paths and create a Dataset class that will read images and supply them to the dataloader. . # collapse-show df = pd.read_csv(data_path + &quot;train.csv&quot;) df.head() . . image_id label . 0 1000015157.jpg | 0 | . 1 1000201771.jpg | 3 | . 2 100042118.jpg | 1 | . 3 1000723321.jpg | 1 | . 4 1000812911.jpg | 3 | . # collapse-show class LeafData(Dataset): def __init__(self, data, directory, transform=None): self.data = data self.directory = directory self.transform = transform def __len__(self): return len(self.data) def __getitem__(self, idx): # import path = os.path.join(self.directory, self.data.iloc[idx][&quot;image_id&quot;]) image = cv2.imread(path, cv2.COLOR_BGR2RGB) # augmentations if self.transform is not None: image = self.transform(image=image)[&quot;image&quot;] return image . . We want to compute stats for raw images, so our data augmentation pipeline should be minimal and not include any heavy transformations we might use during training. Below, we use A.Normalize() with mean = 0 and std = 1 to scale pixel values from [0, 255] to [0, 1] and ToTensorV2() to convert numpy arrays into torch tensors. . # collapse-show augs = A.Compose( [ A.Resize(height=image_size, width=image_size), A.Normalize(mean=(0, 0, 0), std=(1, 1, 1)), ToTensorV2(), ] ) . . Let&#39;s check if our code works correctly. We define a DataLoader to load images in batches from LeafData and plot the first batch. . ####### EXAMINE SAMPLE BATCH # dataset image_dataset = LeafData(data=df, directory=data_path + &quot;train_images/&quot;, transform=augs) # data loader image_loader = DataLoader( image_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True, ) # display images for batch_idx, inputs in enumerate(image_loader): fig = plt.figure(figsize=(14, 7)) for i in range(8): ax = fig.add_subplot(2, 4, i + 1, xticks=[], yticks=[]) plt.imshow(inputs[i].numpy().transpose(1, 2, 0)) break . Looks like everithing is working correctly! Now we can use our image_loader to compute image stats. . 3. Computing image stats . The computation is done in three steps: . Define placeholders to store two batch-level stats: sum and squared sum of pixel values. The first will be used to compute means, and the latter will be needed for standard deviation calculations. | Loop through the batches and add up channel-specific sum and squared sum values. | Perform final calculations to obtain data-level mean and standard deviation. | The first two steps are done in the snippet below. Note that we set axis = [0, 2, 3] to compute mean values with respect to axis 1. The dimensions of inputs is [batch_size x 3 x image_size x image_size], so we need to make sure we aggregate values per each RGB channel separately. . ####### COMPUTE MEAN / STD # placeholders psum = torch.tensor([0.0, 0.0, 0.0]) psum_sq = torch.tensor([0.0, 0.0, 0.0]) # loop through images for inputs in tqdm(image_loader): psum += inputs.sum(axis=[0, 2, 3]) psum_sq += (inputs**2).sum(axis=[0, 2, 3]) . 100%|██████████| 2675/2675 [04:21&lt;00:00, 10.23it/s] . Finally, we make some further calculations: . mean: simply divide the sum of pixel values by the total count - number of pixels in the dataset computed as len(df) * image_size * image_size | standard deviation: use the following equation: total_std = sqrt(psum_sq / count - total_mean ** 2) | . Why we use such a weird formula for STD? Well, because this is how the variance equation can be simplified to make use of the sum of squares when other data is not available. If you are not sure about this, expand the cell below to see a calculation example or read this for some details. . . #collapse-hide # Consider three vectors: A = [1, 1] B = [2, 2] C = [1, 1, 2, 2] # Let&#39;s compute SDs in a classical way: 1. Mean(A) = 1; Mean(B) = 2; Mean(C) = 1.5 2. SD(A) = SD(B) = 0 # because there is no variation around the means 3. SD(C) = sqrt(1/4 * ((1 - 1.5)**2 + (1 - 1.5)**2 + (1 - 1.5)**2 + (1 - 1.5)**2)) = 1/2 # Note that SD(C) is clearly not equal to SD(A) + SD(B), which is zero. # Instead, we could compute SD(C) in three steps using the equation above: 1. psum = 1 + 1 + 2 + 2 = 6 2. psum_sq = (1**2 + 1**2 + 2**2 + 2**2) = 10 3. SD(C) = sqrt((psum_sq - 1/N * psum**2) / N) = sqrt((10 - 36 / 4) / 4) = sqrt(1/4) = 1/2 # We get the same result as in the classical way! . . ####### FINAL CALCULATIONS # pixel count count = len(df) * image_size * image_size # mean and std total_mean = psum / count total_var = (psum_sq / count) - (total_mean**2) total_std = torch.sqrt(total_var) # output print(&quot;mean: &quot; + str(total_mean)) print(&quot;std: &quot; + str(total_std)) . mean: tensor([0.4417, 0.5110, 0.3178]) std: tensor([0.2330, 0.2358, 0.2247]) . This is it! Now you can plug in the mean and std values to A.Normalize() in your data augmentation pipeline to make sure your dataset is normalized :) . 4. Closing words . I hope this tutorial was helpful for those looking for a quick guide on computing the image dataset stats. From my experience, normalizing images with respect to the data-level mean and std does not always help to improve the performance, but it is one of the things I always try first. Happy learning and stay tuned for the next posts! .",
            "url": "https://kozodoi.me/blog/20210308/compute-image-stats",
            "relUrl": "/blog/20210308/compute-image-stats",
            "date": " • Mar 8, 2021"
        }
        
    
  
    
        ,"post10": {
            "title": "Gradient Accumulation in PyTorch",
            "content": "Last update: 15.10.2021. All opinions are my own. . 1. Overview . Deep learning models are getting bigger and bigger. It becomes difficult to fit such networks in the GPU memory. This is especially relevant in computer vision applications where we need to reserve some memory for high-resolution images as well. As a result, we are sometimes forced to use small batches during training, which may lead to a slower convergence and lower accuracy. . This blog post provides a quick tutorial on how to increase the effective batch size by using a trick called gradient accumulation. Simply speaking, gradient accumulation means that we will use a small batch size but save the gradients and update network weights once every couple of batches. Automated solutions for this exist in higher-level frameworks such as fast.ai or lightning, but those who love using PyTorch might find this tutorial useful. . 2. What is gradient accumulation . When training a neural network, we usually divide our data in mini-batches and go through them one by one. The network predicts batch labels, which are used to compute the loss with respect to the actual targets. Next, we perform backward pass to compute gradients and update model weights in the direction of those gradients. . Gradient accumulation modifies the last step of the training process. Instead of updating the network weights on every batch, we can save gradient values, proceed to the next batch and add up the new gradients. The weight update is then done only after several batches have been processed by the model. . Gradient accumulation helps to imitate a larger batch size. Imagine you want to use 32 images in one batch, but your hardware crashes once you go beyond 8. In that case, you can use batches of 8 images and update weights once every 4 batches. If you accumulate gradients from every batch in between, the results will be (almost) the same and you will be able to perform training on a less expensive machine! . 3. How to make it work . The implementation of gradient accumulation is rather straightforward. The standard training loop without accumulation usually looks like this: . # loop through batches for inputs, labels in data_loader: # extract inputs and labels inputs = inputs.to(device) labels = labels.to(device) # passes and weights update with torch.set_grad_enabled(True): # forward pass preds = model(inputs) loss = criterion(preds, labels) # backward pass loss.backward() # weights update optimizer.step() optimizer.zero_grad() . Now let&#39;s implement gradient accumulation! There are three things we need to do: . Specify the accum_iter parameter. This is just an integer value indicating once in how many batches we would like to update the network weights. | Condition the weight update on the index of the running batch. This requires using enumerate(data_loader) to store the batch index when looping through the data. | Divide the running loss by acum_iter. This normalizes the loss to reduce the contribution of each mini-batch we are actually processing. Depending on the way you compute the loss, you might not need this step: if you average loss within each batch, the division is already correct and there is no need for extra normalization. | # batch accumulation parameter accum_iter = 4 # loop through enumaretad batches for batch_idx, (inputs, labels) in enumerate(data_loader): # extract inputs and labels inputs = inputs.to(device) labels = labels.to(device) # passes and weights update with torch.set_grad_enabled(True): # forward pass preds = model(inputs) loss = criterion(preds, labels) # normalize loss to account for batch accumulation loss = loss / accum_iter # backward pass loss.backward() # weights update if ((batch_idx + 1) % accum_iter == 0) or (batch_idx + 1 == len(data_loader)): optimizer.step() optimizer.zero_grad() . It is really that simple! The gradients are computed when we call loss.backward() and are stored by PyTorch until we call optimizer.zero_grad(). Therefore, we just need to move the weight update performed in optimizer.step() and the gradient reset under the if condition that check the batch index. It is important to also update weights on the last batch when batch_idx + 1 == len(data_loader) - this makes sure that data from the last batches are not discarded and used for optimizing the network. . Please also note that some network architectures have batch-specific operations. For instance, batch normalization is performed on a batch level and therefore may yield slightly different results when using the same effective batch size with and without gradient accumulation. This means that you should not expect to see a 100% match between the results. . In my experience, the potential performance gains from increasing the number of cases used to update the network weights are largest when one is forced to use very small batches (e.g., 8 or 10). Therefore, I always recommend using gradient accumulation when working with large architectures that consume a lof of GPU memory. . 4. Closing words . This is it! I hope this brief tutorial helps you to finally fit that model on your machine and train it with the batch size it deserves. If you are interested, check out my other blog posts on tips on deep learning and PyTorch. Happy learning! .",
            "url": "https://kozodoi.me/blog/20210219/gradient-accumulation",
            "relUrl": "/blog/20210219/gradient-accumulation",
            "date": " • Feb 19, 2021"
        }
        
    
  
    
        ,"post11": {
            "title": "Training PyTorch Models on TPU",
            "content": "Last update: 19.10.2021. All opinions are my own. . 1. Overview . Deep learning heavily relies on Graphical Processing Units (GPUs) to enable fast training. Recently, Google introduced Tensor Processing Units (TPUs) to further advance the speed of computations used in neural networks. Using cloud TPUs is possible on Kaggle and Google Colab. While TPU chips have been optimized for TensorFlow, PyTorch users can also take advantage of the better compute. This requires using PyTorch/XLA and implementing certain changes in the modeling pipeline. . Moving a PyTorch pipeline to TPU includes the following steps: . installing relevant packages ans setting up TPU | adjusting syntax of some modeling steps such as initialization, optimizer and verbosity | distributing data loaders over multiple TPU cores | wrapping data processing, training and inference into a master function | . This post provides a tutorial on using PyTorch/XLA to build the TPU pipeline. The code is optimized for multi-core TPU training. Many of the ideas are adapted from here and here. We will focus on a computer vision application, but the framework can be used with other deep learning models as well. We will use data from RSNA STR Pulmonary Embolism Detection Kaggle competition on detecting pulmonary embolism on more than 1.7 million CT scans. . 2. Preparations and packages . When setting up a script, it is important to introduce two TPU-related parameters: batch size and number of workers. . Google recommends using 128 images per batch for the best performance on the current TPU v3 chips. The v3 chips have 8 cores. This implies that each of the 8 cores can receive a batch of 128 images at each training step, and the modeling can be performed simultaneously on the separate cores. The model weights are then updated based on the outcomes observed on each core. Therefore, the batch size of 128 actually implies 128 * 8 images in each iteration. . #collapse-show # partitioning num_folds = 5 use_fold = 0 # image params image_size = 128 # modeling batch_size = 128 # num_images = batch_size*num_tpu_workers batches_per_epoch = 1000 # num_images = batch_size*batches_per_epoch*num_tpu_workers num_epochs = 1 batch_verbose = 100 num_tpu_workers = 8 # learning rate eta = 0.0001 step = 1 gamma = 0.5 # paths data_path = &#39;/kaggle/input/rsna-str-pulmonary-embolism-detection/&#39; image_path = &#39;/kaggle/input/rsna-str-pe-detection-jpeg-256/train-jpegs/&#39; . . After specifying the parameters, we need to set up TPU by installing and importing torch_xla using the snippet below. There are two options: install the last stable XLA version (1.7 as of 30.03.2021) or the so-called &#39;nightly&#39; version that includes the latest updates but may be unstable. I recommend going for the stable version. . We also specify XLA_USE_BF16 variable (default tensor precision format) and XLA_TENSOR_ALLOCATOR_MAXSIZE variable (maximum tensor allocator size). When working in Google Colab we can also run assert os.environ[&#39;COLAB_TPU_ADDR&#39;] to check that Colab is correctly connected to a TPU instance. . Don&#39;t be discouraged if you see error messages during the installation of fastai, kornia and allennlp. The installation would still proceed to the required versions of torch and torchvision needed to work with TPUs. This may take a few minutes. . # XLA version xla_version = &#39;1.7&#39; # &#39;nightly&#39; or &#39;1.7&#39; # installation !curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py !python pytorch-xla-env-setup.py --verion $xla_version # XLA imports import torch_xla import torch_xla.debug.metrics as met import torch_xla.distributed.data_parallel as dp import torch_xla.distributed.parallel_loader as pl import torch_xla.utils.utils as xu import torch_xla.core.xla_model as xm import torch_xla.distributed.xla_multiprocessing as xmp import torch_xla.test.test_utils as test_utils # configurations import os os.environ[&#39;XLA_USE_BF16&#39;] = &#39;1&#39; os.environ[&#39;XLA_TENSOR_ALLOCATOR_MAXSIZE&#39;] = &#39;1000000000&#39; . Running on TPU [&#39;10.0.0.2:8470&#39;] % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 5115 100 5115 0 0 9199 0 --:--:-- --:--:-- --:--:-- 9183 Updating... This may take around 2 minutes. Updating TPU runtime to pytorch-dev20200515 ... ... Setting up libopenblas-dev:amd64 (0.2.20+ds-4) ... update-alternatives: using /usr/lib/x86_64-linux-gnu/openblas/libblas.so to provide /usr/lib/x86_64-linux-gnu/libblas.so (libblas.so-x86_64-linux-gnu) in auto mode update-alternatives: using /usr/lib/x86_64-linux-gnu/openblas/liblapack.so to provide /usr/lib/x86_64-linux-gnu/liblapack.so (liblapack.so-x86_64-linux-gnu) in auto mode Processing triggers for libc-bin (2.27-3ubuntu1) ... . Next, we import all other relevant libraries. . #collapse-hide import numpy as np import pandas as pd import torch import torchvision import torch.nn as nn import torch.optim as optim from torch.optim import lr_scheduler from torchvision import transforms, models, datasets from torch.utils.data import Dataset from PIL import Image, ImageFile ImageFile.LOAD_TRUNCATED_IMAGES = True import cv2 from sklearn.model_selection import GroupKFold import glob import random import time import sys import os import gc import matplotlib.pyplot as plt import seaborn as sns %matplotlib inline import warnings warnings.filterwarnings(&#39;ignore&#39;) . . 3. Data preparation . Mostly, the data processing pipeline does not need adjustments when training on TPU instead of GPU. It is only necessary to change the data loaders such that they distribute image batches over the TPU cores. This is covered in the next section that overviews the modeling stage, since the data samplers need to be wrapped into the modeling function. Feel free to skip this section if you already know how to process the image data. . It is important to note that given a more efficient training, data import usually becomes a computational bottleneck. Hence, it is crucial to optimize data reading/processing as much as possible. For the best read performance, I recommend transforming the data to .tfrec format. You can read more about using .tfrec with PyTorch here. . Below, we construct a standard Dataset class to read JPG images of the CT scans. Each image has ten binary labels indicating the presence of pulmonary embolism and its characteristics. . #collapse-hide label_names = [&#39;pe_present_on_image&#39;, &#39;negative_exam_for_pe&#39;, &#39;rv_lv_ratio_gte_1&#39;, &#39;rv_lv_ratio_lt_1&#39;, &#39;leftsided_pe&#39;, &#39;chronic_pe&#39;, &#39;rightsided_pe&#39;, &#39;acute_and_chronic_pe&#39;, &#39;central_pe&#39;, &#39;indeterminate&#39;] . . #collapse-show ### DATASET class PEData(Dataset): def __init__(self, data, directory, transform = None, load_jpg = False, labeled = False): self.data = data self.directory = directory self.transform = transform self.load_jpg = load_jpg self.labeled = labeled def __len__(self): return len(self.data) def __getitem__(self, idx): # import img_name = glob.glob(os.path.join(self.directory, &#39;/&#39;.join(self.data.iloc[idx][[&#39;StudyInstanceUID&#39;, &#39;SeriesInstanceUID&#39;]]) + &#39;/*&#39; + self.data.iloc[idx][&#39;SOPInstanceUID&#39;] + &#39;.jpg&#39;))[0] image = cv2.imread(img_name) # switch channels and normalize image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) image = image / 255.0 # convert image = torch.tensor(image, dtype = torch.float) image = image.permute(2, 0, 1) # augmentations image = self.transform(image) # output if self.labeled: labels = torch.tensor(self.data.iloc[idx][label_names].values.astype(&#39;int&#39;), dtype = torch.float) return image, labels else: return image ### AUGMENTATIONS train_trans = test_trans = transforms.Compose([transforms.ToPILImage(), transforms.Resize(image_size), transforms.ToTensor()]) . . We split data into training and validation folds such that images from the same patient - StudyInstanceUID - do not appear in both. We will only use a single fold for demonstration purposes. . #collapse-hide # partitioning train = pd.read_csv(data_path + &#39;train.csv&#39;) gkf = GroupKFold(n_splits = num_folds) train[&#39;fold&#39;] = -1 for fold, (_, val_idx) in enumerate(gkf.split(train, groups = train[&#39;StudyInstanceUID&#39;])): train.loc[val_idx, &#39;fold&#39;] = fold # load splits data_train = train.loc[train.fold != use_fold].reset_index(drop = True) data_valid = train.loc[train.fold == use_fold].reset_index(drop = True) # datasets train_dataset = PEData(data = data_train, directory = image_path, transform = train_trans, load_jpg = load_jpegs, labeled = True) valid_dataset = PEData(data = data_valid, directory = image_path, transform = test_trans, load_jpg = load_jpegs, labeled = True) . . Before proceeding to modeling, let&#39;s take a look at a sample batch of training images using our processing pipeline. . #collapse-show # sample loader sample_loader = torch.utils.data.DataLoader(valid_dataset, shuffle = False, batch_size = 8, num_workers = 1) # display images for batch_idx, (inputs, labels) in enumerate(sample_loader): fig = plt.figure(figsize = (14, 7)) for i in range(8): ax = fig.add_subplot(2, 4, i + 1, xticks = [], yticks = []) plt.imshow(inputs[i].numpy().transpose(1, 2, 0)) ax.set_title(labels.numpy()[:, i]) break . . 4. Model setup . The modeling stage needs to be modified because the modeling is performed simultaneously on multiple TPU cores. This requires changes to model initialization, optimizer and building a master function to distribute data loaders, training and inference over multi-core TPU chips. Let&#39;s dive in! . We start with the model. We use ResNet-34 with ten output nodes corresponding to each of the binary labels. After initializing the model, we need to wrap it into the MX object that can be sent to TPU. This is done by a simple command mx = xmp.MpModelWrapper(model). . # initialization def init_model(): model = models.resnet34(pretrained = True) model.fc = torch.nn.Linear(in_features = 512, out_features = len(label_names), bias = True) return model # model wrapper model = init_model() mx = xmp.MpModelWrapper(model) . Tracking the running loss when training on multiple TPU cores can a bit difficult since we need to aggregate batch losses between the TPU cores. The following helper class allows to externally store the loss values and update it based on the batch outputs from each worker. . class AverageMeter(object): &#39;&#39;&#39;Computes and stores the average and current value&#39;&#39;&#39; def __init__(self): self.reset() def reset(self): self.val = 0 self.avg = 0 self.sum = 0 self.count = 0 def update(self, val, n = 1): self.val = val self.sum += val * n self.count += n self.avg = self.sum / self.count . Next, we need to wrap modeling into a single master function that can be distributed over TPU cores. We will first define functions for training and inference and then introduce the wrapper function. . The training pass must have several steps: . the optimizer step is done with xm.optimizer_step(optimizer) | the printing statements need to be defined as xm.master_print() instead of print() in order to only print a statement once (otherwise, each TPU core will print it) | dataloader should be defined outside of the function and read as an argument to distribute it over the cores | running loss can be computed using the defined AverageMeter() object | . In addition, it is important to clear the TPU memory as often as possible to ensure that the modeling does not crash: . del [object] to delete objects once they are not needed | gc.collect() to collect garbage left in memory | . #collapse-show ### TRAINING def train_fn(epoch, para_loader, optimizer, criterion, scheduler, device): # initialize model.train() trn_loss_meter = AverageMeter() # training loop for batch_idx, (inputs, labels) in enumerate(para_loader): # extract inputs and labels inputs = inputs.to(device) labels = labels.to(device) optimizer.zero_grad() # forward and backward pass preds = model(inputs) loss = criterion(preds, labels) loss.backward() xm.optimizer_step(optimizer, barrier = True) # barrier is required on single-core training but can be dropped with multiple cores # compute loss trn_loss_meter.update(loss.detach().item(), inputs.size(0)) # feedback if (batch_idx &gt; 0) and (batch_idx % batch_verbose == 0): xm.master_print(&#39;-- batch {} | cur_loss = {:.6f}, avg_loss = {:.6f}&#39;.format( batch_idx, loss.item(), trn_loss_meter.avg)) # clear memory del inputs, labels, preds, loss gc.collect() # early stop if batch_idx &gt; batches_per_epoch: break # scheduler step scheduler.step() # clear memory del para_loader, batch_idx gc.collect() return trn_loss_meter.avg . . Similar to the training pass, inference function uses dataloader as an argument, updates loss using the AverageMeter() object and clears memory after each batch. . #collapse-show ### INFERENCE def valid_fn(epoch, para_loader, criterion, device): # initialize model.eval() val_loss_meter = AverageMeter() # validation loop for batch_idx, (inputs, labels) in enumerate(para_loader): # extract inputs and labels inputs = inputs.to(device) labels = labels.to(device) # compute preds with torch.no_grad(): preds = model(inputs) loss = criterion(preds, labels) # compute loss val_loss_meter.update(loss.detach().item(), inputs.size(0)) # feedback if (batch_idx &gt; 0) and (batch_idx % batch_verbose == 0): xm.master_print(&#39;-- batch {} | cur_loss = {:.6f}, avg_loss = {:.6f}&#39;.format( batch_idx, loss.item(), val_loss_meter.avg)) # clear memory del inputs, labels, preds, loss gc.collect() # clear memory del para_loader, batch_idx gc.collect() return val_loss_meter.avg . . The master modeling function also includes several TPU-based modifications. . First, we need to create a distributed data sampler that reads our Dataset object and distributes batches over TPU cores. This is done with torch.utils.data.distributed.DistributedSampler(), which allows data loaders from different cores to only take a portion of the whole dataset. Setting num_replicas to xm.xrt_world_size() checks the number of available TPU cores. After defining the sampler, we can set up a data loader that uses the sampler. . Second, the model is sent to TPU with the following code: . device = xm.xla_device() model = mx.to(device) . Third, we need to update learning rate since the modeling is done simultaneously on batches on different cores: scaled_eta = eta * xm.xrt_world_size(). . Finally, we continue keeping track of the memory and clearing it whenever possible, and use xm.master_print() for displaying intermediate results. We also set up the function to return lists with the training and validation loss values. . #collapse-show ### MASTER FUNCTION def _run(model): ### DATA PREP # data samplers train_sampler = torch.utils.data.distributed.DistributedSampler(train_dataset, num_replicas = xm.xrt_world_size(), rank = xm.get_ordinal(), shuffle = True) valid_sampler = torch.utils.data.distributed.DistributedSampler(valid_dataset, num_replicas = xm.xrt_world_size(), rank = xm.get_ordinal(), shuffle = False) # data loaders valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size = batch_size, sampler = valid_sampler, num_workers = 0, pin_memory = True) train_loader = torch.utils.data.DataLoader(train_dataset, batch_size = batch_size, sampler = train_sampler, num_workers = 0, pin_memory = True) ### MODEL PREP # send to TPU device = xm.xla_device() model = mx.to(device) # scale LR scaled_eta = eta * xm.xrt_world_size() # optimizer and loss criterion = nn.BCEWithLogitsLoss() optimizer = optim.Adam(model.parameters(), lr = scaled_eta) scheduler = lr_scheduler.StepLR(optimizer, step_size = step, gamma = gamma) ### MODELING # placeholders trn_losses = [] val_losses = [] best_val_loss = 1 # modeling loop gc.collect() for epoch in range(num_epochs): # display info xm.master_print(&#39;-&#39;*55) xm.master_print(&#39;EPOCH {}/{}&#39;.format(epoch + 1, num_epochs)) xm.master_print(&#39;-&#39;*55) xm.master_print(&#39;- initialization | TPU cores = {}, lr = {:.6f}&#39;.format( xm.xrt_world_size(), scheduler.get_lr()[len(scheduler.get_lr()) - 1] / xm.xrt_world_size())) epoch_start = time.time() gc.collect() # update train_loader shuffling train_loader.sampler.set_epoch(epoch) # training pass train_start = time.time() xm.master_print(&#39;- training...&#39;) para_loader = pl.ParallelLoader(train_loader, [device]) trn_loss = train_fn(epoch = epoch + 1, para_loader = para_loader.per_device_loader(device), criterion = criterion, optimizer = optimizer, scheduler = scheduler, device = device) del para_loader gc.collect() # validation pass valid_start = time.time() xm.master_print(&#39;- validation...&#39;) para_loader = pl.ParallelLoader(valid_loader, [device]) val_loss = valid_fn(epoch = epoch + 1, para_loader = para_loader.per_device_loader(device), criterion = criterion, device = device) del para_loader gc.collect() # save weights if val_loss &lt; best_val_loss: xm.save(model.state_dict(), &#39;weights_{}.pt&#39;.format(model_name)) best_val_loss = val_loss # display info xm.master_print(&#39;- elapsed time | train = {:.2f} min, valid = {:.2f} min&#39;.format( (valid_start - train_start) / 60, (time.time() - valid_start) / 60)) xm.master_print(&#39;- average loss | train = {:.6f}, valid = {:.6f}&#39;.format( trn_loss, val_loss)) xm.master_print(&#39;-&#39;*55) xm.master_print(&#39;&#39;) # save losses trn_losses.append(trn_loss) val_losses.append(val_loss) del trn_loss, val_loss gc.collect() # print results xm.master_print(&#39;Best results: loss = {:.6f} (epoch {})&#39;.format(np.min(val_losses), np.argmin(val_losses) + 1)) return trn_losses, val_losses . . 5. Modeling . After all helper functions have been introduced, we can finally launch the training! To do that, we need to define the last wrapper function that runs the modeling on multiple TPU cores: _mp_fn(rank, flags). Within the wrapper function, we set default tensor type to make sure that new tensors are initialized as float torch tensors on TPU and then run the modeling. It is important to set nprocs to the number of available TPU cores. The FLAGS object can be used to pass further training arguments to the modeling function. . Running xmp.spawn() will launch our _mp_fn() on multiple TPU cores! Since it does not provide any output, it is useful to save losses or other objects you might be interested in after running the _run() function. . # wrapper function def _mp_fn(rank, flags): torch.set_default_tensor_type(&#39;torch.FloatTensor&#39;) trn_losses, val_losses = _run(model) np.save(&#39;trn_losses.npy&#39;, np.array(trn_losses)) np.save(&#39;val_losses.npy&#39;, np.array(val_losses)) # modeling gc.collect() FLAGS = {} xmp.spawn(_mp_fn, args = (FLAGS,), nprocs = num_tpu_workers, start_method = &#39;fork&#39;) . - EPOCH 1/1 - - initialization | TPU cores = 8, lr = 0.000010 - training... -- batch 100 | cur_loss = 0.343750, avg_loss = 0.367342 -- batch 200 | cur_loss = 0.285156, avg_loss = 0.333299 -- batch 300 | cur_loss = 0.241211, avg_loss = 0.311770 -- batch 400 | cur_loss = 0.194336, avg_loss = 0.292837 -- batch 500 | cur_loss = 0.179688, avg_loss = 0.275745 -- batch 600 | cur_loss = 0.180664, avg_loss = 0.260264 -- batch 700 | cur_loss = 0.166016, avg_loss = 0.246196 -- batch 800 | cur_loss = 0.139648, avg_loss = 0.232805 -- batch 900 | cur_loss = 0.085449, avg_loss = 0.220550 -- batch 1000 | cur_loss = 0.109375, avg_loss = 0.209381 - validation... -- batch 100 | cur_loss = 0.184570, avg_loss = 0.353043 - elapsed time | train = 125.82 min, valid = 26.05 min - average loss | train = 0.209277, valid = 0.366887 - Best results: loss = 0.366887 (epoch 1) . The training is working! Note that every 100 batches displayed in the snippet above actually refer to 100 * batch_size * num_tpu_workers images, since every core processes the same amount of different images simultaneously but printing is done just from one core. . 6. Closing words . This is the end of this blog post. Using a computer vision application, we demonstrated how to use PyTorch/XLA to take advantage of TPU when training deep learning models. We covered important changes that need to be implemented into the modeling pipeline to enable TPU-based training, including data processing, modeling and displaying results. I hope this post will help you to get started with TPUs! . If you are interested in further reading, make sure to check tutorial notebooks developed by PyTorch/XLA team available at their GitHub repo. .",
            "url": "https://kozodoi.me/blog/20201030/pytorch-xla-tpu",
            "relUrl": "/blog/20201030/pytorch-xla-tpu",
            "date": " • Oct 30, 2020"
        }
        
    
  
    
        ,"post12": {
            "title": "Pre-Training with Surrogate Labels",
            "content": "Last update: 22.10.2021. All opinions are my own. . 1. Overview . In many real-world settings, the size of the labeled training sample is lower compared to the unlabeled test data. This blogpost demonstrates a technique that can improve the performance of neural networks in such settings by learning from both training and test data. This is done by pre-training a model on the complete data set using a surrogate label. The approach can help to reduce the impact of sampling bias by exposing the model to the test data and benefit from a larger sample size while learning. . We will focus on a computer vision application, but the idea can be used with deep learning models in other domains. We will use data from SIIM-ISIC Melanoma Classification Kaggle competition to distinguish malignant and benign lesions on medical images. The modeling is performed in tensorflow. A shorter and interactive version of this blogpost is also available as a Kaggle notebook. . 2. Intuition . How to make use of the test sample on the pre-training stage? The labels are only observed for the training data. Luckily, in many settings, there is a bunch of meta-data available for both labeled and unlabeled images. Consider the task of lung cancer detection. The CT scans of cancer patients may contain information on the patient&#39;s age and gender. In contrast with the label, which requires medical tests or experts&#39; diagnosis, meta-data is available at no additional cost. Another example is bird image classification, where the image meta-data such as time and location of the photo can serve the same purpose. In this blogpost, we will focus on malignant lesion classification, where patient meta-data is available for all images. . We can leverage meta-data in the following way: . Pre-train a supplementary model on the complete train + test data using one of the meta-features as a surrogate label. | Initialize from the pre-trained weights when training the main model. | The intuition behind this approach is that by learning to classify images according to one of meta variables, the model can learn some of the visual features that might be useful for the main task, which in our case is malignant lesion classification. For instance, lesion size and skin color can be helpful in determining both lesion location (surrogate label) and lesion type (actual label). Exposing the model to the test data also allows it to take a sneak peek at test images, which may help to learn patterns prevalent in the test distribution. . P.S. The notebook heavily relies on the great modeling pipeline developed by Chris Deotte for the SIIM-ISIC competition and reuses much of his original code. Kindly refer to his notebook for general questions on the pipeline where he provided comments and documentation. . 3. Initialization . #collapse-hide ### PACKAGES !pip install -q efficientnet &gt;&gt; /dev/null import pandas as pd, numpy as np from kaggle_datasets import KaggleDatasets import tensorflow as tf, re, math import tensorflow.keras.backend as K import efficientnet.tfkeras as efn from sklearn.model_selection import KFold from sklearn.metrics import roc_auc_score import matplotlib.pyplot as plt from scipy.stats import rankdata import PIL, cv2 . . Let&#39;s set up training parameters such as image size, number of folds and batch size. In addition to these parameters, we introduce USE_PRETRAIN_WEIGHTS variable to reflect whether we want to pre-train a supplementary model on full data before training the main melanoma classification model. . For demonstration purposes, we use EfficientNet B0, 128x128 image size and no TTA. Feel free to experiment with larger architectures and images sizes by editing this notebook. . #collapse-show # DEVICE DEVICE = &quot;TPU&quot; # USE DIFFERENT SEED FOR DIFFERENT STRATIFIED KFOLD SEED = 42 # NUMBER OF FOLDS. USE 3, 5, OR 15 FOLDS = 5 # WHICH IMAGE SIZES TO LOAD EACH FOLD IMG_SIZES = [128]*FOLDS # BATCH SIZE AND EPOCHS BATCH_SIZES = [32]*FOLDS EPOCHS = [10]*FOLDS # WHICH EFFICIENTNET TO USE EFF_NETS = [0]*FOLDS # WEIGHTS FOR FOLD MODELS WHEN PREDICTING TEST WGTS = [1/FOLDS]*FOLDS # PRETRAINED WEIGHTS USE_PRETRAIN_WEIGHTS = True . . Below, we connect to TPU or GPU for faster training. . #collapse-hide # CONNECT TO DEVICE if DEVICE == &quot;TPU&quot;: print(&quot;connecting to TPU...&quot;) try: tpu = tf.distribute.cluster_resolver.TPUClusterResolver() print(&#39;Running on TPU &#39;, tpu.master()) except ValueError: print(&quot;Could not connect to TPU&quot;) tpu = None if tpu: try: print(&quot;initializing TPU ...&quot;) tf.config.experimental_connect_to_cluster(tpu) tf.tpu.experimental.initialize_tpu_system(tpu) strategy = tf.distribute.experimental.TPUStrategy(tpu) print(&quot;TPU initialized&quot;) except _: print(&quot;failed to initialize TPU&quot;) else: DEVICE = &quot;GPU&quot; if DEVICE != &quot;TPU&quot;: print(&quot;Using default strategy for CPU and single GPU&quot;) strategy = tf.distribute.get_strategy() if DEVICE == &quot;GPU&quot;: print(&quot;Num GPUs Available: &quot;, len(tf.config.experimental.list_physical_devices(&#39;GPU&#39;))) AUTO = tf.data.experimental.AUTOTUNE REPLICAS = strategy.num_replicas_in_sync print(f&#39;REPLICAS: {REPLICAS}&#39;) . . connecting to TPU... Running on TPU grpc://10.0.0.2:8470 initializing TPU ... TPU initialized REPLICAS: 8 . 4. Image processing . First, we specify data paths. The data is stored as tfrecords to enable fast processing. You can read more on the data here. . #collapse-show # IMAGE PATHS GCS_PATH = [None]*FOLDS for i,k in enumerate(IMG_SIZES): GCS_PATH[i] = KaggleDatasets().get_gcs_path(&#39;melanoma-%ix%i&#39;%(k,k)) files_train = np.sort(np.array(tf.io.gfile.glob(GCS_PATH[0] + &#39;/train*.tfrec&#39;))) files_test = np.sort(np.array(tf.io.gfile.glob(GCS_PATH[0] + &#39;/test*.tfrec&#39;))) . . The read_labeled_tfrecord() function provides two outputs: . Image tensor. | Either anatom_site_general_challenge or target as a label. The former is a one-hot-encoded categorical feature with six possible values indicating the lesion location. The latter is a binary target indicating whether the lesion is malignant. The selection of the label is controlled by the pretraining argument read from the get_dataset() function below. Setting pretraining = True implies using anatom_site_general_challenge as a surrogate label. | We also set up read_unlabeled_tfrecord() that returns image and image name. . #collapse-show def read_labeled_tfrecord(example, pretraining = False): if pretraining: tfrec_format = { &#39;image&#39; : tf.io.FixedLenFeature([], tf.string), &#39;image_name&#39; : tf.io.FixedLenFeature([], tf.string), &#39;anatom_site_general_challenge&#39;: tf.io.FixedLenFeature([], tf.int64), } else: tfrec_format = { &#39;image&#39; : tf.io.FixedLenFeature([], tf.string), &#39;image_name&#39; : tf.io.FixedLenFeature([], tf.string), &#39;target&#39; : tf.io.FixedLenFeature([], tf.int64) } example = tf.io.parse_single_example(example, tfrec_format) return example[&#39;image&#39;], tf.one_hot(example[&#39;anatom_site_general_challenge&#39;], 6) if pretraining else example[&#39;target&#39;] def read_unlabeled_tfrecord(example, return_image_name=True): tfrec_format = { &#39;image&#39; : tf.io.FixedLenFeature([], tf.string), &#39;image_name&#39; : tf.io.FixedLenFeature([], tf.string), } example = tf.io.parse_single_example(example, tfrec_format) return example[&#39;image&#39;], example[&#39;image_name&#39;] if return_image_name else 0 def prepare_image(img, dim = 256): img = tf.image.decode_jpeg(img, channels = 3) img = tf.cast(img, tf.float32) / 255.0 img = img * circle_mask img = tf.reshape(img, [dim,dim, 3]) return img def count_data_items(filenames): n = [int(re.compile(r&quot;-([0-9]*) .&quot;).search(filename).group(1)) for filename in filenames] return np.sum(n) . . The get_dataset() function is a wrapper function that loads and processes images given the arguments that control the import options. . #collapse-show def get_dataset(files, shuffle = False, repeat = False, labeled = True, pretraining = False, return_image_names = True, batch_size = 16, dim = 256): ds = tf.data.TFRecordDataset(files, num_parallel_reads = AUTO) ds = ds.cache() if repeat: ds = ds.repeat() if shuffle: ds = ds.shuffle(1024*2) #if too large causes OOM in GPU CPU opt = tf.data.Options() opt.experimental_deterministic = False ds = ds.with_options(opt) if labeled: ds = ds = ds.map(lambda example: read_labeled_tfrecord(example, pretraining), num_parallel_calls=AUTO) else: ds = ds.map(lambda example: read_unlabeled_tfrecord(example, return_image_names), num_parallel_calls = AUTO) ds = ds.map(lambda img, imgname_or_label: ( prepare_image(img, dim = dim), imgname_or_label), num_parallel_calls = AUTO) ds = ds.batch(batch_size * REPLICAS) ds = ds.prefetch(AUTO) return ds . . We also use a circular crop (a.k.a. microscope augmentation) to improve image consistency. The snippet below creates a circular mask, which is applied in the prepare_image() function. . #collapse-show # CIRCLE CROP PREPARATIONS circle_img = np.zeros((IMG_SIZES[0], IMG_SIZES[0]), np.uint8) circle_img = cv2.circle(circle_img, (int(IMG_SIZES[0]/2), int(IMG_SIZES[0]/2)), int(IMG_SIZES[0]/2), 1, thickness = -1) circle_img = np.repeat(circle_img[:, :, np.newaxis], 3, axis = 2) circle_mask = tf.cast(circle_img, tf.float32) . . Let&#39;s have a quick look at a batch of our images: . #collapse-hide # LOAD DATA AND APPLY AUGMENTATIONS def show_dataset(thumb_size, cols, rows, ds): mosaic = PIL.Image.new(mode=&#39;RGB&#39;, size=(thumb_size*cols + (cols-1), thumb_size*rows + (rows-1))) for idx, data in enumerate(iter(ds)): img, target_or_imgid = data ix = idx % cols iy = idx // cols img = np.clip(img.numpy() * 255, 0, 255).astype(np.uint8) img = PIL.Image.fromarray(img) img = img.resize((thumb_size, thumb_size), resample = PIL.Image.BILINEAR) mosaic.paste(img, (ix*thumb_size + ix, iy*thumb_size + iy)) nn = target_or_imgid.numpy().decode(&quot;utf-8&quot;) display(mosaic) return nn files_train = tf.io.gfile.glob(GCS_PATH[0] + &#39;/train*.tfrec&#39;) ds = tf.data.TFRecordDataset(files_train, num_parallel_reads = AUTO).shuffle(1024) ds = ds.take(10).cache() ds = ds.map(read_unlabeled_tfrecord, num_parallel_calls = AUTO) ds = ds.map(lambda img, target: (prepare_image(img, dim = IMG_SIZES[0]), target), num_parallel_calls = AUTO) ds = ds.take(12*5) ds = ds.prefetch(AUTO) # DISPLAY IMAGES name = show_dataset(128, 5, 2, ds) . . i# 5. Modeling . Pre-trained model with surrogate label . The build_model() function incorporates three important features that depend on the training regime: . When building a model for pre-training, we use CategoricalCrossentropy as a loss because anatom_site_general_challenge is a categorical variable. When building a model that classifies lesions as benign/malignant, we use BinaryCrossentropy as a loss. | When training a final binary classification model, we load the pre-trained weights using base.load_weights(&#39;base_weights.h5&#39;) if use_pretrain_weights == True. | We use a dense layer with six output nodes and softmax activation when doing pre-training and a dense layer with a single output node and sigmoid activation when training a final model. | #collapse-show EFNS = [efn.EfficientNetB0, efn.EfficientNetB1, efn.EfficientNetB2, efn.EfficientNetB3, efn.EfficientNetB4, efn.EfficientNetB5, efn.EfficientNetB6, efn.EfficientNetB7] def build_model(dim = 256, ef = 0, pretraining = False, use_pretrain_weights = False): # base inp = tf.keras.layers.Input(shape = (dim,dim,3)) base = EFNS[ef](input_shape = (dim,dim,3), weights = &#39;imagenet&#39;, include_top = False) # base weights if use_pretrain_weights: base.load_weights(&#39;base_weights.h5&#39;) x = base(inp) x = tf.keras.layers.GlobalAveragePooling2D()(x) if pretraining: x = tf.keras.layers.Dense(6, activation = &#39;softmax&#39;)(x) model = tf.keras.Model(inputs = inp, outputs = x) opt = tf.keras.optimizers.Adam(learning_rate = 0.001) loss = tf.keras.losses.CategoricalCrossentropy() model.compile(optimizer = opt, loss = loss) else: x = tf.keras.layers.Dense(1, activation = &#39;sigmoid&#39;)(x) model = tf.keras.Model(inputs = inp, outputs = x) opt = tf.keras.optimizers.Adam(learning_rate = 0.001) loss = tf.keras.losses.BinaryCrossentropy(label_smoothing = 0.01) model.compile(optimizer = opt, loss = loss, metrics = [&#39;AUC&#39;]) return model . . #collapse-hide ### LEARNING RATE SCHEDULE def get_lr_callback(batch_size=8): lr_start = 0.000005 lr_max = 0.00000125 * REPLICAS * batch_size lr_min = 0.000001 lr_ramp_ep = 5 lr_sus_ep = 0 lr_decay = 0.8 def lrfn(epoch): if epoch &lt; lr_ramp_ep: lr = (lr_max - lr_start) / lr_ramp_ep * epoch + lr_start elif epoch &lt; lr_ramp_ep + lr_sus_ep: lr = lr_max else: lr = (lr_max - lr_min) * lr_decay**(epoch - lr_ramp_ep - lr_sus_ep) + lr_min return lr lr_callback = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose=False) return lr_callback . . The pre-trained model is trained on both training and test data. Here, we use the original training data merged with the complete test set as a training sample. We fix the number of training epochs to EPOCHS and do not perform early stopping. You can also experiment with setting up a small validation sample from both training and test data to perform early stopping. . #collapse-show ### PRE-TRAINED MODEL if USE_PRETRAIN_WEIGHTS: # USE VERBOSE=0 for silent, VERBOSE=1 for interactive, VERBOSE=2 for commit VERBOSE = 2 # DISPLAY INFO if DEVICE == &#39;TPU&#39;: if tpu: tf.tpu.experimental.initialize_tpu_system(tpu) # CREATE TRAIN AND VALIDATION SUBSETS files_train = tf.io.gfile.glob(GCS_PATH[0] + &#39;/train*.tfrec&#39;) print(&#39;#### Using 2020 train data&#39;) files_train += tf.io.gfile.glob(GCS_PATH[0] + &#39;/test*.tfrec&#39;) print(&#39;#### Using 2020 test data&#39;) np.random.shuffle(files_train) # BUILD MODEL K.clear_session() tf.random.set_seed(SEED) with strategy.scope(): model = build_model(dim = IMG_SIZES[0], ef = EFF_NETS[0], pretraining = True) # SAVE BEST MODEL EACH FOLD sv = tf.keras.callbacks.ModelCheckpoint( &#39;weights.h5&#39;, monitor=&#39;loss&#39;, verbose=0, save_best_only=True, save_weights_only=True, mode=&#39;min&#39;, save_freq=&#39;epoch&#39;) # TRAIN print(&#39;Training...&#39;) history = model.fit( get_dataset(files_train, dim = IMG_SIZES[0], batch_size = BATCH_SIZES[0], shuffle = True, repeat = True, pretraining = True), epochs = EPOCHS[0], callbacks = [sv, get_lr_callback(BATCH_SIZES[0])], steps_per_epoch = count_data_items(files_train)/BATCH_SIZES[0]//REPLICAS, verbose = VERBOSE) else: print(&#39;#### NOT using a pre-trained model&#39;) . . #### Using 2020 train data #### Using 2020 test data Downloading data from https://github.com/Callidior/keras-applications/releases/download/efficientnet/efficientnet-b0_weights_tf_dim_ordering_tf_kernels_autoaugment_notop.h5 16809984/16804768 [==============================] - 0s 0us/step Training... Epoch 1/10 170/170 - 10s - loss: 1.7556 - lr: 5.0000e-06 Epoch 2/10 170/170 - 10s - loss: 1.1257 - lr: 6.8000e-05 Epoch 3/10 170/170 - 11s - loss: 0.8906 - lr: 1.3100e-04 Epoch 4/10 170/170 - 10s - loss: 0.8118 - lr: 1.9400e-04 Epoch 5/10 170/170 - 9s - loss: 0.8222 - lr: 2.5700e-04 Epoch 6/10 170/170 - 9s - loss: 0.8626 - lr: 3.2000e-04 Epoch 7/10 170/170 - 9s - loss: 0.8402 - lr: 2.5620e-04 Epoch 8/10 170/170 - 9s - loss: 0.8257 - lr: 2.0516e-04 Epoch 9/10 170/170 - 10s - loss: 0.8091 - lr: 1.6433e-04 Epoch 10/10 170/170 - 10s - loss: 0.7865 - lr: 1.3166e-04 . The pre-training is complete! Now, we need to resave weights of our pre-trained model to make it easier to load them in the future. We are not really interested in the classification head, so we only export the weights of the convolutional part of the network. We can index these layers using model.layers[1]. . #collapse-show # LOAD WEIGHTS AND CHECK MODEL if USE_PRETRAIN_WEIGHTS: model.load_weights(&#39;weights.h5&#39;) model.summary() # EXPORT BASE WEIGHTS if USE_PRETRAIN_WEIGHTS: model.layers[1].save_weights(&#39;base_weights.h5&#39;) . . Model: &#34;model&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= input_1 (InputLayer) [(None, 128, 128, 3)] 0 _________________________________________________________________ efficientnet-b0 (Model) (None, 4, 4, 1280) 4049564 _________________________________________________________________ global_average_pooling2d (Gl (None, 1280) 0 _________________________________________________________________ dense (Dense) (None, 6) 7686 ================================================================= Total params: 4,057,250 Trainable params: 4,015,234 Non-trainable params: 42,016 _________________________________________________________________ . Main classification model . Now we can train a final classification model using a cross-validation framework on the training data! . We need to take care of a couple of changes: . Make sure that we don&#39;t use test data in the training folds. | Set use_pretrain_weights = True and pretraining = False in the build_model() function to initialize from the pre-trained weights in the beginning of each fold. | #collapse-show # USE VERBOSE=0 for silent, VERBOSE=1 for interactive, VERBOSE=2 for commit VERBOSE = 0 skf = KFold(n_splits = FOLDS, shuffle = True, random_state = SEED) oof_pred = []; oof_tar = []; oof_val = []; oof_names = []; oof_folds = [] preds = np.zeros((count_data_items(files_test),1)) for fold,(idxT,idxV) in enumerate(skf.split(np.arange(15))): # DISPLAY FOLD INFO if DEVICE == &#39;TPU&#39;: if tpu: tf.tpu.experimental.initialize_tpu_system(tpu) print(&#39;#&#39;*25); print(&#39;#### FOLD&#39;,fold+1) # CREATE TRAIN AND VALIDATION SUBSETS files_train = tf.io.gfile.glob([GCS_PATH[fold] + &#39;/train%.2i*.tfrec&#39;%x for x in idxT]) np.random.shuffle(files_train); print(&#39;#&#39;*25) files_valid = tf.io.gfile.glob([GCS_PATH[fold] + &#39;/train%.2i*.tfrec&#39;%x for x in idxV]) files_test = np.sort(np.array(tf.io.gfile.glob(GCS_PATH[fold] + &#39;/test*.tfrec&#39;))) # BUILD MODEL K.clear_session() tf.random.set_seed(SEED) with strategy.scope(): model = build_model(dim = IMG_SIZES[fold], ef = EFF_NETS[fold], use_pretrain_weights = USE_PRETRAIN_WEIGHTS, pretraining = False) # SAVE BEST MODEL EACH FOLD sv = tf.keras.callbacks.ModelCheckpoint( &#39;fold-%i.h5&#39;%fold, monitor=&#39;val_auc&#39;, verbose=0, save_best_only=True, save_weights_only=True, mode=&#39;max&#39;, save_freq=&#39;epoch&#39;) # TRAIN print(&#39;Training...&#39;) history = model.fit( get_dataset(files_train, shuffle = True, repeat = True, dim = IMG_SIZES[fold], batch_size = BATCH_SIZES[fold]), epochs = EPOCHS[fold], callbacks = [sv,get_lr_callback(BATCH_SIZES[fold])], steps_per_epoch = count_data_items(files_train)/BATCH_SIZES[fold]//REPLICAS, validation_data = get_dataset(files_valid, shuffle = False, repeat = False, dim = IMG_SIZES[fold]), verbose = VERBOSE ) model.load_weights(&#39;fold-%i.h5&#39;%fold) # PREDICT OOF print(&#39;Predicting OOF...&#39;) ds_valid = get_dataset(files_valid,labeled=False,return_image_names=False,shuffle=False,dim=IMG_SIZES[fold],batch_size=BATCH_SIZES[fold]*4) ct_valid = count_data_items(files_valid); STEPS = ct_valid/BATCH_SIZES[fold]/4/REPLICAS pred = model.predict(ds_valid,steps=STEPS,verbose=VERBOSE)[:ct_valid,] oof_pred.append(pred) # GET OOF TARGETS AND NAMES ds_valid = get_dataset(files_valid,dim=IMG_SIZES[fold],labeled=True, return_image_names=True) oof_tar.append(np.array([target.numpy() for img, target in iter(ds_valid.unbatch())]) ) oof_folds.append(np.ones_like(oof_tar[-1],dtype=&#39;int8&#39;)*fold ) ds = get_dataset(files_valid,dim=IMG_SIZES[fold],labeled=False,return_image_names=True) oof_names.append(np.array([img_name.numpy().decode(&quot;utf-8&quot;) for img, img_name in iter(ds.unbatch())])) # PREDICT TEST print(&#39;Predicting Test...&#39;) ds_test = get_dataset(files_test,labeled=False,return_image_names=False,shuffle=False,dim=IMG_SIZES[fold],batch_size=BATCH_SIZES[fold]*4) ct_test = count_data_items(files_test); STEPS = ct_test/BATCH_SIZES[fold]/4/REPLICAS pred = model.predict(ds_test,steps=STEPS,verbose=VERBOSE)[:ct_test,] preds[:,0] += (pred * WGTS[fold]).reshape(-1) . . ######################### #### FOLD 1 ######################### Training... Predicting OOF... Predicting Test... ######################### #### FOLD 2 ######################### Training... Predicting OOF... Predicting Test... ######################### #### FOLD 3 ######################### Training... Predicting OOF... Predicting Test... ######################### #### FOLD 4 ######################### Training... Predicting OOF... Predicting Test... ######################### #### FOLD 5 ######################### Training... Predicting OOF... Predicting Test... . #collapse-show # COMPUTE OOF AUC oof = np.concatenate(oof_pred); true = np.concatenate(oof_tar); names = np.concatenate(oof_names); folds = np.concatenate(oof_folds) auc = roc_auc_score(true,oof) print(&#39;Overall OOF AUC = %.4f&#39;%auc) . . Overall OOF AUC = 0.8414 . How does the OOF AUC compare to a model without the pre-training stage? To check this, we can simply set USE_PRETRAIN_WEIGHTS = False in the beginning of the notebook. This is done in thus version of the Kaggle notebook, yielding a model with a lower OOF AUC (0.8329 compared to 0.8414 with pre-training). . Compared to a model initialized from the Imagenet weights, pre-training on a surrogate label brings a CV improvement. The AUC gain also translates into the performance gain on the competition leaderboard (increase from 0.8582 to 0.8809). Great news! . 6. Closing words . This is the end of this blogpost. Using a computer vision application, we demonstrated how to use meta-data to construct a surrogate label and pre-train a CNN on both training and test data to improve performance. . The pre-trained model can be further optimized to increase performance gains. Using a validation subset on the pre-training stage can help to tune the number of epochs and other learning parameters. Another idea could be to construct a surrogate label with more unique values (e.g., combination of anatom_site_general_challenge and sex) to make the pre-training task more challenging and motivate the model to learn better. On the other hand, further optimizing the main classification model may reduce the benefit of pre-training. .",
            "url": "https://kozodoi.me/blog/20200830/pre-training",
            "relUrl": "/blog/20200830/pre-training",
            "date": " • Aug 30, 2020"
        }
        
    
  
    
        ,"post13": {
            "title": "Profit-Driven Demand Forecasting",
            "content": "Last update: 21.03.2021. All opinions are my own. . 1. Overview . Demand forecasting is an important task that helps to optimize inventory planning. Optimized stocks reduce retailer&#39;s costs and increase customer satisfaction due to faster delivery time. . The 2020 edition of the Data Mining Cup was devoted to profit-driven demand prediction for a set of items using past purchase data. Together with Elizaveta Zinovyeva, we represented the Humboldt University of Berlin and finished in the top-15 of the leaderboard. . This blog post provides a detailed walkthrough covering the crucial steps of our solution: . data preparation and feature engineering | aggregation of transactional data into the daily format | implementation of custom profit loss functions | two-stage demand forecasting with LightGBM | hyper-parameter tuning with hyperopt | . Feel free to jump directly to the sections interesting to you! The code with our solution is available on Github. . 2. Data preparation . Data overview . The competition data includes three files: . items.csv: item-specific characteristics such as brand, manufacturer, etc | orders.csv: purchase transactions over the 6-month period | infos.csv: prices and promotions in the unlabeled test set | . Let&#39;s have a look at the data: . # packages import numpy as np import pandas as pd import matplotlib.pyplot as plt import seaborn as sns # data import infos = pd.read_csv(&#39;../data/raw/infos.csv&#39;, sep = &#39;|&#39;) items = pd.read_csv(&#39;../data/raw/items.csv&#39;, sep = &#39;|&#39;) orders = pd.read_csv(&#39;../data/raw/orders.csv&#39;, sep = &#39;|&#39;) print(infos.shape) print(items.shape) print(orders.shape) . (10463, 3) (10463, 8) (2181955, 5) . infos.head(3) . itemID simulationPrice promotion . 0 | 1 | 3.43 | NaN | . 1 | 2 | 9.15 | NaN | . 2 | 3 | 14.04 | NaN | . items.head(3) . itemID brand manufacturer customerRating category1 category2 category3 recommendedRetailPrice . 0 | 1 | 0 | 1 | 4.38 | 1 | 1 | 1 | 8.84 | . 1 | 2 | 0 | 2 | 3.00 | 1 | 2 | 1 | 16.92 | . 2 | 3 | 0 | 3 | 5.00 | 1 | 3 | 1 | 15.89 | . orders.head(3) . time transactID itemID order salesPrice . 0 | 2018-01-01 00:01:56 | 2278968 | 450 | 1 | 17.42 | . 1 | 2018-01-01 00:01:56 | 2278968 | 83 | 1 | 5.19 | . 2 | 2018-01-01 00:07:11 | 2255797 | 7851 | 2 | 20.47 | . For each of the 10,463 items, we need to predict the total number of orders in the 14-day period following the last day in orders. . Preprocessing . Let&#39;s prepare the data! First, we merge item-level data in items and infos: . print(infos.shape) print(items.shape) items = pd.merge(infos, items, on = &#39;itemID&#39;, how = &#39;left&#39;) print(items.shape) del infos . (10463, 3) (10463, 8) (10463, 10) . Next, we check and convert feature types to the appropriate format: . #collapse-hide print(&#39;-&#39; * 50) print(items.dtypes) print(&#39;-&#39; * 50) print(orders.dtypes) print(&#39;-&#39; * 50) # items for var in [&#39;itemID&#39;, &#39;brand&#39;, &#39;manufacturer&#39;, &#39;category1&#39;, &#39;category2&#39;, &#39;category3&#39;]: items[var] = items[var].astype(&#39;str&#39;).astype(&#39;object&#39;) # orders for var in [&#39;transactID&#39;, &#39;itemID&#39;]: orders[var] = orders[var].astype(&#39;str&#39;).astype(&#39;object&#39;) # dates orders[&#39;time&#39;] = pd.to_datetime(orders[&#39;time&#39;].astype(&#39;str&#39;), infer_datetime_format = True) . . -- itemID int64 simulationPrice float64 promotion object brand int64 manufacturer int64 customerRating float64 category1 int64 category2 int64 category3 int64 recommendedRetailPrice float64 dtype: object -- time object transactID int64 itemID int64 order int64 salesPrice float64 dtype: object -- . Finally, we unfold the promotion feature containing a sequence of coma-separated dates. We use split_nested_features() from dptools to split a string column into separate features. . dptools is a package developed by me to simplify common data preprocessing and feature engineering tasks. Below, you will see more examples on using dptools for other applications. You can read more about the package here. . #collapse-show # import packages !pip install dptools from dptools import * # split promotion feature items = split_nested_features(items, split_vars = &#39;promotion&#39;, sep = &#39;,&#39;) print(items.head(3)) # convert dates promotion_vars = items.filter(like = &#39;promotion_&#39;).columns for var in promotion_vars: items[var] = pd.to_datetime(items[var], infer_datetime_format = True) . . Added 3 split-based features. . itemID simulationPrice brand manufacturer customerRating category1 category2 category3 recommendedRetailPrice promotion_0 promotion_1 promotion_2 . 0 | 1 | 3.43 | NaN | 1 | 4.38 | 1 | 1 | 1 | 8.84 | NaN | NaN | NaN | . 1 | 2 | 9.15 | NaN | 2 | 3.00 | 1 | 2 | 1 | 16.92 | NaN | NaN | NaN | . 2 | 3 | 14.04 | NaN | 3 | 5.00 | 1 | 3 | 1 | 15.89 | NaN | NaN | NaN | . We now export the data as csv. I use save_csv_version() to automatically add a version number to the file name to prevent overwriting the data after making changes. . save_csv_version(&#39;../data/prepared/orders.csv&#39;, orders, index = False, compression = &#39;gzip&#39;) save_csv_version(&#39;../data/prepared/items.csv&#39;, items, index = False, compression = &#39;gzip&#39;) . Saved as ../data/prepared/orders_v2.csv Saved as ../data/prepared/items_v2.csv . 3. Aggregation and feature engineering . Data aggregation . Let&#39;s work with orders, which provides a list of transactions with timestamps. . We need to aggregate this data for future modeling. Since the task is a 14-day demand forecasting, a simple way would be to aggregate transactions on a two-week basis. However, this could lead to losing some more granular information. We aggregate transactions by day: . #collapse-show orders[&#39;day_of_year&#39;] = orders[&#39;time&#39;].dt.dayofyear orders_price = orders.groupby([&#39;itemID&#39;, &#39;day_of_year&#39;])[&#39;salesPrice&#39;].agg(&#39;mean&#39;).reset_index() orders = orders.groupby([&#39;itemID&#39;, &#39;day_of_year&#39;])[&#39;order&#39;].agg(&#39;sum&#39;).reset_index() orders.head(3) . . itemID day_of_year order . 0 | 1 | 23 | 1 | . 1 | 1 | 25 | 1 | . 2 | 1 | 29 | 307 | . Adding missing item-day combinations . The aggregated data only contains entries for day-item pairs for which there is at least one transaction. This results in missing information: . most items are only sold on a few days; no data on days with no orders is recorded | there are a few items that are never sold and therefore do not appear in orders | . To account for the missing data, we add entries with order = 0 for missing day-item combinations. This increases the number of observations from 100,771 to 1,883,340 and provides useful information about zero sales. . #collapse-show # add items that were never sold before missing_itemIDs = set(items[&#39;itemID&#39;].unique()) - set(orders[&#39;itemID&#39;].unique()) missing_rows = pd.DataFrame({&#39;itemID&#39;: list(missing_itemIDs), &#39;day_of_year&#39;: np.ones(len(missing_itemIDs)).astype(&#39;int&#39;), &#39;order&#39;: np.zeros(len(missing_itemIDs)).astype(&#39;int&#39;)}) orders = pd.concat([orders, missing_rows], axis = 0) print(orders.shape) # add zeros for days with no transactions agg_orders = orders.groupby([&#39;itemID&#39;, &#39;day_of_year&#39;]).order.unique().unstack(&#39;day_of_year&#39;).stack(&#39;day_of_year&#39;, dropna = False) agg_orders = agg_orders.reset_index() agg_orders.columns = [&#39;itemID&#39;, &#39;day_of_year&#39;, &#39;order&#39;] agg_orders[&#39;order&#39;].fillna(0, inplace = True) agg_orders[&#39;order&#39;] = agg_orders[&#39;order&#39;].astype(int) print(agg_orders.shape) . . (100771, 3) (1883340, 3) . Labeling promotions . The data documentation says that promotions in the training data are not explicitly marked. . We need to manually mark promotion days. Ignoring it complicates forecasting because the number of orders in some days explodes without an apparent reason. In such cases, the underlying reason is likely a promotion, which should be reflected in a corresponding feature. . We need to be very careful about marking promotions. Labeling too many days as promotions based on the number of orders risks introducing data leakage since the number of orders is unknown at the prediction time. Below, I use find_peaks() to isolate peaks in the order time series and encode them as promotions: . #collapse-show # computations agg_orders[&#39;promotion&#39;] = 0 for itemID in tqdm(agg_orders[&#39;itemID&#39;].unique()): promo = np.zeros(len(agg_orders[agg_orders[&#39;itemID&#39;] == itemID])) avg = agg_orders[(agg_orders[&#39;itemID&#39;] == itemID)][&#39;order&#39;].median() std = agg_orders[(agg_orders[&#39;itemID&#39;] == itemID)][&#39;order&#39;].std() peaks, _ = find_peaks(np.append(agg_orders[agg_orders[&#39;itemID&#39;] == itemID][&#39;order&#39;].values, avg), # append avg to enable marking last point as promo prominence = max(5, std), # peak difference with neighbor points; max(5,std) to exclude cases when std is too small height = avg + 2*std) # minimal height of a peak promo[peaks] = 1 agg_orders.loc[agg_orders[&#39;itemID&#39;] == itemID, &#39;promotion&#39;] = promo # compare promotion number promo_in_train = (agg_orders[&#39;promotion&#39;].sum() / agg_orders[&#39;day_of_year&#39;].max()) / len(items) promo_in_test = (3*len(items) - items.promotion_0.isnull().sum() - items.promotion_2.isnull().sum() - items.promotion_1.isnull().sum()) / 14 / len(items) print(&#39;Daily p(promotion) per item in train: {}&#39;.format(np.round(promo_in_train, 4))) print(&#39;Daily p(promotion) per item in test: {}&#39;.format(np.round(promo_in_test , 4))) . . Daily p(promotion) per item in train: 0.0079 Daily p(promotion) per item in test: 0.0141 . Our method identifies 14,911 promotions. Compared to the test set where promotions are explicitly reported, this amounts to about half as many promos per item and day. . Let&#39;s look visualize promotions for some items: . #collapse-hide # compute promo count promo_count = agg_orders.groupby(&#39;itemID&#39;)[&#39;promotion&#39;].agg(&#39;sum&#39;).reset_index() promo_count = promo_count.sort_values(&#39;promotion&#39;).reset_index(drop = True) # plot some items item_plots = [0, 2000, 4000, 6000, 8000, 9000, 10000, 10100, 10200, 10300, 10400, 10462] fig = plt.figure(figsize = (16, 12)) for i in range(len(item_plots)): plt.subplot(3, 4, i + 1) df = agg_orders[agg_orders.itemID == promo_count[&#39;itemID&#39;][item_plots[i]]] plt.scatter(df[&#39;day_of_year&#39;], df[&#39;order&#39;], c = df[&#39;promotion&#39;]) plt.ylabel(&#39;Total Orders&#39;) plt.xlabel(&#39;Day&#39;) . . . The yellow marker indicates promotions. Our method identifies some outliers as promos but misses a few points that are less prominent. At the same time, we can not be sure that these cases are necessarily promotions: the large number of orders on these days could be observed due to other reasons. We will stick to this solution but note that it might require further improvement. . Feature engineering . Now that the data is aggregated, we construct transaction-based features and the targets. For each day, we compute target as the total number of orders in the following 14 days. The days preceding the considered day are used to extract features. We extract slices of the past [1, 7, ..., 35] days and compute features based on data from that slice. . For each item, we compute the following features: . total count of orders and ordered items | total count of promotions | mean item price | recency of the last order | . The number of orders and promotions is also aggregated on a manufacturer and category level. . In addition, we use tsfresh package to automatically extract features based on the order dynamics in the last 35 days. tsfresh computes hundreds of features describing the time series. We only keep features with no missing values. . #collapse-show # packages from tsfresh import extract_features # parameters days_input = [1, 7, 14, 21, 28, 35] days_target = 14 # preparations day_first = np.max(days_input) day_last = agg_orders[&#39;day_of_year&#39;].max() - days_target + 1 orders = None # merge manufacturer and category agg_orders = agg_orders.merge(items[[&#39;itemID&#39;, &#39;manufacturer&#39;]], how = &#39;left&#39;) agg_orders = agg_orders.merge(items[[&#39;itemID&#39;, &#39;category&#39;]], how = &#39;left&#39;) # computations for day_of_year in tqdm(list(range(149, day_last)) + [agg_orders[&#39;day_of_year&#39;].max()]): ### VALIDAION: TARGET, PROMOTIONS, PRICES # day intervals target_day_min = day_of_year + 1 target_day_max = day_of_year + days_target # compute target and promo: labeled data if day_of_year &lt; agg_orders[&#39;day_of_year&#39;].max(): # target and future promo tmp_df = agg_orders[(agg_orders[&#39;day_of_year&#39;] &gt;= target_day_min) &amp; (agg_orders[&#39;day_of_year&#39;] &lt;= target_day_max) ].groupby(&#39;itemID&#39;)[&#39;order&#39;, &#39;promotion&#39;].agg(&#39;sum&#39;).reset_index() tmp_df.columns = [&#39;itemID&#39;, &#39;target&#39;, &#39;promo_in_test&#39;] # future price tmp_df[&#39;mean_price_test&#39;] = agg_orders[(agg_orders[&#39;day_of_year&#39;] &gt;= target_day_min) &amp; (agg_orders[&#39;day_of_year&#39;] &lt;= target_day_max) ].groupby(&#39;itemID&#39;)[&#39;salesPrice&#39;].agg(&#39;mean&#39;).reset_index()[&#39;salesPrice&#39;] # merge manufacturer and category tmp_df = tmp_df.merge(items[[&#39;itemID&#39;, &#39;manufacturer&#39;, &#39;category&#39;]], how = &#39;left&#39;, on = &#39;itemID&#39;) # future price per manufacturer tmp_df_manufacturer = agg_orders[(agg_orders[&#39;day_of_year&#39;] &gt;= target_day_min) &amp; (agg_orders[&#39;day_of_year&#39;] &lt;= target_day_max) ].groupby(&#39;manufacturer&#39;)[&#39;salesPrice&#39;].agg(&#39;mean&#39;).reset_index() tmp_df_manufacturer.columns = [&#39;manufacturer&#39;, &#39;mean_price_test_manufacturer&#39;] tmp_df = tmp_df.merge(tmp_df_manufacturer, how = &#39;left&#39;, on = &#39;manufacturer&#39;) # future price per category tmp_df_category = agg_orders[(agg_orders[&#39;day_of_year&#39;] &gt;= target_day_min) &amp; (agg_orders[&#39;day_of_year&#39;] &lt;= target_day_max) ].groupby(&#39;category&#39;)[&#39;salesPrice&#39;].agg(&#39;mean&#39;).reset_index() tmp_df_category.columns = [&#39;category&#39;, &#39;mean_price_test_category&#39;] tmp_df = tmp_df.merge(tmp_df_category, how = &#39;left&#39;, on = &#39;category&#39;) # future promo per manufacturer tmp_df_manufacturer = agg_orders[(agg_orders[&#39;day_of_year&#39;] &gt;= target_day_min) &amp; (agg_orders[&#39;day_of_year&#39;] &lt;= target_day_max) ].groupby(&#39;manufacturer&#39;)[&#39;promotion&#39;].agg(&#39;sum&#39;).reset_index() tmp_df_manufacturer.columns = [&#39;manufacturer&#39;, &#39;promo_in_test_manufacturer&#39;] tmp_df = tmp_df.merge(tmp_df_manufacturer, how = &#39;left&#39;, on = &#39;manufacturer&#39;) # future promo per category tmp_df_category = agg_orders[(agg_orders[&#39;day_of_year&#39;] &gt;= target_day_min) &amp; (agg_orders[&#39;day_of_year&#39;] &lt;= target_day_max) ].groupby(&#39;category&#39;)[&#39;promotion&#39;].agg(&#39;sum&#39;).reset_index() tmp_df_category.columns = [&#39;category&#39;, &#39;promo_in_test_category&#39;] tmp_df = tmp_df.merge(tmp_df_category, how = &#39;left&#39;, on = &#39;category&#39;) # compute target and promo: unlabeled data else: # placeholders tmp_df = pd.DataFrame({&#39;itemID&#39;: items.itemID, &#39;target&#39;: np.nan, &#39;promo_in_test&#39;: np.nan, &#39;mean_price_test&#39;: items.simulationPrice, &#39;manufacturer&#39;: items.manufacturer, &#39;category&#39;: items.category, &#39;promo_in_test_manufacturer&#39;: np.nan, &#39;promo_in_test_category&#39;: np.nan}) ### TRAINING: LAG-BASED FEATURES # compute features for day_input in days_input: # day intervals input_day_min = day_of_year - day_input + 1 input_day_max = day_of_year # frequency, promo and price tmp_df_input = agg_orders[(agg_orders[&#39;day_of_year&#39;] &gt;= input_day_min) &amp; (agg_orders[&#39;day_of_year&#39;] &lt;= input_day_max) ].groupby(&#39;itemID&#39;) tmp_df[&#39;order_sum_last_&#39; + str(day_input)] = tmp_df_input[&#39;order&#39;].agg(&#39;sum&#39;).reset_index()[&#39;order&#39;] tmp_df[&#39;order_count_last_&#39; + str(day_input)] = tmp_df_input[&#39;order&#39;].agg(lambda x: len(x[x &gt; 0])).reset_index()[&#39;order&#39;] tmp_df[&#39;promo_count_last_&#39; + str(day_input)] = tmp_df_input[&#39;promotion&#39;].agg(&#39;sum&#39;).reset_index()[&#39;promotion&#39;] tmp_df[&#39;mean_price_last_&#39; + str(day_input)] = tmp_df_input[&#39;salesPrice&#39;].agg(&#39;mean&#39;).reset_index()[&#39;salesPrice&#39;] # frequency, promo per manufacturer tmp_df_input = agg_orders[(agg_orders[&#39;day_of_year&#39;] &gt;= input_day_min) &amp; (agg_orders[&#39;day_of_year&#39;] &lt;= input_day_max) ].groupby(&#39;manufacturer&#39;) tmp_df_manufacturer = tmp_df_input[&#39;order&#39;].agg(&#39;sum&#39;).reset_index() tmp_df_manufacturer.columns = [&#39;manufacturer&#39;, &#39;order_manufacturer_sum_last_&#39; + str(day_input)] tmp_df_manufacturer[&#39;order_manufacturer_count_last_&#39; + str(day_input)] = tmp_df_input[&#39;order&#39;].agg(lambda x: len(x[x &gt; 0])).reset_index()[&#39;order&#39;] tmp_df_manufacturer[&#39;promo_manufacturer_count_last_&#39; + str(day_input)] = tmp_df_input[&#39;promotion&#39;].agg(&#39;sum&#39;).reset_index()[&#39;promotion&#39;] tmp_df = tmp_df.merge(tmp_df_manufacturer, how = &#39;left&#39;, on = &#39;manufacturer&#39;) # frequency, promo per category tmp_df_input = agg_orders[(agg_orders[&#39;day_of_year&#39;] &gt;= input_day_min) &amp; (agg_orders[&#39;day_of_year&#39;] &lt;= input_day_max) ].groupby(&#39;category&#39;) tmp_df_category = tmp_df_input[&#39;order&#39;].agg(&#39;sum&#39;).reset_index() tmp_df_category.columns = [&#39;category&#39;, &#39;order_category_sum_last_&#39; + str(day_input)] tmp_df_category[&#39;order_category_count_last_&#39; + str(day_input)] = tmp_df_input[&#39;order&#39;].agg(lambda x: len(x[x &gt; 0])).reset_index()[&#39;order&#39;] tmp_df_category[&#39;promo_category_count_last_&#39; + str(day_input)] = tmp_df_input[&#39;promotion&#39;].agg(&#39;sum&#39;).reset_index()[&#39;promotion&#39;] tmp_df = tmp_df.merge(tmp_df_category, how = &#39;left&#39;, on = &#39;category&#39;) # frequency, promo per all items tmp_df_input = agg_orders[(agg_orders[&#39;day_of_year&#39;] &gt;= input_day_min) &amp; (agg_orders[&#39;day_of_year&#39;] &lt;= input_day_max)] tmp_df[&#39;order_all_sum_last_&#39; + str(day_input)] = tmp_df_input[&#39;order&#39;].agg(&#39;sum&#39;) tmp_df[&#39;order_all_count_last_&#39; + str(day_input)] = tmp_df_input[&#39;order&#39;].agg(lambda x: len(x[x &gt; 0])) tmp_df[&#39;promo_all_count_last_&#39; + str(day_input)] = tmp_df_input[&#39;promotion&#39;].agg(&#39;sum&#39;) # recency if day_input == max(days_input): tmp_df_input = agg_orders[(agg_orders[&#39;day_of_year&#39;] &gt;= input_day_min) &amp; (agg_orders[&#39;day_of_year&#39;] &lt;= input_day_max) &amp; (agg_orders[&#39;order&#39;] &gt; 0) ].groupby(&#39;itemID&#39;) tmp_df[&#39;days_since_last_order&#39;] = (day_of_year - tmp_df_input[&#39;day_of_year&#39;].agg(&#39;max&#39;)).reindex(tmp_df.itemID).reset_index()[&#39;day_of_year&#39;] tmp_df[&#39;days_since_last_order&#39;].fillna(day_input, inplace = True) # tsfresh features if day_input == max(days_input): tmp_df_input = agg_orders[(agg_orders[&#39;day_of_year&#39;] &gt;= input_day_min) &amp; (agg_orders[&#39;day_of_year&#39;] &lt;= input_day_max)] tmp_df_input = tmp_df_input[[&#39;day_of_year&#39;, &#39;itemID&#39;, &#39;order&#39;]] extracted_features = extract_features(tmp_df_input, column_id = &#39;itemID&#39;, column_sort = &#39;day_of_year&#39;) extracted_features[&#39;itemID&#39;] = extracted_features.index tmp_df = tmp_df.merge(extracted_features, how = &#39;left&#39;, on = &#39;itemID&#39;) ### FINAL PREPARATIONS # day of year tmp_df.insert(1, column = &#39;day_of_year&#39;, value = day_of_year) # merge data orders = pd.concat([orders, tmp_df], axis = 0) # drop manufacturer and category del orders[&#39;manufacturer&#39;] del orders[&#39;category&#39;] ##### REMOVE MISSINGS good_nas = [&#39;target&#39;, &#39;mean_price_test_category&#39;, &#39;mean_price_test_manufacturer&#39;, &#39;promo_in_test&#39;, &#39;promo_in_test_category&#39;, &#39;promo_in_test_manufacturer&#39;] nonas = list(orders.columns[orders.isnull().sum() == 0]) + good_nas orders = orders[nonas] print(orders.shape) ##### COMPUTE MEAN PRICE RATIOS print(orders.shape) price_vars = [&#39;mean_price_last_1&#39;, &#39;mean_price_last_7&#39;, &#39;mean_price_last_14&#39;, &#39;mean_price_last_21&#39;, &#39;mean_price_last_28&#39;, &#39;mean_price_last_35&#39;] for var in price_vars: orders[&#39;ratio_&#39; + str(var)] = orders[&#39;mean_price_test&#39;] / orders[var] orders[&#39;ratio_manufacturer_&#39; + str(var)] = orders[&#39;mean_price_test_manufacturer&#39;] / orders[var] orders[&#39;ratio_category_&#39; + str(var)] = orders[&#39;mean_price_test_category&#39;] / orders[var] print(orders.shape) . . (1391579, 458) (1391579, 470) . The feature extraction takes about ten hours and outputs a data set with 470 features. Great job! . Now, let&#39;s create features in the items data set: . ratio of the actual and recommended price | item category index constructed of three subcategories | customer rating relative to the average rating of the items of the same manufacturer or category | . #collapse-show # price ratio items[&#39;recommended_simulation_price_ratio&#39;] = items[&#39;simulationPrice&#39;] / items[&#39;recommendedRetailPrice&#39;] # detailed item category items[&#39;category&#39;] = items[&#39;category1&#39;].astype(str) + items[&#39;category2&#39;].astype(str) + items[&#39;category3&#39;].astype(str) items[&#39;category&#39;] = items[&#39;category&#39;].astype(int) # customer rating ratio per manufacturer rating_manufacturer = items.groupby(&#39;manufacturer&#39;)[&#39;customerRating&#39;].agg(&#39;mean&#39;).reset_index() rating_manufacturer.columns = [&#39;manufacturer&#39;, &#39;mean_customerRating_manufacturer&#39;] items = items.merge(rating_manufacturer, how = &#39;left&#39;, on = &#39;manufacturer&#39;) items[&#39;customerRating_manufacturer_ratio&#39;] = items[&#39;customerRating&#39;] / items[&#39;mean_customerRating_manufacturer&#39;] del items[&#39;mean_customerRating_manufacturer&#39;] # customer rating ratio per category rating_category = items.groupby(&#39;category&#39;)[&#39;customerRating&#39;].agg(&#39;mean&#39;).reset_index() rating_category.columns = [&#39;category&#39;, &#39;mean_customerRating_category&#39;] items = items.merge(rating_category, how = &#39;left&#39;, on = &#39;category&#39;) items[&#39;customerRating_category_ratio&#39;] = items[&#39;customerRating&#39;] / items[&#39;mean_customerRating_category&#39;] del items[&#39;mean_customerRating_category&#39;] . . We can now merge orders and items. We also partition the data into the labeled training set and the unlabeled test set, compute some missing features for the test set and export the data as csv. . #collapse-hide ##### DATA PARTITIONING # merge data df = pd.merge(orders, items, on = &#39;itemID&#39;, how = &#39;left&#39;) # partition intro train and test df_train = df[df[&#39;day_of_year&#39;] &lt; df[&#39;day_of_year&#39;].max()] df_test = df[df[&#39;day_of_year&#39;] == df[&#39;day_of_year&#39;].max()] ##### COMPUTE FEATURES FOR TEST DATA # add promotion info to test promo_vars = df_test.filter(like = &#39;promotion_&#39;).columns df_test[&#39;promo_in_test&#39;] = 3 - df_test[promo_vars].isnull().sum(axis = 1) df_test[&#39;promo_in_test&#39;].describe() del df_test[&#39;promo_in_test_manufacturer&#39;], df_test[&#39;promo_in_test_category&#39;] # future promo per manufacturer tmp_df_manufacturer = df_test.groupby(&#39;manufacturer&#39;)[&#39;promo_in_test&#39;].agg(&#39;sum&#39;).reset_index() tmp_df_manufacturer.columns = [&#39;manufacturer&#39;, &#39;promo_in_test_manufacturer&#39;] df_test = df_test.merge(tmp_df_manufacturer, how = &#39;left&#39;, on = &#39;manufacturer&#39;) # future promo per category tmp_df_category = df_test.groupby(&#39;category&#39;)[&#39;promo_in_test&#39;].agg(&#39;sum&#39;).reset_index() tmp_df_category.columns = [&#39;category&#39;, &#39;promo_in_test_category&#39;] df_test = df_test.merge(tmp_df_category, how = &#39;left&#39;, on = &#39;category&#39;) del df_test[&#39;mean_price_test_manufacturer&#39;], df_test[&#39;mean_price_test_category&#39;] # future price per manufacturer tmp_df_manufacturer = df_test.groupby(&#39;manufacturer&#39;)[&#39;mean_price_test&#39;].agg(&#39;mean&#39;).reset_index() tmp_df_manufacturer.columns = [&#39;manufacturer&#39;, &#39;mean_price_test_manufacturer&#39;] df_test = df_test.merge(tmp_df_manufacturer, how = &#39;left&#39;, on = &#39;manufacturer&#39;) # future price per category tmp_df_category = df_test.groupby(&#39;category&#39;)[&#39;mean_price_test&#39;].agg(&#39;mean&#39;).reset_index() tmp_df_category.columns = [&#39;category&#39;, &#39;mean_price_test_category&#39;] df_test = df_test.merge(tmp_df_category, how = &#39;left&#39;, on = &#39;category&#39;) # mean price ratios for var in price_vars: df_test[&#39;ratio_&#39; + str(var)] = df_test[&#39;mean_price_test&#39;] / df_test[var] df_test[&#39;ratio_manufacturer_&#39; + str(var)] = df_test[&#39;mean_price_test_manufacturer&#39;] / df_test[var] df_test[&#39;ratio_category_&#39; + str(var)] = df_test[&#39;mean_price_test_category&#39;] / df_test[var] ##### DROP FEATURES # drop promotion dates df_test.drop(promo_vars, axis = 1, inplace = True) df_train.drop(promo_vars, axis = 1, inplace = True) # drop mean prices price_vars = price_vars + [&#39;mean_price_test_manufacturer&#39;, &#39;mean_price_test_category&#39;] df_test.drop(price_vars, axis = 1, inplace = True) df_train.drop(price_vars, axis = 1, inplace = True) # export data save_csv_version(&#39;../data/prepared/df.csv&#39;, df_train, index = False, compression = &#39;gzip&#39;) save_csv_version(&#39;../data/prepared/df_test.csv&#39;, df_test, index = False, compression = &#39;gzip&#39;, min_version = 3) print(df_train.shape) print(df_test.shape) . . Saved as ../data/prepared/df_v14.csv Saved as ../data/prepared/df_test_v14.csv (1381116, 476) (10463, 476) . 4. Modeling . Custom loss functions . Machine learning encompasses a wide range of statically-inspired performance metrics such as MSE, MAE and others. In practice, machine learning models are used by a company that has specific goals. Usually, these goals can not be expressed in terms of such simple metrics. Therefore, it is important to come up with an evaluation metric consistent with the company&#39;s objectives to ensure that we judge performance on a criterion that actually matters. . In the DMC 2020 task, we are given a profit function of the retailer. The function accounts for asymmetric error costs: underpredicting demand results in lost revenue because the retailer can not sell a product that is not ready to ship, whereas overpredicting demand incurs a fee for storing the excessive amount of product. . Below, we derive profit according to the task description: . . Let&#39;s implement the profit function in Python: . def profit(y_true, y_pred, price): &#39;&#39;&#39; Computes retailer&#39;s profit. Arguments: - y_true (numpy array): ground truth demand values - y_pred (numpy array): predicted demand values - price (numpy array): item prices Returns: - profit value &#39;&#39;&#39; # remove negative and round y_pred = np.where(y_pred &gt; 0, y_pred, 0) y_pred = np.round(y_pred).astype(&#39;int&#39;) # sold units units_sold = np.minimum(y_true, y_pred) # overstocked units units_overstock = y_pred - y_true units_overstock[units_overstock &lt; 0] = 0 # profit revenue = units_sold * price fee = units_overstock * price * 0.6 profit = revenue - fee profit = profit.sum() return profit . The function above is great for evaluating quality of our predictions. But can we directly optimize it during modeling? . LightGBM supports custom loss functions on both training and validation stages. To use a custom loss during training, one needs to supply a function with its first and second-order derivatives. . Ideally, we would like to define the loss as a difference between the potential profit (given our demand prediction) and the oracle profit (when demand prediction is correct). However, such a loss is not differentiable. We can not compute derivatives to plug it as a training loss. Instead, we need to come up with a slightly different function that approximates profit and satisfies the loss conditions. . We define the loss as a squared difference between the oracle profit and profit based on predicted demand. In this setting, we can compute loss derivatives with respect to the prediction (Gradient and Hessian): . . The snippet below implements the training and validation losses for LightGBM. You can notice that we do not include the squared prices in the loss functions. The reason is that with sklearn API, it is difficult to include external variables like prices in the loss, so we will include the price later. . # collpase-show ##### TRAINING LOSS def asymmetric_mse(y_true, y_pred): &#39;&#39;&#39; Asymmetric MSE objective for training LightGBM regressor. Arguments: - y_true (numpy array): ground truth target values - y_pred (numpy array): estimated target values Returns: - gradient - hessian &#39;&#39;&#39; residual = (y_true - y_pred).astype(&#39;float&#39;) grad = np.where(residual &gt; 0, -2*residual, -0.72*residual) hess = np.where(residual &gt; 0, 2.0, 0.72) return grad, hess ##### VALIDATION LOSS def asymmetric_mse_eval(y_true, y_pred): &#39;&#39;&#39; Asymmetric MSE evaluation metric for evaluating LightGBM regressor. Arguments: - y_true (numpy array): ground truth target values - y_pred (numpy array): estimated target values Returns: - metric name - metric value - whether the metric is maximized &#39;&#39;&#39; residual = (y_true - y_pred).astype(&#39;float&#39;) loss = np.where(residual &gt; 0, 2*residual**2, 0.72*residual**2) return &#39;asymmetric_mse_eval&#39;, np.mean(loss), False . How to deal with the prices? . One option is to account for them within the fit() method. LightGBM supports weighing observations using the arguments sample_weight and eval_sample_weight. You will see how we supply price vectors in the modeling code in the next section. Note that including prices as weights instead of plugging them into the loss leads to losing some information, since Gradients and Hessians are computed without the price multiplication. Still, this approach provides a pretty close approximation of the original profit function. If you are interested in including prices in the loss, you can check lightGBM API that allows more flexibility. . The only missing piece is the relationship between the penalty size and the prediction error. By taking a square root of the profit differences instead of the absolute value, we penalize larger errors more than the smaller ones. However, our profit changes linearly with the error size. This is how we can can address it: . transform target using a non-linear transformation (e.g. square root) | train a model that optimizes the MSE loss on the transformed target | apply the inverse transformation to the model predictions | . Target transformation smooths out the square effect in MSE. We still penalize large errors more, but the large errors on a transformed scale are also smaller compared to the original scale. This helps to balance the two effects and approximate a linear relationship between the error size and the loss penalty. . Modeling pipeline . Good, let&#39;s start building models! First, we extract the target and flag ID features not used for prediction. . #collapse-hide # extract target y = df_train[&#39;target&#39;] X = df_train.drop(&#39;target&#39;, axis = 1) del df_train print(X.shape, y.shape) # format test data X_test = df_test.drop(&#39;target&#39;, axis = 1) del df_test print(X_test.shape) # relevant features drop_feats = [&#39;itemID&#39;, &#39;day_of_year&#39;] features = [var for var in X.columns if var not in drop_feats] . . (1381116, 475) (1381116,) (10463, 475) . The modeling pipeline uses multiple tricks discovered during the model refinement process. We toggle these tricks using logical variables that define the following training options: . target_transform = True: transforms target to reduce penalty for large errors. Motivation for this is provided in the previous section. | train_on_positive = False: trains only on cases with positive sales (i.e., at least one of the order lags is greater than zero) and predicts null demand for items with no sales. This substantially reduces the training time but also leads to a drop in the performance. | two_stage = True: trains a two-stage model: (i) binary classifier predicting whether the future sales will be zero; (ii) regression model predicting the volume of sales. Predictions of the regression model are only stored for cases where the classifier predicts positive sales. | tuned_params = True: imports optimized LightGBM hyper-parameter values. The next section describes the tuning procedure. | . #collapse-hide ##### TRAINING OPTIONS # target transformation target_transform = True # train on positive sales only train_on_positive = False # two-stage model two_stage = True # use tuned meta-params tuned_params = True ##### CLASSIFIER PARAMETERS # rounds and options cores = 4 stop_rounds = 100 verbose = 100 seed = 23 # LGB parameters lgb_params = { &#39;boosting_type&#39;: &#39;goss&#39;, &#39;objective&#39;: asymmetric_mse, &#39;metrics&#39;: asymmetric_mse_eval, &#39;n_estimators&#39;: 1000, &#39;learning_rate&#39;: 0.1, &#39;bagging_fraction&#39;: 0.8, &#39;feature_fraction&#39;: 0.8, &#39;lambda_l1&#39;: 0.1, &#39;lambda_l2&#39;: 0.1, &#39;silent&#39;: True, &#39;verbosity&#39;: -1, &#39;nthread&#39; : cores, &#39;random_state&#39;: seed, } # load optimal parameters if tuned_params: par_file = open(&#39;../lgb_meta_params_100.pkl&#39;, &#39;rb&#39;) lgb_params = pickle.load(par_file) lgb_params[&#39;nthread&#39;] = cores lgb_params[&#39;random_state&#39;] = seed # second-stage LGB if two_stage: lgb_classifier_params = lgb_params.copy() lgb_classifier_params[&#39;objective&#39;] = &#39;binary&#39; lgb_classifier_params[&#39;metrics&#39;] = &#39;logloss&#39; . . We also define the partitioning parameters. We use a sliding window approach with 7 folds, where each subsequent fold is shifted by one day into the past. . #collapse-hide num_folds = 7 # no. CV folds test_days = 14 # no. days in the test set . . Let&#39;s explain the partitioning using the first fold as an example. Each fold is divided into training and validation subsets. The first 35 days are cut off and only used to compute lag-based features for the days starting from 36. Days 36 - 145 are used for training. For each of these days, we have features based on the previous 35 days and targets based on the next 14 days. Days 159 - 173 are used for validation. Days 146 - 158 between training and validation subsets are skipped to avoid data leakage: the target for these days would use information from the validation period. . . We can now set up a modeling loop with the following steps for each of the folds: . extract data from the fold and partition it into training and validation sets | train LightGBM on the training set and perform early stopping on the validation set | save predictions for the validation set (denoted as OOF) and predictions for the test set | save feature importance and performance on the validation fold | . #collapse-show # placeholders importances = pd.DataFrame() preds_oof = np.zeros((num_folds, items.shape[0])) reals_oof = np.zeros((num_folds, items.shape[0])) prices_oof = np.zeros((num_folds, items.shape[0])) preds_test = np.zeros(items.shape[0]) oof_rmse = [] oof_profit = [] oracle_profit = [] clfs = [] train_idx = [] valid_idx = [] # objects train_days = X[&#39;day_of_year&#39;].max() - test_days + 1 - num_folds - X[&#39;day_of_year&#39;].min() # no. days in the train set time_start = time.time() # modeling loop for fold in range(num_folds): ##### PARTITIONING # dates if fold == 0: v_end = X[&#39;day_of_year&#39;].max() else: v_end = v_end - 1 v_start = v_end t_end = v_start - (test_days + 1) t_start = t_end - (train_days - 1) # extract index train_idx.append(list(X[(X.day_of_year &gt;= t_start) &amp; (X.day_of_year &lt;= t_end)].index)) valid_idx.append(list(X[(X.day_of_year &gt;= v_start) &amp; (X.day_of_year &lt;= v_end)].index)) # extract samples X_train, y_train = X.iloc[train_idx[fold]][features], y.iloc[train_idx[fold]] X_valid, y_valid = X.iloc[valid_idx[fold]][features], y.iloc[valid_idx[fold]] X_test = X_test[features] # keep positive cases if train_on_positive: y_train = y_train.loc[(X_train[&#39;order_sum_last_28&#39;] &gt; 0) | (X_train[&#39;promo_in_test&#39;] &gt; 0)] X_train = X_train.loc[(X_train[&#39;order_sum_last_28&#39;] &gt; 0) | (X_train[&#39;promo_in_test&#39;] &gt; 0)] # information print(&#39;-&#39; * 65) print(&#39;- train period days: {} -- {} (n = {})&#39;.format(t_start, t_end, len(train_idx[fold]))) print(&#39;- valid period days: {} -- {} (n = {})&#39;.format(v_start, v_end, len(valid_idx[fold]))) print(&#39;-&#39; * 65) ##### MODELING # target transformation if target_transform: y_train = np.sqrt(y_train) y_valid = np.sqrt(y_valid) # first stage model if two_stage: y_train_binary, y_valid_binary = y_train.copy(), y_valid.copy() y_train_binary[y_train_binary &gt; 0] = 1 y_valid_binary[y_valid_binary &gt; 0] = 1 clf_classifier = lgb.LGBMClassifier(**lgb_classifier_params) clf_classifier = clf_classifier.fit(X_train, y_train_binary, eval_set = [(X_train, y_train_binary), (X_valid, y_valid_binary)], eval_metric = &#39;logloss&#39;, early_stopping_rounds = stop_rounds, verbose = verbose) preds_oof_fold_binary = clf_classifier.predict(X_valid) preds_test_fold_binary = clf_classifier.predict(X_test) # training clf = lgb.LGBMRegressor(**lgb_params) clf = clf.fit(X_train, y_train, eval_set = [(X_train, y_train), (X_valid, y_valid)], eval_metric = asymmetric_mse_eval, sample_weight = X_train[&#39;simulationPrice&#39;].values, eval_sample_weight = [X_train[&#39;simulationPrice&#39;].values, X_valid[&#39;simulationPrice&#39;].values], early_stopping_rounds = stop_rounds, verbose = verbose) clfs.append(clf) # inference if target_transform: preds_oof_fold = postprocess_preds(clf.predict(X_valid)**2) reals_oof_fold = y_valid**2 preds_test_fold = postprocess_preds(clf.predict(X_test)**2) / num_folds else: preds_oof_fold = postprocess_preds(clf.predict(X_valid)) reals_oof_fold = y_valid preds_test_fold = postprocess_preds(clf.predict(X_test)) / num_folds # impute zeros if train_on_positive: preds_oof_fold[(X_valid[&#39;order_sum_last_28&#39;] == 0) &amp; (X_valid[&#39;promo_in_test&#39;] == 0)] = 0 preds_test_fold[(X_test[&#39;order_sum_last_28&#39;] == 0) &amp; (X_test[&#39;promo_in_test&#39;] == 0)] = 0 # multiply with first stage predictions if two_stage: preds_oof_fold = preds_oof_fold * np.round(preds_oof_fold_binary) preds_test_fold = preds_test_fold * np.round(preds_test_fold_binary) # write predictions preds_oof[fold, :] = preds_oof_fold reals_oof[fold, :] = reals_oof_fold preds_test += preds_test_fold # save prices prices_oof[fold, :] = X.iloc[valid_idx[fold]][&#39;simulationPrice&#39;].values ##### EVALUATION # evaluation oof_rmse.append(np.sqrt(mean_squared_error(reals_oof[fold, :], preds_oof[fold, :]))) oof_profit.append(profit(reals_oof[fold, :], preds_oof[fold, :], price = X.iloc[valid_idx[fold]][&#39;simulationPrice&#39;].values)) oracle_profit.append(profit(reals_oof[fold, :], reals_oof[fold, :], price = X.iloc[valid_idx[fold]][&#39;simulationPrice&#39;].values)) # feature importance fold_importance_df = pd.DataFrame() fold_importance_df[&#39;Feature&#39;] = features fold_importance_df[&#39;Importance&#39;] = clf.feature_importances_ fold_importance_df[&#39;Fold&#39;] = fold + 1 importances = pd.concat([importances, fold_importance_df], axis = 0) # information print(&#39;-&#39; * 65) print(&#39;FOLD {:d}/{:d}: RMSE = {:.2f}, PROFIT = {:.0f}&#39;.format(fold + 1, num_folds, oof_rmse[fold], oof_profit[fold])) print(&#39;-&#39; * 65) print(&#39;&#39;) # print performance print(&#39;&#39;) print(&#39;-&#39; * 65) print(&#39;- AVERAGE RMSE: {:.2f}&#39;.format(np.mean(oof_rmse))) print(&#39;- AVERAGE PROFIT: {:.0f} ({:.2f}%)&#39;.format(np.mean(oof_profit), 100 * np.mean(oof_profit) / np.mean(oracle_profit))) print(&#39;- RUNNING TIME: {:.2f} minutes&#39;.format((time.time() - time_start) / 60)) print(&#39;-&#39; * 65) . . -- - train period days: 41 -- 151 (n = 1161393) - valid period days: 166 -- 166 (n = 10463) -- Training until validation scores don&#39;t improve for 100 rounds Early stopping, best iteration is: [532] training&#39;s binary_logloss: 0.238417 valid_1&#39;s binary_logloss: 0.347182 Training until validation scores don&#39;t improve for 100 rounds Early stopping, best iteration is: [788] training&#39;s rmse: 0.611924 training&#39;s asymmetric_mse_eval: 2.82734 valid_1&#39;s rmse: 0.98004 valid_1&#39;s asymmetric_mse_eval: 5.83945 -- FOLD 1/7: RMSE = 74.46, PROFIT = 4146664 -- ... -- - train period days: 35 -- 145 (n = 1161393) - valid period days: 160 -- 160 (n = 10463) -- Training until validation scores don&#39;t improve for 100 rounds Early stopping, best iteration is: [711] training&#39;s binary_logloss: 0.22193 valid_1&#39;s binary_logloss: 0.338637 Training until validation scores don&#39;t improve for 100 rounds Early stopping, best iteration is: [782] training&#39;s rmse: 0.600011 training&#39;s asymmetric_mse_eval: 2.71385 valid_1&#39;s rmse: 0.987073 valid_1&#39;s asymmetric_mse_eval: 5.67223 -- FOLD 7/7: RMSE = 74.46, PROFIT = 3958098 -- -- - AVERAGE RMSE: 69.66 - AVERAGE PROFIT: 3997598 (54.19%) - RUNNING TIME: 88.53 minutes -- . Looks good! The modeling pipeline took us about 1.5 hours to run. . Forecasting demand with our models results in 3,997,598 daily profit, which is about 54% of the maximum possible profit. Let&#39;s visualize the results: . # collapse-hide fig = plt.figure(figsize = (20, 7)) # residual plot plt.subplot(1, 2, 1) plt.scatter(reals_oof.reshape(-1), preds_oof.reshape(-1)) axis_lim = np.max([reals_oof.max(), preds_oof.max()]) plt.ylim(top = 1.02*axis_lim) plt.xlim(right = 1.02*axis_lim) plt.plot((0, axis_lim), (0, axis_lim), &#39;r--&#39;) plt.title(&#39;Residual Plot&#39;) plt.ylabel(&#39;Predicted demand&#39;) plt.xlabel(&#39;Actual demand&#39;) # feature importance plt.subplot(1, 2, 2) top_feats = 50 cols = importances[[&#39;Feature&#39;, &#39;Importance&#39;]].groupby(&#39;Feature&#39;).mean().sort_values(by = &#39;Importance&#39;, ascending = False)[0:top_feats].index importance = importances.loc[importances.Feature.isin(cols)] sns.barplot(x = &#39;Importance&#39;, y = &#39;Feature&#39;, data = importance.sort_values(by = &#39;Importance&#39;, ascending = False), ci = 0) plt.title(&#39;Feature Importance&#39;) plt.tight_layout() . . . The scatterplot shows that there is a space for further improvement: many predictions are far from the 45-degree line where predicted and real orders are equal. The important features mostly contain price information followed by features that count the previous orders. . We can now use predictions stored in preds_test to create a submission. Mission accomplished! . 5. Hyper-parameter tuning . One way to improve our solution is to optimize the LightGBM hyper-parameters. . We tune hyper-parameters using the hyperopt package, which performs optimization using Tree of Parzen Estimators (TPE) as a search algorithm. You don&#39;t really need to know how TPE works. As a user, you are only required to supply a parameter grid indicating the range of possible values. Compared to standard tuning methods like grid search or random search, TPE explores the search space more efficiently, allowing you to find a suitable solution faster. If you want to read more, see the package documentation here. . So, let&#39;s specify hyper-parameter ranges! We create a dictionary using the following options: . hp.choice(&#39;name&#39;, list_of_values): sets a hyper-parameter to one of the values from a list. This is suitable for hyper-parameters that can have multiple distinct values like boosting_type | hp.uniform(&#39;name&#39;, min, max): sets a hyper-parameter to a float between min and max. This works well with hyper-parameters such as learning_rate | hp.quniform(&#39;name&#39;, min, max, step): sets a hyper-parameter to a value between min and max with a step size of step. This is useful for integer parameters like max_depth | . #collapse-show # training params lgb_reg_params = { &#39;boosting_type&#39;: hp.choice(&#39;boosting_type&#39;, [&#39;gbdt&#39;, &#39;goss&#39;]), &#39;objective&#39;: &#39;rmse&#39;, &#39;metrics&#39;: &#39;rmse&#39;, &#39;n_estimators&#39;: 10000, &#39;learning_rate&#39;: hp.uniform(&#39;learning_rate&#39;, 0.0001, 0.3), &#39;max_depth&#39;: hp.quniform(&#39;max_depth&#39;, 1, 16, 1), &#39;num_leaves&#39;: hp.quniform(&#39;num_leaves&#39;, 10, 64, 1), &#39;bagging_fraction&#39;: hp.uniform(&#39;bagging_fraction&#39;, 0.3, 1), &#39;feature_fraction&#39;: hp.uniform(&#39;feature_fraction&#39;, 0.3, 1), &#39;lambda_l1&#39;: hp.uniform(&#39;lambda_l1&#39;, 0, 1), &#39;lambda_l2&#39;: hp.uniform(&#39;lambda_l2&#39;, 0, 1), &#39;silent&#39;: True, &#39;verbosity&#39;: -1, &#39;nthread&#39; : 4, &#39;random_state&#39;: 77, } # evaluation params lgb_fit_params = { &#39;eval_metric&#39;: &#39;rmse&#39;, &#39;early_stopping_rounds&#39;: 100, &#39;verbose&#39;: False, } # combine params lgb_space = dict() lgb_space[&#39;reg_params&#39;] = lgb_reg_params lgb_space[&#39;fit_params&#39;] = lgb_fit_params . . Next, we create HPOpt object that performs tuning. We can avoid this in a simple tuning task, but defining an object gives us more control of the optimization process, which is useful with a custom loss. We define three object methods: . process: runs optimization. By default, HPO uses fmin() to minimize the specified loss | lgb_reg: initializes LightGBM model | train_reg: trains LightGBM and computes the loss. Since we aim to maximize profit, we simply define loss as negative profit | . # collapse-show class HPOpt(object): # INIT def __init__(self, x_train, x_test, y_train, y_test): self.x_train = x_train self.x_test = x_test self.y_train = y_train self.y_test = y_test # optimization process def process(self, fn_name, space, trials, algo, max_evals): fn = getattr(self, fn_name) try: result = fmin(fn = fn, space = space, algo = algo, max_evals = max_evals, trials = trials) except Exception as e: return {&#39;status&#39;: STATUS_FAIL, &#39;exception&#39;: str(e)} return result, trials # LGBM initialization def lgb_reg(self, para): para[&#39;reg_params&#39;][&#39;max_depth&#39;] = int(para[&#39;reg_params&#39;][&#39;max_depth&#39;]) para[&#39;reg_params&#39;][&#39;num_leaves&#39;] = int(para[&#39;reg_params&#39;][&#39;num_leaves&#39;]) reg = lgb.LGBMRegressor(**para[&#39;reg_params&#39;]) return self.train_reg(reg, para) # training and inference def train_reg(self, reg, para): # fit LGBM reg.fit(self.x_train, self.y_train, eval_set = [(self.x_train, self.y_train), (self.x_test, self.y_test)], sample_weight = self.x_train[&#39;simulationPrice&#39;].values, eval_sample_weight = [self.x_train[&#39;simulationPrice&#39;].values, self.x_test[&#39;simulationPrice&#39;].values], **para[&#39;fit_params&#39;]) # inference if target_transform: preds = postprocess_preds(reg.predict(self.x_test)**2) reals = self.y_test**2 else: preds = postprocess_preds(reg.predict(self.x_test)) reals = self.y_test # compute loss [negative profit] loss = np.round(-profit(reals, preds, price = self.x_test[&#39;simulationPrice&#39;].values)) return {&#39;loss&#39;: loss, &#39;status&#39;: STATUS_OK} . . To prevent overfitting, we perform tuning on a different subset of data compared to the models trained in the previous section by going one day further in the past. . # collapse-hide # validation dates v_end = 158 # 1 day before last validation fold in code_03_modeling v_start = v_end # same as v_start # training dates t_start = 28 # first day in the data t_end = v_start - 15 # validation day - two weeks # extract index train_idx = list(X[(X.day_of_year &gt;= t_start) &amp; (X.day_of_year &lt;= t_end)].index) valid_idx = list(X[(X.day_of_year &gt;= v_start) &amp; (X.day_of_year &lt;= v_end)].index) # extract samples X_train, y_train = X.iloc[train_idx][features], y.iloc[train_idx] X_valid, y_valid = X.iloc[valid_idx][features], y.iloc[valid_idx] # target transformation if target_transform: y_train = np.sqrt(y_train) y_valid = np.sqrt(y_valid) # information print(&#39;-&#39; * 65) print(&#39;- train period days: {} -- {} (n = {})&#39;.format(t_start, t_end, len(train_idx))) print(&#39;- valid period days: {} -- {} (n = {})&#39;.format(v_start, v_end, len(valid_idx))) print(&#39;-&#39; * 65) . . -- - train period days: 28 -- 143 (n = 1213708) - valid period days: 158 -- 158 (n = 10463) -- . Now, we just need to instantiate the HPOpt object and launch the tuning trials! The optimization will run automatically, and we would only need to extract the optimized values: . # collapse-show # instantiate objects hpo_obj = HPOpt(X_train, X_valid, y_train, y_valid) trials = Trials() # perform tuning lgb_opt_params = hpo_obj.process(fn_name = &#39;lgb_reg&#39;, space = lgb_space, trials = trials, algo = tpe.suggest, max_evals = tuning_trials) # merge best params to fixed params params = list(lgb_opt_params[0].keys()) for par_id in range(len(params)): lgb_reg_params[params[par_id]] = lgb_opt_params[0][params[par_id]] # postprocess lgb_reg_params[&#39;boosting_type&#39;] = boost_types[lgb_reg_params[&#39;boosting_type&#39;]] lgb_reg_params[&#39;max_depth&#39;] = int(lgb_reg_params[&#39;max_depth&#39;]) lgb_reg_params[&#39;num_leaves&#39;] = int(lgb_reg_params[&#39;num_leaves&#39;]) # print best params print(&#39;Best meta-parameters:&#39;) lgb_reg_params . . Best meta-parameters: . {&#39;boosting_type&#39;: &#39;goss&#39;, &#39;objective&#39;: &#39;rmse&#39;, &#39;metrics&#39;: &#39;rmse&#39;, &#39;n_estimators&#39;: 10000, &#39;learning_rate&#39;: 0.004012, &#39;max_depth&#39;: 10, &#39;num_leaves&#39;: 64, &#39;bagging_fraction&#39;: 0.934688, &#39;feature_fraction&#39;: 0.668076, &#39;lambda_l1&#39;: 0.280133, &#39;lambda_l2&#39;: 0.589682, &#39;silent&#39;: True, &#39;verbosity&#39;: -1, &#39;nthread&#39;: 4, &#39;random_state&#39;: 77} . Done! Now we can save the optimized values and import them when setting up the model. . # collapse-hide par_file = open(&#39;../lgb_meta_params.pkl&#39;, &#39;wb&#39;) pickle.dump(lgb_reg_params, par_file) par_file.close() . . 6. Closing words . This blog post has finally come to an end. Thank you for reading! . We looked at important stages of our solution and covered steps such as data aggregation, feature engineering, custom loss functions, target transformation and hyper-parameter tuning. . Our final solution was a simple ensemble of multiple LightGBM models with different features and training options discussed in this post. If you are interested in the ensembling part, you can find the codes in my Github. . Please feel free to use the comment window below to ask questions and stay tuned for the next editions of Data Mining Cup! .",
            "url": "https://kozodoi.me/blog/20200727/demand-forecasting",
            "relUrl": "/blog/20200727/demand-forecasting",
            "date": " • Jul 27, 2020"
        }
        
    
  
    
        ,"post14": {
            "title": "Detecting Blindness with Deep Learning",
            "content": "Last update: 09.06.2022. All opinions are my own. . 1. Overview . Can deep learning help to detect blindness? . This blog post describes a project that develops a convolutional neural network (CNN) for predicting the severity of the diabetic retinopathy based on the patient&#39;s retina photos. The project was completed within the scope of the Udacity ML Engineer nano-degree program and the Kaggle competition hosted by the Asia Pacific Tele-Ophthalmology Society (APTOS). ^ The blog post provides a project walkthrough covering the following steps: . data exploration and image preprocessing to normalize images from different clinics | using transfer learning to pre-train CNN on a larger data set | employing techniques such as learning rate scheduler, test-time augmentation and others | . The modeling is performed in PyTorch. All notebooks and a PDF report are available on Github. . 2. Motivation . Diabetic retinopathy (DR) is one of the leading causes of vision loss. The World Health Organization reports that more than 300 million people worldwide have diabetes (Wong et al. 2016). In 2019, the global prevalence of DR among individuals with diabetes was at more than 25% (Thomas et al. 2019). The prevalence has been rising rapidly in developing countries. . Early detection and treatment are crucial steps towards preventing DR. The screening procedure requires a trained clinical expert to examine the fundus photographs of the patient&#39;s retina. This creates delays in diagnosis and treatment. This is especially relevant for developing countries, which often lack qualified medical staff to perform the diagnosis. Automated detection of DR can speed up the efficiency and coverage of the screening programs. . . Image source: https://www.eyeops.com/contents/our-services/eye-diseases/diabetic-retinopathy . 3. Data preparation . Data preparation is a very important step that is frequently underestimated. The quality of the input data has a strong impact on the resulting performance of the developed machine learning models. Therefore, it is crucial to take some time to look at the data and think about possible issues that should be addressed before moving on to the modeling stage. Let&#39;s do that! . Data exploration . The data set is available for the download at the competition&#39;s website. The data includes 3,662 labeled retina images of clinical patients and a test set with 1,928 images with unknown labels. . The images are labeled by a clinical expert. The integer labels indicate the severity of DR on a scale from 0 to 4, where 0 indicates no disease and 5 is the proliferative stage of DR. . Let&#39;s start by importing the data and looking at the class distribution. . #collapse-hide ##### PACKAGES import numpy as np import pandas as pd import torch import torchvision from torchvision import transforms, datasets from torch.utils.data import Dataset from PIL import Image, ImageFile ImageFile.LOAD_TRUNCATED_IMAGES = True import cv2 from tqdm import tqdm_notebook as tqdm import random import time import sys import os import math import matplotlib.pyplot as plt import seaborn as sns pd.set_option(&#39;display.max_columns&#39;, None) %matplotlib inline import warnings warnings.filterwarnings(&#39;ignore&#39;) ##### CLASS DISTRIBUTION # import data train = pd.read_csv(&#39;../input/aptos2019-blindness-detection/train.csv&#39;) test = pd.read_csv(&#39;../input/aptos2019-blindness-detection/sample_submission.csv&#39;) # plot fig = plt.figure(figsize = (15, 5)) plt.hist(train[&#39;diagnosis&#39;]) plt.title(&#39;Class Distribution&#39;) plt.ylabel(&#39;Number of examples&#39;) plt.xlabel(&#39;Diagnosis&#39;) . . The data is imbalanced: 49% images are from healthy patients. The remaining 51% are different stages of DR. The least common class is 3 (severe stage) with only 5% of the total examples. . The data is collected from multiple clinics using a variety of camera models, which creates discrepancies in the image resolution, aspect ratio and other parameters. This is demonstrated in the snippet below, where we plot histograms of image width, height and aspect ratio. . #collapse-hide # placeholder image_stats = [] # import loop for index, observation in tqdm(train.iterrows(), total = len(train)): # import image img = cv2.imread(&#39;../input/aptos2019-blindness-detection/train_images/{}.png&#39;.format(observation[&#39;id_code&#39;])) # compute stats height, width, channels = img.shape ratio = width / height # save image_stats.append(np.array((observation[&#39;diagnosis&#39;], height, width, channels, ratio))) # construct DF image_stats = pd.DataFrame(image_stats) image_stats.columns = [&#39;diagnosis&#39;, &#39;height&#39;, &#39;width&#39;, &#39;channels&#39;, &#39;ratio&#39;] # create plot fig = plt.figure(figsize = (15, 5)) # width plt.subplot(1, 3, 1) plt.hist(image_stats[&#39;width&#39;]) plt.title(&#39;(a) Image Width&#39;) plt.ylabel(&#39;Number of examples&#39;) plt.xlabel(&#39;Width&#39;) # height plt.subplot(1, 3, 2) plt.hist(image_stats[&#39;height&#39;]) plt.title(&#39;(b) Image Height&#39;) plt.ylabel(&#39;Number of examples&#39;) plt.xlabel(&#39;Height&#39;) # ratio plt.subplot(1, 3, 3) plt.hist(image_stats[&#39;ratio&#39;]) plt.title(&#39;(c) Aspect Ratio&#39;) plt.ylabel(&#39;Number of examples&#39;) plt.xlabel(&#39;Ratio&#39;) . . Now, let&#39;s look into the actual eyes! The code below creates the EyeData dataset class to import images. We also create a DataLoader object to load sample images and visualize the first batch. . #collapse-hide ##### DATASET # image preprocessing def prepare_image(path, image_size = 256): # import image = cv2.imread(path) image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # resize image = cv2.resize(image, (int(image_size), int(image_size))) # convert to tensor image = torch.tensor(image) image = image.permute(2, 1, 0) return image # dataset class EyeData(Dataset): # initialize def __init__(self, data, directory, transform = None): self.data = data self.directory = directory self.transform = transform # length def __len__(self): return len(self.data) # get items def __getitem__(self, idx): img_name = os.path.join(self.directory, self.data.loc[idx, &#39;id_code&#39;] + &#39;.png&#39;) image = prepare_image(img_name) image = self.transform(image) label = torch.tensor(self.data.loc[idx, &#39;diagnosis&#39;]) return {&#39;image&#39;: image, &#39;label&#39;: label} ##### EXAMINE SAMPLE BATCH # transformations sample_trans = transforms.Compose([transforms.ToPILImage(), transforms.ToTensor(), ]) # dataset sample = EyeData(data = train, directory = &#39;../input/aptos2019-blindness-detection/train_images&#39;, transform = sample_trans) # data loader sample_loader = torch.utils.data.DataLoader(dataset = sample, batch_size = 10, shuffle = False, num_workers = 4) # display images for batch_i, data in enumerate(sample_loader): # extract data inputs = data[&#39;image&#39;] labels = data[&#39;label&#39;].view(-1, 1) # create plot fig = plt.figure(figsize = (15, 7)) for i in range(len(labels)): ax = fig.add_subplot(2, len(labels)/2, i + 1, xticks = [], yticks = []) plt.imshow(inputs[i].numpy().transpose(1, 2, 0)) ax.set_title(labels.numpy()[i]) break . . . The illustration further emphasizes differences in the aspect ratio and lighting conditions. . The severity of DR is diagnosed by the presence of visual cues such as abnormal blood vessels, hard exudates and so-called cotton wool spots. You can read more about the diagnosing process here. Comparing the sample images, we can see the presence of exudates and cotton wool spots on some of the retina images of sick patients. . Image preprocessing . To simplify the classification task for our model, we need to ensure that retina images look similar. . First, using cameras with different aspect ratios results in some images having large black areas around the eye. The black areas do not contain information relevant for prediction and can be cropped. However, the size of black areas varies from one image to another. To address this, we develop a cropping function that converts the image to grayscale and marks black areas based on the pixel intensity. Next, we find a mask of the image by selecting rows and columns in which all pixels exceed the intensity threshold. This helps to remove vertical or horizontal rectangles filled with black similar to the ones observed in the upper-right image. After removing the black stripes, we resize the images to the same height and width. . Another issue is the eye shape. Depending on the image parameters, some eyes have a circular form, whereas others look like ovals. Since the size and shape of cues located in the retina determine the disease severity, it is crucial to standardize the eye shape as well. To do so, we develop another function that makes a circular crop around the center of the image. . Finally, we correct for the lightning and brightness discrepancies by smoothing the images using a Gaussian filter. . The snippet below provides the updated prepare_image() function that incorporates the discussed preprocessing steps. . #collapse-show ### image preprocessing function def prepare_image(path, sigmaX = 10, do_random_crop = False): # import image image = cv2.imread(path) image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # perform smart crops image = crop_black(image, tol = 7) if do_random_crop == True: image = random_crop(image, size = (0.9, 1)) # resize and color image = cv2.resize(image, (int(image_size), int(image_size))) image = cv2.addWeighted(image, 4, cv2.GaussianBlur(image, (0, 0), sigmaX), -4, 128) # circular crop image = circle_crop(image, sigmaX = sigmaX) # convert to tensor image = torch.tensor(image) image = image.permute(2, 1, 0) return image ### automatic crop of black areas def crop_black(img, tol = 7): if img.ndim == 2: mask = img &gt; tol return img[np.ix_(mask.any(1),mask.any(0))] elif img.ndim == 3: gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY) mask = gray_img &gt; tol check_shape = img[:,:,0][np.ix_(mask.any(1),mask.any(0))].shape[0] if (check_shape == 0): return img else: img1 = img[:,:,0][np.ix_(mask.any(1),mask.any(0))] img2 = img[:,:,1][np.ix_(mask.any(1),mask.any(0))] img3 = img[:,:,2][np.ix_(mask.any(1),mask.any(0))] img = np.stack([img1, img2, img3], axis = -1) return img ### circular crop around center def circle_crop(img, sigmaX = 10): height, width, depth = img.shape largest_side = np.max((height, width)) img = cv2.resize(img, (largest_side, largest_side)) height, width, depth = img.shape x = int(width / 2) y = int(height / 2) r = np.amin((x,y)) circle_img = np.zeros((height, width), np.uint8) cv2.circle(circle_img, (x,y), int(r), 1, thickness = -1) img = cv2.bitwise_and(img, img, mask = circle_img) return img ### random crop def random_crop(img, size = (0.9, 1)): height, width, depth = img.shape cut = 1 - random.uniform(size[0], size[1]) i = random.randint(0, int(cut * height)) j = random.randint(0, int(cut * width)) h = i + int((1 - cut) * height) w = j + int((1 - cut) * width) img = img[i:h, j:w, :] return img . . Next, we define a new EyeData class that uses the new processing functions and visualize a batch of sample images after corrections. . #collapse-show ##### DATASET # dataset class class EyeData(Dataset): # initialize def __init__(self, data, directory, transform = None): self.data = data self.directory = directory self.transform = transform # length def __len__(self): return len(self.data) # get items def __getitem__(self, idx): img_name = os.path.join(self.directory, self.data.loc[idx, &#39;id_code&#39;] + &#39;.png&#39;) image = prepare_image(img_name) image = self.transform(image) label = torch.tensor(self.data.loc[idx, &#39;diagnosis&#39;]) return {&#39;image&#39;: image, &#39;label&#39;: label} ##### EXAMINE SAMPLE BATCH image_size = 256 # transformations sample_trans = transforms.Compose([transforms.ToPILImage(), transforms.ToTensor(), ]) # dataset sample = EyeData(data = train, directory = &#39;../input/aptos2019-blindness-detection/train_images&#39;, transform = sample_trans) # data loader sample_loader = torch.utils.data.DataLoader(dataset = sample, batch_size = 10, shuffle = False, num_workers = 4) # display images for batch_i, data in enumerate(sample_loader): # extract data inputs = data[&#39;image&#39;] labels = data[&#39;label&#39;].view(-1, 1) # create plot fig = plt.figure(figsize = (15, 7)) for i in range(len(labels)): ax = fig.add_subplot(2, len(labels)/2, i + 1, xticks = [], yticks = []) plt.imshow(inputs[i].numpy().transpose(1, 2, 0)) ax.set_title(labels.numpy()[i]) break . . . This looks much better! Comparing the retina images to the ones before the preprocessing, we can see that the apparent discrepancies between the photos are now fixed. The eyes now have a similar circular shape, and the color scheme is more consistent. This should help the model to detect the signs of the DR. . Check out this notebook by Nakhon Ratchasima for more ideas on the image preprocessing for retina photos. The functions in this project are largely inspired by his work during the competition. . 4. Modeling . CNNs achieve state-of-the-art performance in computer vision tasks. Recent medical research also shows a high potential of CNNs in DR classification (Gulshan et al. 2016). . In this project, we employ a CNN model with the EfficientNet architecture. EfficientNet is one of the recent state-of-the-art image classification models (Tan et al. 2019). It encompasses 8 architecture variants (B0 to B7) that differ in the model complexity and default image size. . The architecture of EfficientNet B0 is visualized below. We test multiple EfficientNet architectures and use the one that demonstrates the best performance. . . The modeling pipeline consists of three stages: . Pre-training. The data set has a limited number of images (N = 3,662). We pre-train the CNN model on a larger data set from the previous Kaggle competition. | Fine-tuning. We fine-tune the model on the target data set. We use cross-validation and make modeling decisions based on the performance of the out-of-fold predictions. | Inference. We aggregate predictions of the models trained on different combinations of training folds and use test-time augmentation to further improve the performance. | . Pre-training . Due to small sample size, we can not train a complex neural architecture from scratch. This is where transfer learning comes in handy. The idea of transfer learning is to pre-train a model on a different data (source domain) and fine-tune it on a relevant data set (target domain). . A good candidate for the source domain is the ImageNet database. Most published CNN models are trained on that data. However, ImageNet images are substantially different from the retina images we want to classify. Although initializing CNN with ImageNet weights might help the network to transfer the knowledge of basic image patterns such as shapes and edges, we still need to learn a lot from the target domain. . It turns out that APTOS had hosted another Kaggle competition on the DR classification in 2015. The data set of the 2015 competition features 35,126 retina images labeled by a clinician using the same scale as the target data set. The data is available for the download here. . This enables us to use following pipeline: . initialize weights from a CNN trained on ImageNet | train the CNN on the 2015 data set | fine-tune the CNN on the 2019 data set | . Let&#39;s start modeling! First, we enable GPU support and fix random seeds. The function seed_everything() sets seed for multiple packages, including numpy and pytorch, to ensure reproducibility. . #collapse-show # GPU check train_on_gpu = torch.cuda.is_available() if not train_on_gpu: print(&#39;CUDA is not available. Training on CPU...&#39;) device = torch.device(&#39;cpu&#39;) else: print(&#39;CUDA is available. Training on GPU...&#39;) device = torch.device(&#39;cuda:0&#39;) # set seed def seed_everything(seed = 23): os.environ[&#39;PYTHONHASHSEED&#39;] = str(seed) random.seed(seed) np.random.seed(seed) torch.manual_seed(seed) torch.cuda.manual_seed(seed) torch.backends.cudnn.deterministic = True torch.backends.cudnn.benchmark = False seed = 23 seed_everything(seed) . . CUDA is available. Training on GPU... . Let&#39;s take a quick look at the class distribution in the 2015 data. . #collapse-show # import data train = pd.read_csv(&#39;../input/diabetic-retinopathy-resized/trainLabels.csv&#39;) train.columns = [&#39;id_code&#39;, &#39;diagnosis&#39;] test = pd.read_csv(&#39;../input/aptos2019-blindness-detection/train.csv&#39;) # check shape print(train.shape, test.shape) print(&#39;-&#39; * 15) print(train[&#39;diagnosis&#39;].value_counts(normalize = True)) print(&#39;-&#39; * 15) print(test[&#39;diagnosis&#39;].value_counts(normalize = True)) . . (35126, 2) (3662, 2) 0 0.734783 2 0.150658 1 0.069550 3 0.024853 4 0.020156 Name: diagnosis, dtype: float64 0 0.492900 2 0.272802 1 0.101038 4 0.080557 3 0.052703 Name: diagnosis, dtype: float64 . The imbalance in the source data is stronger than in the target data: 73% of images represent healthy patients, whereas the most severe stage of the DR is only found in 2% of the images. To address the imbalance, we will use the target data set as a validation sample during training. . We create two Dataset objects to enable different augmentations on training and inference stages: EyeTrainData and EyeTestData. The former includes a random crop that is skipped for the test data. . #collapse-hide # dataset class: train class EyeTrainData(Dataset): # initialize def __init__(self, data, directory, transform = None): self.data = data self.directory = directory self.transform = transform # length def __len__(self): return len(self.data) # get items def __getitem__(self, idx): img_name = os.path.join(self.directory, self.data.loc[idx, &#39;id_code&#39;] + &#39;.jpeg&#39;) image = prepare_image(img_name, do_random_crop = True) image = self.transform(image) label = torch.tensor(self.data.loc[idx, &#39;diagnosis&#39;]) return {&#39;image&#39;: image, &#39;label&#39;: label} # dataset class: test class EyeTestData(Dataset): # initialize def __init__(self, data, directory, transform = None): self.data = data self.directory = directory self.transform = transform # length def __len__(self): return len(self.data) # get items def __getitem__(self, idx): img_name = os.path.join(self.directory, self.data.loc[idx, &#39;id_code&#39;] + &#39;.png&#39;) image = prepare_image(img_name, do_random_crop = False) image = self.transform(image) label = torch.tensor(self.data.loc[idx, &#39;diagnosis&#39;]) return {&#39;image&#39;: image, &#39;label&#39;: label} . . We use a batch size of 20 and set the image size of 256. The choice of these parameters is a trade-off between performance and resource capacity. Feel free to try larger image and batch sizes if you have resources. . We use the following data augmentations during training: . random horizontal flip | random vertical flip | random rotation in the range [-360 degrees, 360 degrees] | . #collapse-show # parameters batch_size = 20 image_size = 256 # train transformations train_trans = transforms.Compose([transforms.ToPILImage(), transforms.RandomRotation((-360, 360)), transforms.RandomHorizontalFlip(), transforms.RandomVerticalFlip(), transforms.ToTensor() ]) # validation transformations valid_trans = transforms.Compose([transforms.ToPILImage(), transforms.ToTensor(), ]) # create datasets train_dataset = EyeTrainData(data = train, directory = &#39;../input/diabetic-retinopathy-resized/resized_train/resized_train&#39;, transform = train_trans) valid_dataset = EyeTestData(data = test, directory = &#39;../input/aptos2019-blindness-detection/train_images&#39;, transform = valid_trans) # create data loaders train_loader = torch.utils.data.DataLoader(train_dataset, batch_size = batch_size, shuffle = True, num_workers = 4) valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size = batch_size, shuffle = False, num_workers = 4) . . Next, let&#39;s instantiate the EfficentNet model. We use B4 architecture and initialize pre-trained ImageNet weights by downloading the model parameters in the PyTorch format. The convolutional part of the network responsible for feature extraction outputs a tensor with 1792 features. To adapt the CNN to our task, we replace the last fully-connected classification layer with a (1792, 5) fully-connected layer. . The CNN is instantiated with init_model(). The argument train ensures that we load ImageNet weights on the training stage and turn off gradient computation on the inference stage. . #collapse-show # model name model_name = &#39;enet_b4&#39; # initialization function def init_model(train = True): ### training mode if train == True: # load pre-trained model model = EfficientNet.from_pretrained(&#39;efficientnet-b4&#39;, num_classes = 5) ### inference mode if train == False: # load pre-trained model model = EfficientNet.from_name(&#39;efficientnet-b4&#39;) model._fc = nn.Linear(model._fc.in_features, 5) # freeze layers for param in model.parameters(): param.requires_grad = False return model # check architecture model = init_model() print(model) . . Since we are dealing with a multiple classification problem, we use cross-entropy as a loss function. We use nn.CrossEntropyLoss() which combines logsoftmax and negative log-likelihood loss and applies them to the output of the last network layer. . The Kaggle competition uses Cohen&#39;s kappa for evaluation. Kappa measures the agreement between the actual and predicted labels. Since Kappa is non-differentiable, we can not use it as a loss function. At the same time, we can use Kappa to evaluate the performance and early stop the training epochs. . We use Adam optimizer with a starting learning rate of 0.001. During training, we use a learning rate scheduler, which multiplies the learning rate by 0.5 after every 5 epochs. This helps to make smaller changes to the network weights when we are getting closer to the optimum. . #collapse-show # loss function criterion = nn.CrossEntropyLoss() # epochs max_epochs = 15 early_stop = 5 # learning rates eta = 1e-3 # scheduler step = 5 gamma = 0.5 # optimizer optimizer = optim.Adam(model.parameters(), lr = eta) scheduler = lr_scheduler.StepLR(optimizer, step_size = step, gamma = gamma) # initialize model and send to GPU model = init_model() model = model.to(device) . . After each training epoch, we validate the model on the target data. We extract class scores from the last fully-connected layer and predict the image class corresponding to the highest score. We train the network for 15 epochs, tracking the validation loss and Cohen&#39;s kappa. If the kappa does not increase for 5 consecutive epochs, we terminate the training process and save model weights for the epoch associated with the highest validation kappa. . #collapse-show # placeholders oof_preds = np.zeros((len(test), 5)) val_kappas = [] val_losses = [] trn_losses = [] bad_epochs = 0 # timer cv_start = time.time() # training and validation loop for epoch in range(max_epochs): ##### PREPARATION # timer epoch_start = time.time() # reset losses trn_loss = 0.0 val_loss = 0.0 # placeholders fold_preds = np.zeros((len(test), 5)) ##### TRAINING # switch regime model.train() # loop through batches for batch_i, data in enumerate(train_loader): # extract inputs and labels inputs = data[&#39;image&#39;] labels = data[&#39;label&#39;].view(-1) inputs = inputs.to(device, dtype = torch.float) labels = labels.to(device, dtype = torch.long) optimizer.zero_grad() # forward and backward pass with torch.set_grad_enabled(True): preds = model(inputs) loss = criterion(preds, labels) loss.backward() optimizer.step() # compute loss trn_loss += loss.item() * inputs.size(0) ##### INFERENCE # switch regime model.eval() # loop through batches for batch_i, data in enumerate(valid_loader): # extract inputs and labels inputs = data[&#39;image&#39;] labels = data[&#39;label&#39;].view(-1) inputs = inputs.to(device, dtype = torch.float) labels = labels.to(device, dtype = torch.long) # compute predictions with torch.set_grad_enabled(False): preds = model(inputs).detach() fold_preds[batch_i * batch_size:(batch_i + 1) * batch_size, :] = preds.cpu().numpy() # compute loss loss = criterion(preds, labels) val_loss += loss.item() * inputs.size(0) # save predictions oof_preds = fold_preds # scheduler step scheduler.step() ##### EVALUATION # evaluate performance fold_preds_round = fold_preds.argmax(axis = 1) val_kappa = metrics.cohen_kappa_score(test[&#39;diagnosis&#39;], fold_preds_round.astype(&#39;int&#39;), weights = &#39;quadratic&#39;) # save perfoirmance values val_kappas.append(val_kappa) val_losses.append(val_loss / len(test)) trn_losses.append(trn_loss / len(train)) ##### EARLY STOPPING # display info print(&#39;- epoch {}/{} | lr = {} | trn_loss = {:.4f} | val_loss = {:.4f} | val_kappa = {:.4f} | {:.2f} min&#39;.format( epoch + 1, max_epochs, scheduler.get_lr()[len(scheduler.get_lr()) - 1], trn_loss / len(train), val_loss / len(test), val_kappa, (time.time() - epoch_start) / 60)) # check if there is any improvement if epoch &gt; 0: if val_kappas[epoch] &lt; val_kappas[epoch - bad_epochs - 1]: bad_epochs += 1 else: bad_epochs = 0 # save model weights if improvement if bad_epochs == 0: oof_preds_best = oof_preds.copy() torch.save(model.state_dict(), &#39;../models/model_{}.bin&#39;.format(model_name)) # break if early stop if bad_epochs == early_stop: print(&#39;Early stopping. Best results: loss = {:.4f}, kappa = {:.4f} (epoch {})&#39;.format( np.min(val_losses), val_kappas[np.argmin(val_losses)], np.argmin(val_losses) + 1)) print(&#39;&#39;) break # break if max epochs if epoch == (max_epochs - 1): print(&#39;Did not met early stopping. Best results: loss = {:.4f}, kappa = {:.4f} (epoch {})&#39;.format( np.min(val_losses), val_kappas[np.argmin(val_losses)], np.argmin(val_losses) + 1)) print(&#39;&#39;) break # load best predictions oof_preds = oof_preds_best # print performance print(&#39;&#39;) print(&#39;Finished in {:.2f} minutes&#39;.format((time.time() - cv_start) / 60)) . . Loaded pretrained weights for efficientnet-b4 - epoch 1/15 | lr = 0.001 | trn_loss = 0.7140 | val_loss = 1.3364 | val_kappa = 0.7268 | 30.03 min - epoch 2/15 | lr = 0.001 | trn_loss = 0.6447 | val_loss = 1.0670 | val_kappa = 0.8442 | 27.19 min - epoch 3/15 | lr = 0.001 | trn_loss = 0.6203 | val_loss = 0.7667 | val_kappa = 0.7992 | 27.21 min - epoch 4/15 | lr = 0.001 | trn_loss = 0.6020 | val_loss = 0.7472 | val_kappa = 0.8245 | 27.68 min - epoch 5/15 | lr = 0.00025 | trn_loss = 0.5899 | val_loss = 0.7720 | val_kappa = 0.8541 | 29.42 min - epoch 6/15 | lr = 0.0005 | trn_loss = 0.5542 | val_loss = 0.9255 | val_kappa = 0.8682 | 29.33 min - epoch 7/15 | lr = 0.0005 | trn_loss = 0.5424 | val_loss = 0.8917 | val_kappa = 0.8763 | 29.91 min - epoch 8/15 | lr = 0.0005 | trn_loss = 0.5359 | val_loss = 0.9555 | val_kappa = 0.8661 | 30.70 min - epoch 9/15 | lr = 0.0005 | trn_loss = 0.5252 | val_loss = 0.8642 | val_kappa = 0.8778 | 28.76 min - epoch 10/15 | lr = 0.000125 | trn_loss = 0.5184 | val_loss = 1.1568 | val_kappa = 0.8403 | 31.14 min - epoch 11/15 | lr = 0.00025 | trn_loss = 0.4974 | val_loss = 0.9464 | val_kappa = 0.8784 | 28.00 min - epoch 12/15 | lr = 0.00025 | trn_loss = 0.4874 | val_loss = 0.9043 | val_kappa = 0.8820 | 27.50 min - epoch 13/15 | lr = 0.00025 | trn_loss = 0.4820 | val_loss = 0.7924 | val_kappa = 0.8775 | 27.36 min - epoch 14/15 | lr = 0.00025 | trn_loss = 0.4758 | val_loss = 0.9300 | val_kappa = 0.8761 | 27.33 min - epoch 15/15 | lr = 6.25e-05 | trn_loss = 0.4693 | val_loss = 0.9109 | val_kappa = 0.8803 | 27.51 min Did not met early stopping. Best results: loss = 0.7472, kappa = 0.8245 (epoch 4) Finished in 429.16 minutes . Training on the Kaggle GPU-enabled machine took us about 7 hours! Let&#39;s visualize the training and validation loss dynamics. . #collapse-show # plot size fig = plt.figure(figsize = (15, 5)) # plot loss dynamics plt.subplot(1, 2, 1) plt.plot(trn_losses, &#39;red&#39;, label = &#39;Training&#39;) plt.plot(val_losses, &#39;green&#39;, label = &#39;Validation&#39;) plt.xlabel(&#39;Epoch&#39;) plt.ylabel(&#39;Loss&#39;) plt.legend() # plot kappa dynamics plt.subplot(1, 2, 2) plt.plot(val_kappas, &#39;blue&#39;, label = &#39;Kappa&#39;) plt.xlabel(&#39;Epoch&#39;) plt.ylabel(&#39;Kappa&#39;) plt.legend() . . . The cross-entropy loss on the validation set reaches minimum already after 3 epochs. At the same time, kappa continues to increase up to the 15th epoch. Since we use kappa to evaluate the quality of our solution, we save weights after 15 epochs. . We also construct a confusion matrix of the trained model. The numbers in the cells are percentages. According to the results, the model does a poor job in distinguishing the mild and moderate stages of DR: 86% of images with mild DR are classified as moderate. The best performance is observed for healthy patients. Overall, we see that the model tends to confuse nearby severity stages but rarely misclassifies the proliferate and mild stages. . . Fine-tuning . Fine-tuning on the target data is performed within 4-fold cross-validation. To ensure that we have enough examples of each class, we perform cross-validation with stratification. . On each iteration, we instantiate the EfficientNet B4 model with the same architecture as in the previous section. Next, we load the saved weights from the model pre-trained on the source data. We freeze weights on all network layers except for the last fully-connected layer. The weights in this layer are fine-tuned. As on the pre-training stage, we use Adam optimizer and implement a learning rate scheduler. We also track performance on the validation folds and stop training if kappa does not increase for 5 consecutive epochs. . The process is repeated for each of the 4 folds, and the best model weights are saved for each combination of the training folds. . The init_model() is updated to load the weights saved on the pre-training stage and freeze the first layers of the network in the training regime. . #collapse-show # model name model_name = &#39;enet_b4&#39; # initialization function def init_model(train = True, trn_layers = 2): ### training mode if train == True: # load pre-trained model model = EfficientNet.from_pretrained(&#39;efficientnet-b4&#39;, num_classes = 5) model.load_state_dict(torch.load(&#39;../models/model_{}.bin&#39;.format(model_name, 1))) # freeze first layers for child in list(model.children())[:-trn_layers]: for param in child.parameters(): param.requires_grad = False ### inference mode if train == False: # load pre-trained model model = EfficientNet.from_pretrained(&#39;efficientnet-b4&#39;, num_classes = 5) model.load_state_dict(torch.load(&#39;../models/model_{}.bin&#39;.format(model_name, 1))) # freeze all layers for param in model.parameters(): param.requires_grad = False return model # check architecture model = init_model() . . The training loop is now wrapped into a cross-validation loop. . #collapse-hide ##### VALIDATION SETTINGS # no. folds num_folds = 4 # creating splits skf = StratifiedKFold(n_splits = num_folds, shuffle = True, random_state = seed) splits = list(skf.split(train[&#39;id_code&#39;], train[&#39;diagnosis&#39;])) # placeholders oof_preds = np.zeros((len(train), 1)) # timer cv_start = time.time() ##### PARAMETERS # loss function criterion = nn.CrossEntropyLoss() # epochs max_epochs = 15 early_stop = 5 # learning rates eta = 1e-3 # scheduler step = 5 gamma = 0.5 ##### CROSS-VALIDATION LOOP for fold in tqdm(range(num_folds)): ####### DATA PREPARATION # display information print(&#39;-&#39; * 30) print(&#39;FOLD {}/{}&#39;.format(fold + 1, num_folds)) print(&#39;-&#39; * 30) # load splits data_train = train.iloc[splits[fold][0]].reset_index(drop = True) data_valid = train.iloc[splits[fold][1]].reset_index(drop = True) # create datasets train_dataset = EyeTrainData(data = data_train, directory = &#39;../input/aptos2019-blindness-detection/train_images&#39;, transform = train_trans) valid_dataset = EyeTrainData(data = data_valid, directory = &#39;../input/aptos2019-blindness-detection/train_images&#39;, transform = valid_trans) # create data loaders train_loader = torch.utils.data.DataLoader(train_dataset, batch_size = batch_size, shuffle = True, num_workers = 4) valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size = batch_size, shuffle = False, num_workers = 4) ####### MODEL PREPARATION # placeholders val_kappas = [] val_losses = [] trn_losses = [] bad_epochs = 0 # load best OOF predictions if fold &gt; 0: oof_preds = oof_preds_best.copy() # initialize and send to GPU model = init_model(train = True) model = model.to(device) # optimizer optimizer = optim.Adam(model._fc.parameters(), lr = eta) scheduler = lr_scheduler.StepLR(optimizer, step_size = step, gamma = gamma) ####### TRAINING AND VALIDATION LOOP for epoch in range(max_epochs): ##### PREPARATION # timer epoch_start = time.time() # reset losses trn_loss = 0.0 val_loss = 0.0 # placeholders fold_preds = np.zeros((len(data_valid), 1)) ##### TRAINING # switch regime model.train() # loop through batches for batch_i, data in enumerate(train_loader): # extract inputs and labels inputs = data[&#39;image&#39;] labels = data[&#39;label&#39;].view(-1) inputs = inputs.to(device, dtype = torch.float) labels = labels.to(device, dtype = torch.long) optimizer.zero_grad() # forward and backward pass with torch.set_grad_enabled(True): preds = model(inputs) loss = criterion(preds, labels) loss.backward() optimizer.step() # compute loss trn_loss += loss.item() * inputs.size(0) ##### INFERENCE # initialize model.eval() # loop through batches for batch_i, data in enumerate(valid_loader): # extract inputs and labels inputs = data[&#39;image&#39;] labels = data[&#39;label&#39;].view(-1) inputs = inputs.to(device, dtype = torch.float) labels = labels.to(device, dtype = torch.long) # compute predictions with torch.set_grad_enabled(False): preds = model(inputs).detach() _, class_preds = preds.topk(1) fold_preds[batch_i * batch_size:(batch_i + 1) * batch_size, :] = class_preds.cpu().numpy() # compute loss loss = criterion(preds, labels) val_loss += loss.item() * inputs.size(0) # save predictions oof_preds[splits[fold][1]] = fold_preds # scheduler step scheduler.step() ##### EVALUATION # evaluate performance fold_preds_round = fold_preds val_kappa = metrics.cohen_kappa_score(data_valid[&#39;diagnosis&#39;], fold_preds_round.astype(&#39;int&#39;), weights = &#39;quadratic&#39;) # save perfoirmance values val_kappas.append(val_kappa) val_losses.append(val_loss / len(data_valid)) trn_losses.append(trn_loss / len(data_train)) ##### EARLY STOPPING # display info print(&#39;- epoch {}/{} | lr = {} | trn_loss = {:.4f} | val_loss = {:.4f} | val_kappa = {:.4f} | {:.2f} min&#39;.format( epoch + 1, max_epochs, scheduler.get_lr()[len(scheduler.get_lr()) - 1], trn_loss / len(data_train), val_loss / len(data_valid), val_kappa, (time.time() - epoch_start) / 60)) # check if there is any improvement if epoch &gt; 0: if val_kappas[epoch] &lt; val_kappas[epoch - bad_epochs - 1]: bad_epochs += 1 else: bad_epochs = 0 # save model weights if improvement if bad_epochs == 0: oof_preds_best = oof_preds.copy() torch.save(model.state_dict(), &#39;../models/model_{}_fold{}.bin&#39;.format(model_name, fold + 1)) # break if early stop if bad_epochs == early_stop: print(&#39;Early stopping. Best results: loss = {:.4f}, kappa = {:.4f} (epoch {})&#39;.format( np.min(val_losses), val_kappas[np.argmin(val_losses)], np.argmin(val_losses) + 1)) print(&#39;&#39;) break # break if max epochs if epoch == (max_epochs - 1): print(&#39;Did not meet early stopping. Best results: loss = {:.4f}, kappa = {:.4f} (epoch {})&#39;.format( np.min(val_losses), val_kappas[np.argmin(val_losses)], np.argmin(val_losses) + 1)) print(&#39;&#39;) break # load best predictions oof_preds = oof_preds_best # print performance print(&#39;&#39;) print(&#39;Finished in {:.2f} minutes&#39;.format((time.time() - cv_start) / 60)) . . FOLD 1/4 Loaded pretrained weights for efficientnet-b4 - epoch 1/15 | lr = 0.001 | trn_loss = 0.6279 | val_loss = 0.5368 | val_kappa = 0.8725 | 7.30 min - epoch 2/15 | lr = 0.001 | trn_loss = 0.5699 | val_loss = 0.5402 | val_kappa = 0.8662 | 7.25 min - epoch 3/15 | lr = 0.001 | trn_loss = 0.5572 | val_loss = 0.5380 | val_kappa = 0.8631 | 7.31 min - epoch 4/15 | lr = 0.001 | trn_loss = 0.5482 | val_loss = 0.5357 | val_kappa = 0.8590 | 7.29 min - epoch 5/15 | lr = 0.00025 | trn_loss = 0.5658 | val_loss = 0.5357 | val_kappa = 0.8613 | 7.25 min - epoch 6/15 | lr = 0.0005 | trn_loss = 0.5537 | val_loss = 0.5346 | val_kappa = 0.8604 | 7.28 min Early stopping. Best results: loss = 0.5346, kappa = 0.8604 (epoch 6) FOLD 2/4 Loaded pretrained weights for efficientnet-b4 - epoch 1/15 | lr = 0.001 | trn_loss = 0.6535 | val_loss = 0.5295 | val_kappa = 0.8767 | 7.24 min - epoch 2/15 | lr = 0.001 | trn_loss = 0.5691 | val_loss = 0.5158 | val_kappa = 0.8717 | 7.20 min - epoch 3/15 | lr = 0.001 | trn_loss = 0.5645 | val_loss = 0.5136 | val_kappa = 0.8732 | 7.23 min - epoch 4/15 | lr = 0.001 | trn_loss = 0.5592 | val_loss = 0.5151 | val_kappa = 0.8705 | 7.26 min - epoch 5/15 | lr = 0.00025 | trn_loss = 0.5530 | val_loss = 0.5213 | val_kappa = 0.8686 | 7.27 min - epoch 6/15 | lr = 0.0005 | trn_loss = 0.5499 | val_loss = 0.5143 | val_kappa = 0.8733 | 7.21 min Early stopping. Best results: loss = 0.5136, kappa = 0.8732 (epoch 3) FOLD 3/4 Loaded pretrained weights for efficientnet-b4 - epoch 1/15 | lr = 0.001 | trn_loss = 0.6503 | val_loss = 0.5286 | val_kappa = 0.8937 | 7.16 min - epoch 2/15 | lr = 0.001 | trn_loss = 0.5916 | val_loss = 0.5166 | val_kappa = 0.8895 | 7.20 min - epoch 3/15 | lr = 0.001 | trn_loss = 0.5702 | val_loss = 0.5115 | val_kappa = 0.8834 | 7.34 min - epoch 4/15 | lr = 0.001 | trn_loss = 0.5606 | val_loss = 0.5133 | val_kappa = 0.8829 | 7.44 min - epoch 5/15 | lr = 0.00025 | trn_loss = 0.5640 | val_loss = 0.5081 | val_kappa = 0.8880 | 7.28 min - epoch 6/15 | lr = 0.0005 | trn_loss = 0.5515 | val_loss = 0.5109 | val_kappa = 0.8871 | 7.20 min Early stopping. Best results: loss = 0.5081, kappa = 0.8880 (epoch 5) FOLD 4/4 Loaded pretrained weights for efficientnet-b4 - epoch 1/15 | lr = 0.001 | trn_loss = 0.6379 | val_loss = 0.5563 | val_kappa = 0.8595 | 7.18 min - epoch 2/15 | lr = 0.001 | trn_loss = 0.5718 | val_loss = 0.5423 | val_kappa = 0.8610 | 7.20 min - epoch 3/15 | lr = 0.001 | trn_loss = 0.5590 | val_loss = 0.5433 | val_kappa = 0.8587 | 7.19 min - epoch 4/15 | lr = 0.001 | trn_loss = 0.5554 | val_loss = 0.5433 | val_kappa = 0.8579 | 7.17 min - epoch 5/15 | lr = 0.00025 | trn_loss = 0.5476 | val_loss = 0.5393 | val_kappa = 0.8608 | 7.18 min - epoch 6/15 | lr = 0.0005 | trn_loss = 0.5509 | val_loss = 0.5331 | val_kappa = 0.8610 | 7.32 min - epoch 7/15 | lr = 0.0005 | trn_loss = 0.5532 | val_loss = 0.5309 | val_kappa = 0.8567 | 7.28 min Early stopping. Best results: loss = 0.5309, kappa = 0.8567 (epoch 7) Finished in 181.29 minutes . The model converges rather quickly. The best validation performance is obtained after 3 to 7 training epochs depending on a fold. . Let&#39;s look at the confusion matrix. The matrix illustrates the advantages of the fine-tuned model over the pre-trained CNN and indicates a better performance in classifying mild stages of the DR. However, we also observe that the model classifies too many examples as moderate (class = 2). . #collapse-hide # construct confusion matrx oof_preds_round = oof_preds.copy() cm = confusion_matrix(train[&#39;diagnosis&#39;], oof_preds_round) cm = cm.astype(&#39;float&#39;) / cm.sum(axis = 1)[:, np.newaxis] annot = np.around(cm, 2) # plot matrix fig, ax = plt.subplots(figsize = (8, 6)) sns.heatmap(cm, cmap = &#39;Blues&#39;, annot = annot, lw = 0.5) ax.set_xlabel(&#39;Prediction&#39;) ax.set_ylabel(&#39;Ground Truth&#39;) ax.set_aspect(&#39;equal&#39;) . . . Inference . Let&#39;s now produce some predictions for the test set! . We aggregate predictions from the models trained during the cross-validation loop. To do so, we extract class scores from the last fully-connected layer and define class predictions as the classes with the maximal score. Next, we average predictions of the 4 networks trained on different combinations of the training folds. . #collapse-hide ##### TRANSFORMATIONS # parameters batch_size = 25 image_size = 256 # test transformations test_trans = transforms.Compose([transforms.ToPILImage(), transforms.RandomHorizontalFlip(), transforms.RandomVerticalFlip(), transforms.ToTensor()]) ##### DATA LOADER # create dataset test_dataset = EyeTestData(data = test, directory = &#39;../input/aptos2019-blindness-detection/test_images&#39;, transform = test_trans) # create data loader test_loader = torch.utils.data.DataLoader(test_dataset, batch_size = batch_size, shuffle = False, num_workers = 4) ##### MODEL ARCHITECTURE # model name model_name = &#39;enet_b4&#39; # initialization function def init_model(train = True, trn_layers = 2): ### training mode if train == True: # load pre-trained model model = EfficientNet.from_pretrained(&#39;efficientnet-b4&#39;, num_classes = 5) # freeze first layers for child in list(model.children())[:-trn_layers]: for param in child.parameters(): param.requires_grad = False ### inference mode if train == False: # load pre-trained model model = EfficientNet.from_name(&#39;efficientnet-b4&#39;) model._fc = nn.Linear(model._fc.in_features, 5) # freeze all layers for param in model.parameters(): param.requires_grad = False ### return model return model # check architecture model = init_model(train = False) . . We also use test-time augmentations by creating 4 versions of the test images with random augmentations (horizontal and vertical flips) and average predictions over the image variants. The final prediction is an average of 4 models times 4 image variants. . #collapse-show # validation settings num_folds = 4 tta_times = 4 # placeholders test_preds = np.zeros((len(test), num_folds)) cv_start = time.time() # prediction loop for fold in tqdm(range(num_folds)): # load model and sent to GPU model = init_model(train = False) model.load_state_dict(torch.load(&#39;../models/model_{}_fold{}.bin&#39;.format(model_name, fold + 1))) model = model.to(device) model.eval() # placeholder fold_preds = np.zeros((len(test), 1)) # loop through batches for _ in range(tta_times): for batch_i, data in enumerate(test_loader): inputs = data[&#39;image&#39;] inputs = inputs.to(device, dtype = torch.float) preds = model(inputs).detach() _, class_preds = preds.topk(1) fold_preds[batch_i * batch_size:(batch_i + 1) * batch_size, :] += class_preds.cpu().numpy() fold_preds = fold_preds / tta_times # aggregate predictions test_preds[:, fold] = fold_preds.reshape(-1) # print performance test_preds_df = pd.DataFrame(test_preds.copy()) print(&#39;Finished in {:.2f} minutes&#39;.format((time.time() - cv_start) / 60)) . . Let&#39;s have a look at the distribution of predictions: . #collapse-hide # show predictions print(&#39;-&#39; * 45) print(&#39;PREDICTIONS&#39;) print(&#39;-&#39; * 45) print(test_preds_df.head()) # show correlation print(&#39;-&#39; * 45) print(&#39;CORRELATION MATRIX&#39;) print(&#39;-&#39; * 45) print(np.round(test_preds_df.corr(), 4)) print(&#39;Mean correlation = &#39; + str(np.round(np.mean(np.mean(test_preds_df.corr())), 4))) # show stats print(&#39;-&#39; * 45) print(&#39;SUMMARY STATS&#39;) print(&#39;-&#39; * 45) print(test_preds_df.describe()) # show prediction distribution print(&#39;-&#39; * 45) print(&#39;ROUNDED PREDICTIONS&#39;) print(&#39;-&#39; * 45) for f in range(num_folds): print(np.round(test_preds_df[f]).astype(&#39;int&#39;).value_counts(normalize = True)) print(&#39;-&#39; * 45) # plot densities test_preds_df.plot.kde() . . PREDICTIONS 0 1 2 3 0 2.0 2.0 2.0 1.5 1 2.0 2.5 2.0 2.0 2 2.0 2.0 2.0 2.0 3 2.0 3.0 2.0 2.0 4 2.0 2.0 2.0 2.0 CORRELATION MATRIX 0 1 2 3 0 1.0000 0.9624 0.9573 0.9534 1 0.9624 1.0000 0.9686 0.9490 2 0.9573 0.9686 1.0000 0.9478 3 0.9534 0.9490 0.9478 1.0000 Mean correlation = 0.9673 SUMMARY STATS 0 1 2 3 count 1928.0000 1928.0000 1928.0000 1928.0000 mean 1.6867 1.7103 1.7152 1.6501 std 0.9727 0.9730 0.9704 0.9606 min 0.0000 0.0000 0.0000 0.0000 25% 1.5000 2.0000 2.0000 1.3750 50% 2.0000 2.0000 2.0000 2.0000 75% 2.0000 2.0000 2.0000 2.0000 max 4.0000 4.0000 4.0000 4.0000 ROUNDED PREDICTIONS 2 0.6654 0 0.2002 1 0.0471 3 0.0440 4 0.0430 Name: 0, dtype: float64 2 0.6701 0 0.1950 4 0.0461 3 0.0451 1 0.0435 Name: 1, dtype: float64 2 0.6789 0 0.1950 4 0.0487 1 0.0389 3 0.0383 Name: 2, dtype: float64 2 0.6856 0 0.2079 1 0.0420 4 0.0409 3 0.0233 Name: 3, dtype: float64 . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f8e403032d0&gt; . The model classifies a lot of images as moderate DR. To reduce the number of such examples, we can change thresholds used to round the averaged predictions into classes. We use the following vector of thresholds: [0.5, 1.75, 2.25, 3.5]. The final prediction is set to zero if the average value is below 0.5; set to one if the average value lies in [0.5, 1.75), etc. This reduces the share of images classified as moderate DR. . #collapse-show # aggregate predictions test_preds = test_preds_df.mean(axis = 1).values # set cutoffs coef = [0.5, 1.75, 2.25, 3.5] # rounding for i, pred in enumerate(test_preds): if pred &lt; coef[0]: test_preds[i] = 0 elif pred &gt;= coef[0] and pred &lt; coef[1]: test_preds[i] = 1 elif pred &gt;= coef[1] and pred &lt; coef[2]: test_preds[i] = 2 elif pred &gt;= coef[2] and pred &lt; coef[3]: test_preds[i] = 3 else: test_preds[i] = 4 . . We are done! We can now export test_preds as csv and submit it to the competition. . 5. Closing words . This blog post provides a complete walkthrough on the project on detecting blindness in the retina images using CNNs. We use image preprocessing to reduce discrepancies across images taken in different clinics, apply transfer learning to leverage knowledge from larger data sets and implement different techniques to improve performance. . If you are still reading this post, you might be wondering about ways to further improve the solution. There are multiple options. First, employing a larger network architecture and increasing the number of training epochs on the pre-training stage has a high potential for a better performance. At the same time, this would require more computing power, which might not be optimal considering the possible use of the automated retina image classification in practice. . Second, image preprocessing can be further improved. During the refinement process, the largest performance gains were attributed to different preprocessing steps. This is a more efficient way to further improve the performance. . Finally, the best solutions of other competitors rely on ensembles of CNNs using different image sizes and/or architectures. Incorporating multiple heterogeneous models and blending their predictions could also improve the proposed solution. Ensembling predictions of models similar to the one discussed in this post is what helped me to place in the top 9% of the leaderboard. .",
            "url": "https://kozodoi.me/blog/20200711/blindness-detection",
            "relUrl": "/blog/20200711/blindness-detection",
            "date": " • Jul 11, 2020"
        }
        
    
  
    
        ,"post15": {
            "title": "Algorithmic Fairness in R",
            "content": "Last update: 18.10.2021. All opinions are my own. . 1. Overview . How to measure fairness of a machine learning model? . To date, a number of algorithmic fairness metrics have been proposed. Demographic parity, proportional parity and equalized odds are among the most commonly used metrics to evaluate fairness across sensitive groups in binary classification problems. Multiple other metrics have been proposed based on performance measures extracted from the confusion matrix (e.g., false positive rate parity, false negative rate parity). . Together with Tibor V. Varga, we developed fairness package for R. The package provides tools to calculate fairness metrics across different sensitive groups. It also provides opportunities to visualize and compare other prediction metrics between the groups. . This blog post provides a tutorial on using the fairness package on a COMPAS data set. The package is published on CRAN and GitHub. . The package implements the following fairness metrics: . Demographic parity (also known as independence) | Proportional parity | Equalized odds (also known as separation) | Predictive rate parity | False positive rate parity | False negative rate parity | Accuracy parity | Negative predictive value parity | Specificity parity | ROC AUC parity | MCC parity | . 2. Installation . The latest stable version published on CRAN is fairness 1.2.2 (as of 14.04.2021). You can install this from CRAN by running: . install.packages(&#39;fairness&#39;) library(fairness) . Installing package into ‘/usr/local/lib/R/site-library’ (as ‘lib’ is unspecified) . You may also install the development version from Github to get the latest features: . devtools::install_github(&#39;kozodoi/fairness&#39;) library(fairness) . 3. Data description . The package includes two exemplary data sets to study fairness: compas and germancredit. . compas . This tutorial uses a simplified version of the landmark COMPAS data set containing the criminal history of defendants from Broward County. You can read more about the data here. To load the data, all you need to do is: . data(&#39;compas&#39;) head(compas) . Two_yr_RecidivismNumber_of_PriorsAge_Above_FourtyFiveAge_Below_TwentyFiveFemaleMisdemeanorethnicityprobabilitypredicted . 1no | -0.68435 | no | no | Male | yes | Other | 0.31515 | 0 | . 2yes | 2.26688 | no | no | Male | no | Caucasian | 0.88546 | 1 | . 3no | -0.68435 | no | no | Female | yes | Caucasian | 0.25526 | 0 | . 4no | -0.68435 | no | no | Male | no | African_American | 0.41739 | 0 | . 5no | -0.68435 | no | no | Male | yes | Hispanic | 0.32009 | 0 | . The data set contains nine variables. The outcome variable is Two_yr_Recidivism, a binary indicator showing whether an individual committed a crime within the two-year period. The data also includes features on prior criminal record (Number_of_Priors, Misdemeanor), features describing age (Age_Above_FourtyFive, Age_Below_TwentyFive), sex and ethnicity (Female, ethnicity). . For illustrative purposes, we have already trained a classifier that uses all features to predict Two_yr_Recidivism and concatenated the predicted probabilities (probability) and predicted classes (predicted) to the data frame. Feel free to use these columns with predictions to test different fairness metrics before evaluating a custom model. . germancredit . The second included data set is a credit scoring data set labeled as germancredit. The data includes 20 features describing the loan applicants and a binary outcome variable BAD indicating whether the applicant defaulted on a loan. Similarly to COMPAS, germancredit also includes two columns with model predictions named probability and predicted. The data can be loaded with: . data(&#39;germancredit&#39;) . 4. Train a classifier . For the purpose of this tutorial, we will train two classifiers using different sets of features: . model that uses all features as input | model that uses all features except for ethnicity | . We partition the COMPAS data into training and validation subsets and use logistic regression as a base classifier. . #collapse-show # extract data compas &lt;- fairness::compas df &lt;- compas[, !(colnames(compas) %in% c(&#39;probability&#39;, &#39;predicted&#39;))] # partitioning params set.seed(77) val_percent &lt;- 0.3 val_idx &lt;- sample(1:nrow(df))[1:round(nrow(df) * val_percent)] # partition the data df_train &lt;- df[-val_idx, ] df_valid &lt;- df[ val_idx, ] # check dim print(nrow(df_train)) print(nrow(df_valid)) . . [1] 4320 [1] 1852 . #collapse-show # fit logit models model1 &lt;- glm(Two_yr_Recidivism ~ ., data = df_train, family = binomial(link = &#39;logit&#39;)) model2 &lt;- glm(Two_yr_Recidivism ~ . -ethnicity, data = df_train, family = binomial(link = &#39;logit&#39;)) . . Let&#39;s append model predictions to the validation set. Later, we will evaluate fairness of the two models based on these predictions. . #collapse-show # produce predictions df_valid$prob_1 &lt;- predict(model1, df_valid, type = &#39;response&#39;) df_valid$prob_2 &lt;- predict(model2, df_valid, type = &#39;response&#39;) head(df_valid) . . Two_yr_RecidivismNumber_of_PriorsAge_Above_FourtyFiveAge_Below_TwentyFiveFemaleMisdemeanorethnicityprob_1prob_2 . 1no | -0.68435 | no | no | Male | no | African_American | 0.36787 | 0.34815 | . 2no | 2.05607 | no | no | Male | no | Hispanic | 0.80241 | 0.83477 | . 3yes | -0.47355 | no | yes | Male | no | African_American | 0.58958 | 0.57305 | . 4no | -0.68435 | yes | no | Male | no | African_American | 0.23956 | 0.22189 | . 5yes | 0.58045 | no | no | Male | no | Caucasian | 0.59155 | 0.60107 | . 5. Intro to algorithimc fairness . An outlook on the confusion matrix . Most fairness metrics are calculated based on a confusion matrix produced by a classification model. The confusion matrix is comprised of four classes: . True positives (TP): the true class is positive and the prediction is positive (correct classification) | False positives (FP): the true class is negative and the prediction is positive (incorrect classification) | True negatives (TN): the true class is negative and the prediction is negative (correct classification) | False negatives (FN): the true class is positive and the prediction is negative (incorrect classification) | . Fairness metrics are calculated by comparing one or more of these measures across sensitive subgroups (e.g., male and female). For a detailed overview of measures coming from the confusion matrix and precise definitions, click here or here. . Fairness metrics functions . The package implements 11 fairness metrics. Many of these are mutually exclusive: results for a given classification problem often cannot be fair in terms of all metrics. Depending on a context, it is important to select an appropriate metric to evaluate fairness. . Below, we describe functions used to compute the implemented metrics. Every function has a similar set of arguments: . data: data.frame containing the input data and model predictions | group: column name indicating the sensitive group (factor variable) | base: base level of the sensitive group for fairness metrics calculation | outcome: column name indicating the binary outcome variable | outcome_base: base level of the outcome variable (i.e., negative class) for fairness metrics calculation | . We also need to supply model predictions. Depending on the metric, we need to provide either probabilistic predictions as probs or class predictions as preds. The model predictions can be appended to the original data.frame or provided as a vector. In this tutorial, we will use probabilistic predictions with all functions. When working with probabilistic predictions, some metrics require a cutoff value to convert probabilities into class predictions supplied as cutoff. . The package also supports a continuous group variable (e.g., age). If group is continuous, a user need to supply group_breaks argument to specify breaks in the variable values. More details are provided in the functions documentation. . Before looking at different metrics, we will create a binary numeric version of the outcome variable that we will supply as outcome in fairness metrics functions. We can also work with an original factor outcome Two_yr_Recidivism but in this case we should make sure that predictions and outcome have the same factor levels. . df_valid$Two_yr_Recidivism_01 &lt;- ifelse(df_valid$Two_yr_Recidivism == &#39;yes&#39;, 1, 0) . 6. Computing fairness metrics . Predictive rate parity . Let&#39;s demonstrate the fairness pipeline using predictive rate parity as an example. Predictive rate parity is achieved if the precisions (or positive predictive values) in the subgroups are close to each other. The precision stands for the number of the true positives divided by the total number of examples predicted positive within a group. . Formula: TP / (TP + FP) . Let&#39;s compute predictive rate parity for the first model that uses all features: . res1 &lt;- pred_rate_parity(data = df_valid, outcome = &#39;Two_yr_Recidivism_01&#39;, outcome_base = &#39;0&#39;, group = &#39;ethnicity&#39;, probs = &#39;prob_1&#39;, cutoff = 0.5, base = &#39;Caucasian&#39;) res1$Metric . CaucasianAfrican_AmericanAsianHispanicNative_AmericanOther . Precision 0.585034 | 0.702381 | 0.5000000 | 0.5909091 | 1.000000 | 0.900000 | . Predictive Rate Parity 1.000000 | 1.200581 | 0.8546512 | 1.0100423 | 1.709302 | 1.538372 | . Group size622 | 962 | 160 | 1440 | 4 | 104 | . The first row shows the raw precision values for all ethnicities. The second row displays the relative precisions compared to Caucasian defendants. . In a perfect world, all predictive rate parities should be equal to one, which would mean that precision in every group is the same as in the base group. In practice, values are going to be different. The parity above one indicates that precision in this group is relatively higher, whereas a lower parity implies a lower precision. Observing a large variance in parities should hint us that the model is not performing equally well for different groups. . If the other ethnic group is set as a base group (e.g. Hispanic), the raw precision values do not change, only the relative metrics: . res1h &lt;- pred_rate_parity(data = df_valid, outcome = &#39;Two_yr_Recidivism_01&#39;, outcome_base = &#39;0&#39;, group = &#39;ethnicity&#39;, probs = &#39;prob_1&#39;, cutoff = 0.5, base = &#39;Hispanic&#39;) res1h$Metric . HispanicCaucasianAfrican_AmericanAsianNative_AmericanOther . Precision 0.5909091 | 0.5850340 | 0.702381 | 0.5000000 | 1.000000 | 0.900000 | . Predictive Rate Parity 1.0000000 | 0.9900576 | 1.188645 | 0.8461538 | 1.692308 | 1.523077 | . Group size1440 | 6220 | 962 | 160 | 4 | 104 | . Overall, results suggest that the model precision varies between 0.5 and 1. The lowest precision is observed for Asian defendants. This implies that there are more cases where the model mistakingly predicts that a person will commit a crime among Asians than among, e.g., Native_American defendants. . A standard output of every fairness metric function includes a barchart that visualizes the relative metrics for all subgroups: . res1h$Metric_plot . Some fairness metrics do not require probabilistic predictions and can work with class predictions. When predicted probabilities are supplied, the output includes a density plot displaying the distributions of probabilities in all subgroups: . res1h$Probability_plot . Let&#39;s now compare the results to the second model that does not use ethnicity as a feature: . # model 2 res2 &lt;- pred_rate_parity(data = df_valid, outcome = &#39;Two_yr_Recidivism_01&#39;, outcome_base = &#39;0&#39;, group = &#39;ethnicity&#39;, probs = &#39;prob_2&#39;, cutoff = 0.5, base = &#39;Caucasian&#39;) res2$Metric . CaucasianAfrican_AmericanAsianHispanicNative_AmericanOther . Precision 0.5933333 | 0.703854 | 0.4000000 | 0.5142857 | 1.000000 | 0.625000 | . Predictive Rate Parity 1.0000000 | 1.186271 | 0.6741573 | 0.8667737 | 1.685393 | 1.053371 | . Group size6220 | 962 | 160 | 1440 | 4 | 104 | . We can see two things. . First, excluding ethnicity from the features slightly increases precision for some defendants (Caucasian and African_American) but results in a lower precision for some other groups (Asian and Hispanic). This illustrates that improving a model for one group may cost a fall in the predictive performance for the general population. Depending on the context, it is a task of a decision-maker to decide what is best. . Second, excluding ethnicity does not align the predictive rate parities substantially closer to one. This illustrates another important research finding: removing a sensitive variable does not guarantee that a model stops discriminating. Ethnicity correlates with other features and is still implicitly included in the input data. In order to make the classifier more fair, one would need to consider more sophisticated techniques than simply dropping the sensitive attribute. . In the rest of this tutorial, we will go through the functions that cover the remaining implemented fairness metrics, illustrating the corresponding equations and outputs. You can find more details on each of the fairness metric functions in the package documentation. Please don&#39;t hesitate to use the built-in helper to see further details and examples on the implemented metrics: . ?fairness::pred_rate_parity . Demographic parity . Demographic parity is one of the most popular fairness indicators in the literature. Demographic parity is achieved if the absolute number of positive predictions in the subgroups are close to each other. This measure does not take true class into consideration and only depends on the model predictions. In some literature, demographic parity is also referred to as statistical parity or independence. . Formula: (TP + FP) . res_dem &lt;- dem_parity(data = df_valid, outcome = &#39;Two_yr_Recidivism_01&#39;, outcome_base = &#39;0&#39;, group = &#39;ethnicity&#39;, probs = &#39;prob_1&#39;, cutoff = 0.5, base = &#39;Caucasian&#39;) res_dem$Metric . CaucasianAfrican_AmericanAsianHispanicNative_AmericanOther . Positively classified147 | 504 | 2.00000000 | 22.0000000 | 1.000000000 | 10.00000000 | . Demographic Parity 1 | 3.428571 | 0.01360544 | 0.1496599 | 0.006802721 | 0.06802721 | . Group size622 | 962 | 1600 | 1440 | 4000 | 10400 | . res_dem$Metric_plot . Of course, comparing the absolute number of positive predictions will show a high disparity when the number of cases within each group is different, which artificially boosts the disparity. This is true in our case: . table(df_valid$ethnicity) . Caucasian African_American Asian Hispanic 622 962 16 144 Native_American Other 4 104 . To address this, we can use proportional parity. . Proportional parity . Proportional parity is very similar to demographic parity but modifies it to address the issue discussed above. Proportional parity is achieved if the proportion of positive predictions in the subgroups are close to each other. Similar to the demographic parity, this measure also does not depend on the true labels. . Formula: (TP + FP) / (TP + FP + TN + FN) . res_prop &lt;- prop_parity(data = df_valid, outcome = &#39;Two_yr_Recidivism_01&#39;, outcome_base = &#39;0&#39;, group = &#39;ethnicity&#39;, probs = &#39;prob_1&#39;, cutoff = 0.5, base = &#39;Caucasian&#39;) res_prop$Metric . CaucasianAfrican_AmericanAsianHispanicNative_AmericanOther . Proportion 0.2363344 | 0.5239085 | 0.1250000 | 0.1527778 | 0.250000 | 0.09615385 | . Proportional Parity 1.0000000 | 2.2168102 | 0.5289116 | 0.6464475 | 1.057823 | 0.40685505 | . Group size6220 | 962 | 160 | 1440 | 4 | 10400 | . res_prop$Metric_plot . The proportional parity still shows that African-American defendants are treated unfairly by our model. At the same time, the disparity is lower compared to the one observed with the demographic parity. . All the remaining fairness metrics account for both model predictions and the true labels. . Equalized odds . Equalized odds, also known as separation, are achieved if the sensitivities in the subgroups are close to each other. The group-specific sensitivities indicate the number of the true positives divided by the total number of positives in that group. . Formula: TP / (TP + FN) . res_eq &lt;- equal_odds(data = df_valid, outcome = &#39;Two_yr_Recidivism_01&#39;, outcome_base = &#39;0&#39;, group = &#39;ethnicity&#39;, probs = &#39;prob_1&#39;, cutoff = 0.5, base = &#39;African_American&#39;) res_eq$Metric . African_AmericanCaucasianAsianHispanicNative_AmericanOther . Sensitivity0.6996047 | 0.3659574 | 0.2500000 | 0.2600000 | 0.5000000 | 0.2142857 | . Equalized odds1.0000000 | 0.5230917 | 0.3573446 | 0.3716384 | 0.7146893 | 0.3062954 | . Group size962 | 6220 | 16 | 144 | 4 | 1040 | . Accuracy parity . Accuracy parity is achieved if the accuracies (all accurately classified examples divided by the total number of examples) in the subgroups are close to each other. . Formula: (TP + TN) / (TP + FP + TN + FN) . res_acc &lt;- acc_parity(data = df_valid, outcome = &#39;Two_yr_Recidivism_01&#39;, group = &#39;ethnicity&#39;, probs = &#39;prob_1&#39;, cutoff = 0.5, base = &#39;African_American&#39;) res_acc$Metric . African_AmericanCaucasianAsianHispanicNative_AmericanOther . Accuracy0.6860707 | 0.6623794 | 0.750000 | 0.6805556 | 0.750000 | 0.6730769 | . Accuracy Parity1.0000000 | 0.9654682 | 1.093182 | 0.9919613 | 1.093182 | 0.9810606 | . Group size962 | 6220 | 16 | 144 | 4 | 1040 | . False negative rate parity . False negative rate parity is achieved if the false negative rates (the ratio between the number of false negatives and the total number of positives) in the subgroups are close to each other. . Formula: FN / (TP + FN) . res_fnr &lt;- fnr_parity(data = df_valid, outcome = &#39;Two_yr_Recidivism_01&#39;, outcome_base = &#39;0&#39;, group = &#39;ethnicity&#39;, probs = &#39;prob_1&#39;, cutoff = 0.5, base = &#39;African_American&#39;) res_fnr$Metric . African_AmericanCaucasianAsianHispanicNative_AmericanOther . FNR 0.3003953 | 0.6340426 | 0.750000 | 0.740000 | 0.500000 | 0.7857143 | . FNR Parity 1.0000000 | 2.1106943 | 2.496711 | 2.463421 | 1.664474 | 2.6156015 | . Group size962 | 6220 | 16 | 144 | 4 | 1040 | . False positive rate parity . False positive rate parity is achieved if the false positive rates (the ratio between the number of false positives and the total number of negatives) in the subgroups are close to each other. . Formula: FP / (TN + FP) . res_fpr &lt;- fpr_parity(data = df_valid, outcome = &#39;Two_yr_Recidivism_01&#39;, outcome_base = &#39;0&#39;, group = &#39;ethnicity&#39;, probs = &#39;prob_1&#39;, cutoff = 0.5, base = &#39;African_American&#39;) res_fpr$Metric . African_AmericanCaucasianAsianHispanicNative_AmericanOther . FPR 0.3289474 | 0.1576227 | 0.08333333 | 0.09574468 | 0 | 0.01612903 | . FPR Parity 1.0000000 | 0.4791731 | 0.25333333 | 0.29106383 | 0 | 0.04903226 | . Group size962 | 6220 | 1600 | 14400 | 4 | 10400 | . Negative predictive value parity . Negative predictive value parity is achieved if the negative predictive values in the subgroups are close to each other. The negative predictive value is computed as a ratio between the number of true negatives and the total number of predicted negatives. This function can be considered the ‘inverse’ of the predictive rate parity. . Formula: TN / (TN + FN) . res_npv &lt;- npv_parity(data = df_valid, outcome = &#39;Two_yr_Recidivism_01&#39;, outcome_base = &#39;0&#39;, group = &#39;ethnicity&#39;, probs = &#39;prob_1&#39;, cutoff = 0.5, base = &#39;African_American&#39;) res_npv$Metric . African_AmericanCaucasianAsianHispanicNative_AmericanOther . NPV 0.6681223 | 0.6863158 | 0.7857143 | 0.6967213 | 0.6666667 | 0.6489362 | . NPV Parity 1.0000000 | 1.0272308 | 1.1760037 | 1.0428051 | 0.9978214 | 0.9712835 | . Group size962 | 6220 | 160 | 1440 | 40 | 1040 | . Specificity parity . Specificity parity is achieved if the specificities (the ratio of the number of the true negatives and the total number of negatives) in the subgroups are close to each other. This function can be considered the ‘inverse’ of the equalized odds. . Formula: TN / (TN + FP) . res_sp &lt;- spec_parity(data = df_valid, outcome = &#39;Two_yr_Recidivism_01&#39;, outcome_base = &#39;0&#39;, group = &#39;ethnicity&#39;, probs = &#39;prob_1&#39;, cutoff = 0.5, base = &#39;African_American&#39;) res_sp$Metric . African_AmericanCaucasianAsianHispanicNative_AmericanOther . Specificity 0.6710526 | 0.8423773 | 0.9166667 | 0.9042553 | 1.000000 | 0.983871 | . Specificity Parity 1.0000000 | 1.2553073 | 1.3660131 | 1.3475177 | 1.490196 | 1.466161 | . Group size962 | 6220 | 160 | 1440 | 4 | 104 | . Apart from the parity-based metrics presented above, two additional comparisons are implemented: ROC AUC comparison and Matthews correlation coefficient comparison. . ROC AUC parity . This function calculates ROC AUC and visualizes ROC curves for all subgroups. Note that probabilities must be defined for this function. Also, as ROC evaluates all possible cutoffs, the cutoff argument is excluded from this function. . res_auc &lt;- roc_parity(data = df_valid, outcome = &#39;Two_yr_Recidivism_01&#39;, group = &#39;Female&#39;, probs = &#39;prob_1&#39;, base = &#39;Male&#39;) res_auc$Metric . . MaleFemale . ROC AUC 0.7221429 | 0.7192349 | . ROC AUC Parity 1.0000000 | 0.9959731 | . Group size151 | 337 | . Apart from the standard outputs, the function also returns ROC curves for each of the subgroups: . res_auc$ROCAUC_plot . Matthews correlation coefficient parity . The Matthews correlation coefficient (MCC) takes all four classes of the confusion matrix into consideration. MCC is sometimes referred to as the single most powerful metric in binary classification problems, especially for data with class imbalances. . Formula: (TP×TN-FP×FN)/√((TP+FP)×(TP+FN)×(TN+FP)×(TN+FN)) . res_mcc &lt;- mcc_parity(data = df_valid, outcome = &#39;Two_yr_Recidivism_01&#39;, outcome_base = &#39;0&#39;, group = &#39;Female&#39;, probs = &#39;prob_1&#39;, cutoff = 0.5, base = &#39;Male&#39;) res_mcc$Metric . MaleFemale . MCC0.3316558 | 0.2893650 | . MCC Parity1.0000000 | 0.8724859 | . 7. Closing words . You have read through the fairness R package tutorial! By now, you should have a solid grip on algorithmic group fairness metrics. . We hope that you will be able to use the R package in your data analysis! Please let me know if you run into any issues while working with the package in the comments below or on GitHub. Please also feel free to contact the authors if you have any feedback. . Acknowlegments: . Calders, T., &amp; Verwer, S. (2010). Three naive Bayes approaches for discrimination-free classification. Data Mining and Knowledge Discovery, 21(2), 277-292. | Chouldechova, A. (2017). Fair prediction with disparate impact: A study of bias in recidivism prediction instruments. Big data, 5(2), 153-163. | Feldman, M., Friedler, S. A., Moeller, J., Scheidegger, C., &amp; Venkatasubramanian, S. (2015, August). Certifying and removing disparate impact. In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (pp. 259-268). ACM. | Friedler, S. A., Scheidegger, C., Venkatasubramanian, S., Choudhary, S., Hamilton, E. P., &amp; Roth, D. (2018). A comparative study of fairness-enhancing interventions in machine learning. arXiv preprint arXiv:1802.04422. | Zafar, M. B., Valera, I., Gomez Rodriguez, M., &amp; Gummadi, K. P. (2017, April). Fairness beyond disparate treatment &amp; disparate impact: Learning classification without disparate mistreatment. In Proceedings of the 26th International Conference on World Wide Web (pp. 1171-1180). International World Wide Web Conferences Steering Committee. | .",
            "url": "https://kozodoi.me/blog/20200501/fairness-tutorial",
            "relUrl": "/blog/20200501/fairness-tutorial",
            "date": " • May 1, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About",
          "content": "Hi, I am Nikita! . Click here to download my CV and check out my portfolio to see my work. . Click here to download my CV and check out my portfolio to see my work. . About me . I am Applied ML Scientist working on the frontier of research and business. With PhD in ML and certified AWS expertise, rich experience in diverse areas of ML, and strong communication and public talking skills, I am passionate about using AI to solve challenging business problems and create value. . Currently, I work at AWS, where I design and build cutting-edge Generative AI and ML solutions to solve diverse business problems across industries. My team interacts with large-scale customers to identify, design, build and deploy high-value custom and reusable AI use cases. Before AWS, I completed my PhD at the Humboldt University of Berlin, where I worked on ML applications for credit risk analytics in collaboration with Monedo. . In my free time, I enjoy playing beach volleyball and challenging myself in ML competitions. . Curriculum Vitae . . . About this website . This website hosts my blog, where I share ML tutorials, competitive ML tricks, and interesting project findings. All opinions published here are my own. The website includes different sections featuring my work: . 📁 my portfolio with ML projects on different topics | 🗣 my tech talks with links to presentation slides and talk videos | 📚 my ML publications with paper abstracts and full-text PDFs | 🥇 my Kaggle solutions with links to code and write-ups | 🖥 my certifications with links to certificates and completed courses | 🎓 my teaching experience at the Humboldt University of Berlin | . If you like my blog, you can buy me a cup of tea to support my work. Thanks! . . . . Contact . Would like to have a chat? Click here to send me an e-mail. . I am also happy to connect on different social and professional platforms. Click the badges below to see my profile. . | | | | | .",
          "url": "https://kozodoi.me/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  

  
      ,"page4": {
          "title": "Portfolio",
          "content": ". Portfolio . My public ML portfolio includes projects on different topics, including generative AI, NLP, computer vision, and tabular data. To see more of my work, visit my GitHub page, download my CV or check out the about page. . My public ML portfolio includes projects on different topics, including generative AI, NLP, computer vision, and tabular data. To see more of my work, visit my GitHub page, download my CV or check out the about page. . . My portfolio features the following projects: . &#128214; Text reading complexity prediction with transformers | &#129516; Image-to-text translation of chemical structures with deep learning | &#128200; Fair machine learning in credit scoring applications | . Scroll down to see more generative AI and ML projects grouped by application domains. Click &quot;read more&quot; to see project summaries, and follow GitHub links for code and documentation. . . . Text Readability Prediction with Transformers . Highlights . developed a comprehensive PyTorch / HuggingFace text classification pipeline | build multiple transformers including BERT and RoBERTa with custom pooling layers | implemented an interactive web app for custom text reading complexity estimation | . Tags: natural language processing, deep learning, web app . . Summary . Estimating text reading complexity is a crucial task for school teachers. Offering students text passages at the right level of challenge is important for facilitating a fast development of reading skills. The existing tools to estimate text complexity rely on weak proxies and heuristics, which results in a suboptimal accuracy. In this project, I use deep learning to predict the readability scores of text passages. . My solution implements eight transformer models, including BERT, RoBERTa and others in PyTorch. The models feature a custom regression head that uses a concatenated output of multiple hidden layers. The modeling pipeline includes text augmentations such as sentence order shuffle, backtranslation and injecting target noise. The solution places in the top-9% of the Kaggle competition leaderboard. . The project also includes an interactive web app built in Python. The app allows to estimate reading complexity of a custom text using two of the trained transformer models. The code and documentation are available on GitHub. . &#128220; Read more &#128187; GitHub repo &#128202; Web app &#128203; Blog post . . Image-to-Text Translation of Molecules with Deep Learning . Highlights . built a CNN-LSTM encoder-decoder architecture to translate images into chemical formulas | developed a comprehensive PyTorch GPU/TPU image captioning pipeline | finished in the top-5% of the Kaggle competition leaderboard with a silver medal | . Tags: computer vision, natural language processing, deep learning . . Summary . Organic chemists frequently draw molecular work using structural graph notations. As a result, decades of scanned publications and medical documents contain drawings not annotated with chemical formulas. Time-consuming manual work of experts is required to reliably convert such images into machine-readable formulas. Automated recognition of optical chemical structures could speed up research and development in the field. . The goal of this project is to develop a deep learning based algorithm for chemical image captioning. In other words, the project aims at translating unlabeled chemical images into the text formula strings. To do that, I work with a large dataset of more than 4 million chemical images provided by Bristol-Myers Squibb. . My solution is an ensemble of CNN-LSTM Encoder-Decoder models implemented in PyTorch.The solution reaches the test score of 1.31 Levenstein Distance and places in the top-5% of the competition leaderboard. The code is documented and published on GitHub. . &#128220; Read more &#128187; GitHub repo &#128214; Writeup on Kaggle . . Fair Machine Learning in Credit Scoring . Highlights . benchmarked eight fair ML algorithms on seven credit scoring data sets | investigated profit-fairness trade-off to quantify the cost of fairness | published a paper with the results at the European Journal of Operational Research | . Tags: tabular data, fairness, profit maximization . . Summary . The rise of algorithmic decision-making has spawned much research on fair ML. In this project, I focus on fairness of credit scoring decisions and make three contributions: . revisiting statistical fairness criteria and examining their adequacy for credit scoring | cataloging algorithmic options for incorporating fairness goals in the ML model development pipeline | empirically comparing different fairness algorithms in a profit-oriented credit scoring context using real-world data | . The code and documentation are available on GitHub. A detailed walkthrough and key results are published in this paper. . The study reveals that multiple fairness criteria can be approximately satisfied at once and recommends separation as a proper criterion for measuring scorecard fairness. It also finds fair in-processors to deliver a good profit-fairness balance and shows that algorithmic discrimination can be reduced to a reasonable level at a relatively low cost. . &#128220; Read more &#128187; GitHub repo &#128213; Paper . Further projects . Want to see more? Check out my further ML projects grouped by application areas below. You can also visit my GitHub page, check my recent blog posts, watch public tech talks and read academic publications. . Generative AI Medical Content Generation with LLMs . developed LLM-powered end-to-end content generation solution on AWS | the system provides generation, revision, and fact-checking capabilities | implemented an interactive web app and backend as infrastructure-as-code | . &#128203; Blog post Improving RAG with User Interaction Tool . integrated a human feedback tool into a RAG agent | published a blog post and the code of the solution | . &#128187; GitHub repo &#128203; Blog post Computer Vision Pet Popularity Prediction . built a PyTorch pipeline for predicting pet cuteness from image and tabular data | reached top-4% in the Kaggle competition using Transformers and CNNs | implemented an interactive web app for estimating cuteness of custom pet photos | . &#128187; GitHub repo &#128202; Web app Cassava Leaf Disease Classification . built CNNs and Vision Transformers in PyTorch to classify plant diseases | constructed a stacking ensemble with multiple computer vision models | finished in the top-1% of the Kaggle competition with a gold medal | . &#128187; GitHub repo &#128214; Writeup on Kaggle Catheter and Tube Position Detection on Chest X-Rays . built deep learning models to detect catheter and tube position on X-ray images | developed a comprehensive PyTorch GPU/TPU computer vision pipeline | finished in the top-5% of the Kaggle competition leaderboard with silver medal | . &#128187; GitHub repo &#128214; Writeup on Kaggle Detecting Blindness on Retina Photos . developed CNN models to identify disease types from retina photos | written a detailed report covering problem statement, EDA and modeling | submitted as a capstone project within the Udacity ML Engineer program | . &#128187; GitHub repo &#128203; Blog post &#128214; Detailed report Tabular Data and Graph ML Probabilistic Demand Forecasting with Graph Neural Networks . developed a novel probabilistic demand forecasting architecture combining GNNs and DeepAR | proposed article similarity based data-driven similarity matrix construction | published a paper demonstrating monetary and accuracy gains from the proposed model | . &#128213; Paper &#128202; Slides Profit-Driven Demand Forecasting with Gradient Boosting . developed a two-stage demand forecasting pipeline with LightGBM models | performed a thorough cleaning, aggregation and feature engineering on transactional data | implemented custom loss functions aimed at maximizing the retailer&#39;s profit | . &#128187; GitHub repo &#128203; Blog post Google Analytics Customer Revenue Prediction . worked with two-year transactional data from a Google merchandise store | developed LightGBM models to predict future revenues generated by customers | finished in the top-2% of the Kaggle competition leaderboard with silver medal | . &#128187; GitHub repo Software Packages fairness: Package for Computing Fair ML Metrics . developing and actively maintaining an R package for fair machine learning | the package offers calculation, visualization and comparison of algorithmic fairness metrics | the package is published on CRAN and has more than 16k total downloads | . &#128187; GitHub repo &#128203; Blog post dptools: Package for Data Processing and Feature Engineering . Python package with helper functions to simplify common data processing tasks | functions cover feature engineering, data aggregation, working with missings and more | the source code and documentation are available on GitHub and PyPi | . &#128187; GitHub",
          "url": "https://kozodoi.me/portfolio/",
          "relUrl": "/portfolio/",
          "date": ""
      }
      
  

  
      ,"page5": {
          "title": "Talks",
          "content": "Talks . I regularly give talks on AI and ML topics at academic conferences, AI meetups and other events. Below, you can find links to videos and slides of selected talks. . I regularly give talks on AI and ML topics at academic conferences, AI meetups and other events. Below, you can find links to videos and slides of selected talks. . . This page overviews my public talks including: . &#128483; Talks at AI meetups | &#128218; Talks at ML conferences | &#127891; Teaching-related talks | . . . . Meetups . AI for Credit Risk Analytics GuildData, 2024 (Berlin) | . . Abstract: In financial institutions, ML-based credit scorecards are only trained over the labeled data of previously accepted applicants, whose repayment behavior has been observed. This creates sampling bias: the training data represent a limited region of the distribution on which the model is deployed for screening new customers. In this talk, I will illustrate the adverse impact of sampling bias on training and evaluation of scoring models. I will also overview possible methods to address this problem. | &#128220; Abstract &#128202; Slides | . . Building Chatbots That Know All About Your Business: Retrieval Augmented Generation StreetSmart AI, 2024 (Berlin) | . . Abstract: Generative AI is rapidly spreading across different industries. Retrieval Augmented Generation (RAG) is one of the most popular applications of generative AI and LLMs. In this talk, we will explain how to build conversational assistants that leverage RAG to tap into your company’s data and provide accurate answers on user questions. We will walk through the main components of RAG systems and discuss important techniques to improve and evaluate their quality. | &#128220; Abstract &#128202; Slides | . . Five Techniques for Improving RAG Chatbots DataTalksClub, 2024 (Berlin) | . . Abstract: Retrieval Augmented Generation (RAG) is one of the most popular applications of generative AI and LLMs. Enhancing retrievers is key to improve quality of RAG conversational assistants. In this talk, I will walk you through the main components of RAG systems and dive intro five practical techniques to improve the retrieval quality. | &#128220; Abstract &#128202; Slides &#128249; Video | . . Detecting AI-Generated Texts Generative AI on AWS, 2023 (San Francisco) | . . Abstract: Large language models excel in generating realistic text, which emphasizes the need for systems that detect whether a text is generated or written by a human. Detecting generated text is crucial in many applications such as identifying fake news, filtering product reviews, and assessing student assignments. In this talk, I will discuss typical differences between generated and human text, and overview the prominent state-of-the-art text detection approaches, including watermarking, supervised and zero-shot methods. We will also touch on limitations of the existing detectors and discuss emerging approaches to evade them. | &#128220; Abstract &#128249; Video | . . Lessons for Industry from Kaggle Competitions Street Smart AI, 2023 (Berlin) | . . Abstract: Kaggle is an online platform that hosts large-scale ML competitions. In this talk, I discuss lessons and best practices used by data scientists who compete on Kaggle and other platforms. I will cover lessons and insights that are useful in the industry and can be leveraged in various ML projects. | &#128220; Abstract &#128202; Slides &#128249; Video | . . Fighting Sampling Bias in ML Models in Credit Scoring Solving Real World Problems via ML, 2021 (Berlin) | . . Abstract: Machine learning is widely used to support decision-making in financial institutions. Credit scorecards are a prominent example. Such models are trained over the labeled data of previously accepted applicants, whose repayment behavior has been observed and ignore the rejected applicants. This creates sampling bias: the training data represent a limited region of the distribution on which the model is deployed for screening new customers. In this talk, I will illustrate the adverse impact of sampling bias on training and evaluation of scoring models. I will also overview possible methods to address this problem. | &#128220; Abstract &#128202; Slides &#128249; Video | . . . . . Conferences . Probabilistic Demand Forecasting with Graph Neural Networks Workshop at European Conference on Machine Learning and PKDD, 2023 (Turin) | . . Abstract: Demand forecasting is a prominent business use case that allows retailers to optimize inventory planning, logistics, and core business decisions. One of the key challenges in demand forecasting is accounting for relationships and interactions between articles. Most modern forecasting approaches provide independent article-level predictions that do not consider the impact of related articles. Recent research has attempted addressing this challenge using Graph Neural Networks (GNNs) and showed promising results. This paper builds on previous research on GNNs and makes two contributions. First, we integrate a GNN encoder into a state-of-the-art DeepAR model. The combined model produces probabilistic forecasts, which are crucial for decision-making under uncertainty. Second, we propose to build graphs using article attribute similarity, which avoids reliance on a pre-defined graph structure. Experiments on three real-world datasets show that the proposed approach consistently outperforms non-graph benchmarks. We also show that our approach produces article embeddings that encode article similarity and demand dynamics and are useful for other downstream business tasks beyond forecasting. | &#128220; Abstract &#128202; Slides | . . Active Learning for Reject Inference in Credit Scoring Credit Scoring and Credit Control Conference, 2021 (Edinburgh) | . . Abstract: Credit scoring models are trained on data of previously accepted credit applications, where the borrowers&#39; repayment behavior has been observed. This creates sampling bias: the training data is not representative of the general population of borrowers. The sampling bias deteriorates the model&#39;s performance when the model is used to screen new applications. We use active learning (AL) to develop a dynamic bias correction framework. When screening incoming credit applications, AL algorithm identifies unlabeled cases (i.e., applications rejected by a scorecard) that should be labeled (i.e., granted a loan) based on the expected impact on the scoring model. Issuing a loan to risky customers incurs additional cost for the lender. At the same time, augmenting the training data with applications from the unexplored distribution regions reduces sampling bias and improves the scorecard’s performance on future credit applications. We perform an empirical study on synthetic and real-world credit scoring data to test the suggested AL-driven reject inference framework and investigate the trade-off between the labeling costs and performance gains. Preliminary results indicate that Al can improve scorecard performance and profitability. | &#128220; Abstract &#128202; Slides &#128249; Video | . . Multi-Objective Particle Swarm Optimization for Feature Selection in Credit Scoring Workshop at European Conference on Machine Learning and PKDD, 2020 (Ghent) | . . Abstract: Credit scoring refers to the use of statistical models to support loan approval decisions. An ever-increasing availability of data on potential borrowers emphasizes the importance of feature selection for scoring models. Traditionally, feature selection has been viewed as a single-objective task. Recent research demonstrates the effectiveness of multi-objective approaches. We propose a novel multi-objective feature selection framework for credit scoring that extends previous work by taking into account data acquisition costs and employing a state-of-the-art particle swarm optimization algorithm. Our framework optimizes three fitness functions: the number of features, data acquisition costs and the AUC. Experiments on nine credit scoring data sets demonstrate a highly competitive performance of the proposed framework. | &#128220; Abstract &#128214; Paper &#128202; Slides &#128249; Video | . . Shallow Self-Learning for Reject Inference in Credit Scoring European Conference on Machine Learning and PKDD, 2019 (Würzburg) | . . Abstract: Credit scoring models support loan approval decisions in the financial services industry. Lenders train these models on data from previously granted credit applications, where the borrowers’ repayment behavior has been observed. This approach creates sample bias. The scoring model is trained on accepted cases only. Applying the model to screen applications from the population of all borrowers degrades its performance. Reject inference comprises techniques to overcome sampling bias through assigning labels to rejected cases. This paper makes two contributions. First, we propose a self-learning framework for reject inference. The framework is geared toward real-world credit scoring requirements through considering distinct training regimes for labeling and model training. Second, we introduce a new measure to assess the effectiveness of reject inference strategies. Our measure leverages domain knowledge to avoid artificial labeling of rejected cases during evaluation. We demonstrate this approach to offer a robust and operational assessment of reject inference. Experiments on a real-world credit scoring data set confirm the superiority of the suggested self-learning framework over previous reject inference strategies. We also find strong evidence in favor of the proposed evaluation measure assessing reject inference strategies more reliably, raising the performance of the eventual scoring model. | &#128220; Abstract &#128214; Paper &#128202; Slides &#129703; Poster | . . Profit-Oriented Feature Selection in Credit Scoring Applications Conference on Operations Research, 2018 (Brussels) | . . Abstract: In credit scoring, feature selection aims at removing irrelevant data to improve the performance of the scorecard and its interpretability. Standard feature selection techniques are based on statistical criteria such as correlation. Recent studies suggest that using profit-based indicators for model evaluation may improve the quality of scoring models for businesses. We extend the use of profit measures to feature selection and develop a wrapper-based framework that uses the Expected Maximum Profit measure (EMP) as a fitness function. Experiments on multiple credit scoring data sets provide evidence that EMP-maximizing feature selection helps to develop scorecards that yield a higher expected profit compared to conventional feature selection strategies. | &#128220; Abstract &#128214; Paper &#128202; Slides | . . . . . Teaching . Using Conda for Package and Environment Management Seminar on Applied Predictive Analytics at HU Berlin, 2021 | . . Abstract: The talk provides a brief practical-driven introduction into using Conda for package and environment management in Python-based data science projects. The talks covers the basics of package and environment management and provides a tutorial on the basic Conda functionality. | &#128220; Abstract &#128202; Slides &#128249; Video | . . Click here to read more about my teaching experience at HU Berlin | .",
          "url": "https://kozodoi.me/talks/",
          "relUrl": "/talks/",
          "date": ""
      }
      
  

  
      ,"page6": {
          "title": "Papers",
          "content": "Research . This page summarizes my research activities, including publications in academic journals and conference papers. Check out my public talks or follow me on Google Scholar to see all of my work. . This page summarizes my research activities, including publications in academic journals and conference papers. Check out my public talks or follow me on Google Scholar to see all of my work. . . This page overviews my research activities: . &#128218; Selected academic publications | &#128200; My citation statistics | &#128221; Outlets where I was acting as a reviewer | . . . . Publications . 2024 . Kozodoi, N., Lessmann, S., Alamgir, M., Moreira-Matias, L. Papakonstantinou, K. Fighting Sampling Bias: A Framework for Training and Evaluating Credit Scoring Models ArXiv preprint. | . . Abstract: Scoring models support decision-making in financial institutions. Their estimation and evaluation are based on the data of previously accepted applicants with known repayment behavior. This creates sampling bias: the available labeled data offers a partial picture of the distribution of candidate borrowers, which the model is supposed to score. The paper addresses the adverse effect of sampling bias on model training and evaluation. To improve scorecard training, we propose bias-aware self-learning - a reject inference framework that augments the biased training data by inferring labels for selected rejected applications. For scorecard evaluation, we propose a Bayesian framework that extends standard accuracy measures to the biased setting and provides a reliable estimate of future scorecard performance. Extensive experiments on synthetic and real-world data confirm the superiority of our propositions over various benchmarks in predictive performance and profitability. By sensitivity analysis, we also identify boundary conditions affecting their performance. Notably, we leverage real-world data from a randomized controlled trial to assess the novel methodologies on holdout data that represent the true borrower population. Our findings confirm that reject inference is a difficult problem with modest potential to improve scorecard performance. Addressing sampling bias during scorecard evaluation is a much more promising route to improve scoring practices. For example, our results suggest a profit improvement of about eight percent, when using Bayesian evaluation to decide on acceptance rates. | &#128220; Abstract &#128214; PDF | . . 2023 . Kozodoi, N., Zinovyeva, L., Valentin, S., Pereira, J., Agundez, R. (2023). Probabilistic Demand Forecasting with Graph Neural Networks In Workshop on ML for Irregular Time Series at ECML PKDD 2023. | . . Abstract: Demand forecasting is a prominent business use case that allows retailers to optimize inventory planning, logistics, and core business decisions. One of the key challenges in demand forecasting is accounting for relationships and interactions between articles. Most modern forecasting approaches provide independent article-level predictions that do not consider the impact of related articles. Recent research has attempted addressing this challenge using Graph Neural Networks (GNNs) and showed promising results. This paper builds on previous research on GNNs and makes two contributions. First, we integrate a GNN encoder into a state-of-the-art DeepAR model. The combined model produces probabilistic forecasts, which are crucial for decision-making under uncertainty. Second, we propose to build graphs using article attribute similarity, which avoids reliance on a pre-defined graph structure. Experiments on three real-world datasets show that the proposed approach consistently outperforms non-graph benchmarks. We also show that our approach produces article embeddings that encode article similarity and demand dynamics and are useful for other downstream business tasks beyond forecasting. | &#128220; Abstract &#128214; PDF | . . 2022 . Kozodoi, N. (2022). Machine Learning for Credit Risk Analytics PhD Thesis. Humboldt University of Berlin. | . . Abstract: The rise of machine learning (ML) and the rapid digitization of the economy has substantially changed decision processes in the financial industry. Financial institutions increasingly rely on ML to support decision-making. Credit scoring is one of the prominent ML applications in finance. The task of credit scoring is to distinguish between applicants who will pay back the loan or default. Financial institutions use ML to develop scoring models to estimate a borrower&#39;s probability of default and automate approval decisions. This dissertation focuses on three major challenges associated with building ML-based scorecards in consumer credit scoring: (i) optimizing data acquisition and storage costs when dealing with high-dimensional data of loan applicants; (ii) addressing the adverse effects of sampling bias on training and evaluation of scoring models; (iii) measuring and ensuring the scorecard fairness while maintaining high profitability. The thesis offers a set of tools to remedy each of these challenges and improve decision-making practices in financial institutions. | &#128220; Abstract &#128214; PDF | . . 2021 . Kozodoi, N., Jacob, J., &amp; Lessmann, S. (2021). Fairness in Credit Scoring: Assessment, Implementation and Profit Implications. European Journal of Operational Research, 297, 1083-1094. | . . Abstract: The rise of algorithmic decision-making has spawned much research on fair machine learning (ML). Financial institutions use ML for building risk scorecards that support a range of credit-related decisions. Yet, the literature on fair ML in credit scoring is scarce. The paper makes three contributions. First, we revisit statistical fairness criteria and examine their adequacy for credit scoring. Second, we catalog algorithmic options for incorporating fairness goals in the ML model development pipeline. Last, we empirically compare different fairness processors in a profit-oriented credit scoring context using real-world data. The empirical results substantiate the evaluation of fairness measures, identify suitable options to implement fair credit scoring, and clarify the profit-fairness trade-off in lending decisions. We find that multiple fairness criteria can be approximately satisfied at once and recommend separation as a proper criterion for measuring the fairness of a scorecard. We also find fair in-processors to deliver a good balance between profit and fairness and show that algorithmic discrimination can be reduced to a reasonable level at a relatively low cost. The codes corresponding to the paper are available on GitHub. | &#128220; Abstract &#128214; PDF on arXiv &#128213; Published version &#128187; GitHub repo | . . 2020 . Kozodoi, N., Lessmann, S. (2020). Multi-Objective Particle Swarm Optimization for Feature Selection in Credit Scoring. In Workshop on Mining Data for Financial Applications at ECML PKDD 2020 (pp. 68-76). Springer, Cham. | . . Abstract: Credit scoring refers to the use of statistical models to support loan approval decisions. An ever-increasing availability of data on potential borrowers emphasizes the importance of feature selection for scoring models. Traditionally, feature selection has been viewed as a single-objective task. Recent research demonstrates the effectiveness of multi-objective approaches. We propose a novel multi-objective feature selection framework for credit scoring that extends previous work by taking into account data acquisition costs and employing a state-of-the-art particle swarm optimization algorithm. Our framework optimizes three fitness functions: the number of features, data acquisition costs and the AUC. Experiments on nine credit scoring data sets demonstrate a highly competitive performance of the proposed framework. | &#128220; Abstract &#128214; PDF on ResearchGate &#128213; Published version | . . Kozodoi, N., Katsas, P., Lessmann, S., Moreira-Matias, L., &amp; Papakonstantinou, K. (2020). Shallow Self-Learning for Reject Inference in Credit Scoring. In ECML PKDD 2019 Proceedings (pp. 516-532). Springer, Cham. | . . Abstract: Credit scoring models support loan approval decisions in the financial services industry. Lenders train these models on data from previously granted credit applications, where the borrowers’ repayment behavior has been observed. This approach creates sample bias. The scoring model is trained on accepted cases only. Applying the model to screen applications from the population of all borrowers degrades its performance. Reject inference comprises techniques to overcome sampling bias through assigning labels to rejected cases. This paper makes two contributions. First, we propose a self-learning framework for reject inference. The framework is geared toward real-world credit scoring requirements through considering distinct training regimes for labeling and model training. Second, we introduce a new measure to assess the effectiveness of reject inference strategies. Our measure leverages domain knowledge to avoid artificial labeling of rejected cases during evaluation. We demonstrate this approach to offer a robust and operational assessment of reject inference. Experiments on a real-world credit scoring data set confirm the superiority of the suggested self-learning framework over previous reject inference strategies. We also find strong evidence in favor of the proposed evaluation measure assessing reject inference strategies more reliably, raising the performance of the eventual scoring model. | &#128220; Abstract &#128214; PDF on arXiv &#128213; Published version &#128202; Slides | . . 2019 . Kozodoi, N., Lessmann, S., Papakonstantinou, K., Gatsoulis, Y., &amp; Baesens, B. (2019). A Multi-Objective Approach for Profit-Driven Feature Selection in Credit Scoring. Decision Support Systems, 120, 106-117. | . . Abstract: In credit scoring, feature selection aims at removing irrelevant data to improve the performance of the scorecard and its interpretability. Standard techniques treat feature selection as a single-objective task and rely on statistical criteria such as correlation. Recent studies suggest that using profit-based indicators may improve the quality of scoring models for businesses. We extend the use of profit measures to feature selection and develop a multi-objective wrapper framework based on the NSGA-II genetic algorithm with two fitness functions: the Expected Maximum Profit (EMP) and the number of features. Experiments on multiple credit scoring data sets demonstrate that the proposed approach develops scorecards that can yield a higher expected profit using fewer features than conventional feature selection strategies. | &#128220; Abstract &#128214; PDF on ResearchGate &#128213; Published version | . . Kozodoi, N., Lessmann, S., Baesens, B., &amp; Papakonstantinou, K. (2019). Profit-Oriented Feature Selection in Credit Scoring Applications. In Operations Research 2018 Proceedings (pp. 59-65). Springer, Cham. | . . Abstract: In credit scoring, feature selection aims at removing irrelevant data to improve the performance of the scorecard and its interpretability. Standard feature selection techniques are based on statistical criteria such as correlation. Recent studies suggest that using profit-based indicators for model evaluation may improve the quality of scoring models for businesses. We extend the use of profit measures to feature selection and develop a wrapper-based framework that uses the Expected Maximum Profit measure (EMP) as a fitness function. Experiments on multiple credit scoring data sets provide evidence that EMP-maximizing feature selection helps to develop scorecards that yield a higher expected profit compared to conventional feature selection strategies. | &#128220; Abstract &#128214; PDF on ResearchGate &#128213; Published version &#128202; Slides | . . 2018 . Artinger, F., Kozodoi, N., Wangenheim, F., Gigerenzer, G. (2018). Recency: Prediction with Smart Data. In 2018 AMA Winter Academic Conference Proceedings (pp. L2-L6). | . . Abstract: Since the early 1910s, managers have been using a simple recency-based decision strategy, the hiatus heuristic, to identify valuable customers. This study analyses the role of recency using a library of 60 data sets from business and other areas including weather, sports, and medicine. We find that the hiatus heuristic outperforms complex algorithms from machine learning, stochastic and econometric models in many of these environments. Moreover, if one includes further variables apart from recency in the complex algorithms, their performance does not improve. We show that the results are not so much driven by limited sample size than by the dominant role that recency plays in most of these environments. We conclude that less can be more, that is, relying on smart data such as recency can yield powerful predictions. | &#128220; Abstract &#128213; Published version | . . . . . Citations . . . . Reviews . I have been acting as an expert reviewer at the following academic outlets: . European Journal of Operational Research | Decision Support Systems | International Journal of Forecasting | Expert Systems with Applications | Business and Information Systems Engineering | Journal of Business Analytics | Journal of the Royal Statistical Society | The R Journal | .",
          "url": "https://kozodoi.me/papers/",
          "relUrl": "/papers/",
          "date": ""
      }
      
  

  
      ,"page7": {
          "title": "Kaggle",
          "content": "Kaggle . I enjoy taking part at ML competitions on Kaggle. This page summarizes my achievements and provides links to blog posts, writeups and GitHub repos with my solutions. Check out my Kaggle profile to see more. . I enjoy taking part at ML competitions on Kaggle. This page summarizes my achievements and provides links to blog posts, writeups and GitHub repos with my solutions. Check out my Kaggle profile to see more. . Overall rank . I am in the top-1% Kaggle users in Competitions, Notebooks, Datasets and Discussion. My highest user ranks are: . 🏆 Competitions: 293 (Master) | 📊 Notebooks: 330 (Expert) | 📁 Datasets: 75 (Expert) | 🗣 Discussion: 194 (Expert) | . . . Competitions . My medals are grouped by application areas. Follow the links for summaries, code and documentation. . Computer vision . 🥇 Cassava Leaf Disease Classification, top-1%. Identified sick plants with deep learning 📖 Summary 💻 GitHub | 🥇 SIIM-ISIC Melanoma Classification, top-1%. Trained CNNs for skin lesion classification 📖 Summary 📋 Blog post | 🥈 PetFinder Pawpularity Contest, top-4%. Predicted pet adoption from image and tabular data 💻 GitHub 📊 App | 🥈 RANZCR Catheter and Line Position Challenge, top-5%. Detected catheters on x-rays  📖 Summary 💻 GitHub | 🥉 Prostate Cancer Grade Assessment Challenge, top-6%. Diagnosed cancer on prostate tissue biopsies | 🥉 SETI Breakthrough Listen - E.T. Signal Search, top-8%. Detected anomalies in radio signals 💻 GitHub | 🥉 APTOS 2019 Blindness Detection, top-9%. Identified retinopathy on retina photos 💻 GitHub 📋 Blog post | 🥉 RSNA STR Pulmonary Embolism Detection, top-13%, Classified embolism in chest CT scans 📋 Blog post | . Natural language processing . 🥈 BMS Molecular Translation, top-5%. Built CNN-LSTM for image-to-text translation 📖 Summary 💻 GitHub | 🥉 CommonLit Readability Prize, top-9%. Predicted readability with transformers 💻 GitHub 📊 App 📋 Blog post | . Tabular data . 🥈 Google Analytics Customer Revenue Prediction, top-2%. Predicted customer spendings 💻 GitHub | 🥈 IEEE-CIS Fraud Detection, top-3%. Detected fraudulent consumer transactions 💻 GitHub | 🥈 Home Credit Default Risk, top-4%. Classified risky applicants with gradient boosting 💻 GitHub | 🥉 COVID-19 Vaccine Degradation Prediction, top-6%. Built RNNs for predicting mRNA degradation | 🥉 Instant Gratification, top-6%. Trained classical ML models for synthetic data classification | 🥉 Mechanisms of Action Prediction, top-10%. Classified drugs with deep learning algorithms | . Time series . 🥈 PLAsTiCC Astronomical Classification, top-5%. Identified astronomical objects by their signals 💻 GitHub | 🥉 Riiid! Answer Correctness Prediction, top-7%. Predicted test answer correctness with gradient boosting | . . . Top-rated notebooks . My Kaggle notebooks that received the most upvotes from the community. . 🔥 LightGBM on Meta-Features: classified pulmonary embolism with features extracted from X-rays | 🔥 EfficientNet + Multi-Layer LSTM: translated molecule images to chemical formulas with deep learning | 🔥 Stack Them All!: stacking ensemble pipeline for leaf disease classification with CNN models | .",
          "url": "https://kozodoi.me/kaggle/",
          "relUrl": "/kaggle/",
          "date": ""
      }
      
  

  

  
  

  
  

  
      ,"page11": {
          "title": "",
          "content": "Teaching . I act as a human teacher of Machine Learning at the Chair of Information Systems at HU Berlin. This includes teaching Data Science and supervising dissertations and seminar projects on applied ML topics. . I act as a human teacher of Machine Learning at the Chair of Information Systems at HU Berlin. This includes teaching Data Science and supervising dissertations and seminar projects on applied ML topics. . Courses . Business Analytics and Data Science . preparing Python notebooks with ML exercises | hosting online coding sessions and tutorials | grading students’ ML projects and preparing exam questions | . 💻 Course GitHub . . Advanced Data Analytics for Management Support . hosting in-class Kaggle competition on NLP with deep learning | answering students’ questions in Q&amp;A sessions and discussion forum | grading students’ ML projects | . 💻 Course GitHub 🥇 Kaggle competition . . Applied Predictive Analytics . teaching ML infrastructure (conda, package management, git) | supervising student’s projects on applied ML topics in credit scoring | grading students’ seminar papers | . 📹 Sample video . . Information Systems Seminar . supervising student’s projects on applied ML topics | grading students’ seminar papers | . . . Student dissertations . A Cost-Benefit Analysis of Active Learning Methods in Credit Scoring | Reject Inference using Generative Adversarial Neural Networks | A Multi-Objective Particle Swarm Optimization Approach for Feature Selection | Multi-Objective Feature Selection in Credit Scoring | .",
          "url": "https://kozodoi.me/teaching/",
          "relUrl": "/teaching/",
          "date": ""
      }
      
  

  
      ,"page12": {
          "title": "",
          "content": "Certifications . This page provides a list of certifications and courses I completed. Click here to download my CV and check out my portfolio to see my work. . This page provides a list of certifications and courses I completed. Click here to download my CV and check out my portfolio to see my work. . . This page overviews my certifications and courses: . &#9729; Cloud services | &#129302; Machine learning | &#128187; Coding | &#128190; Databases | . . . . Cloud Services . AWS Certified Machine Learning Specialty Issued by Amazon Web Services | . . Summary: Demonstrated in-depth understanding of AWS Machine Learning services. Learned to build, train, tune, and deploy ML models using the AWS Cloud. Showed the ability to derive insight from AWS ML services using either pretrained models or custom models built from open-source frameworks. | &#128220; Summary &#127891; Certificate | . . AWS Certified Developer - Associate Issued by Amazon Web Services | . . Summary: Showed a comprehensive understanding of application life-cycle management. Demonstrated proficiency in deploying with a CI/CD pipeline and using containers. Showed ability to develop, deploy and debug cloud-based applications that follow AWS best practices. | &#128220; Summary &#127891; Certificate | . . AWS Certified Solutions Architect - Associate Issued by Amazon Web Services | . . Summary: Gained a comprehensive understanding of AWS services and technologies. Demonstrated the ability to design well-architected cloud solutions that are scalable, secure, resilient, efficient and fault-tolerant. | &#128220; Summary &#127891; Certificate | . . AWS Certified Cloud Practitioner Issued by Amazon Web Services | . . Summary: Gained a fundamental understanding of IT services and their uses in the AWS Cloud. Demonstrated cloud fluency and foundational AWS knowledge. Learned to identify essential AWS services necessary to set up AWS-focused projects. | &#128220; Summary &#127891; Certificate | . . . . . Machine Learning . Machine Learning Engineer Nanodegree Issued by Udacity | . . Summary: Learned software engineering and object-oriented programming practices and developed an open-source Python package for data processing. Deployed Machine Learning and Deep Learning models using Amazon SageMaker. Used API Gateway and Lambda to integrate deployed models into interactive web apps. | &#128220; Summary &#127891; Certificate &#128187; Capstone project | . . Deep Learning Nanodegree Issued by Udacity | . . Summary: Learned theoretical foundations of Deep Learning. Implemented different neural network architectures from scratch. Developed PyTorch modeling pipelines using CNNs, RNNs and LSTMs for a variety of prediction tasks. Deployed the trained models on Amazon SageMaker. | &#128220; Summary &#127891; Certificate | . . . . . Coding . Algorithmic Toolbox Issued by University of California, San Diego | . . Summary: Learned key algorithmic techniques and concepts arising frequently in practical applications, including sorting and searching, divide and conquer, greedy algorithms, dynamic programming and recursion. Implemented algorithms to solve a variety of computational problems in Python and analyzed their running and memory complexity. | &#128220; Summary &#127891; Certificate | . . Data Structures Issued by University of California, San Diego | . . Summary: Learned common data structures used in various computational problems, including arrays, linked lists, stacks, queues, hash tables and trees. Analyzed typical use cases for these data structures and complexity of common operations. Practiced implementing data structures in Python programming assignments. | &#128220; Summary &#127891; Certificate | . . Data Structures and Algorithms with Python Issued by Codecademy | . . Summary: Reviewed basic data structures and algorithms and extensively practiced their implementation in Python. Practiced analyzing running and memory complexity of different data structures and algorithms. | &#128220; Summary &#127891; Certificate | . . . . . Databases . SQL for Data Science Issued by University of California, Davis | . . Summary: Learned fundamentals of SQL for Data Science purposes, including extracting, manipulating and combining data. Covered filtering, sorting and aggregating functionality. Practiced subqueries and table joins. Solved a variety of SQL programming tasks. | &#128220; Summary &#127891; Certificate | . .",
          "url": "https://kozodoi.me/certifications/",
          "relUrl": "/certifications/",
          "date": ""
      }
      
  

  
  

  
  

  
  

  

  
  

  
      ,"page18": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://kozodoi.me/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}